Training the 'CAE_3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	100
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/100
----------
Pretraining:	Epoch: [1][10/235]	Loss 0.7585 (0.8787)	
Pretraining:	Epoch: [1][20/235]	Loss 0.7049 (0.7987)	
Pretraining:	Epoch: [1][30/235]	Loss 0.6343 (0.7541)	
Pretraining:	Epoch: [1][40/235]	Loss 0.5762 (0.7244)	
Pretraining:	Epoch: [1][50/235]	Loss 0.5780 (0.6940)	
Pretraining:	Epoch: [1][60/235]	Loss 0.4947 (0.6667)	
Pretraining:	Epoch: [1][70/235]	Loss 0.4403 (0.6399)	
Pretraining:	Epoch: [1][80/235]	Loss 0.4167 (0.6161)	
Pretraining:	Epoch: [1][90/235]	Loss 0.3934 (0.5929)	
Pretraining:	Epoch: [1][100/235]	Loss 0.3275 (0.5703)	
Pretraining:	Epoch: [1][110/235]	Loss 0.3083 (0.5494)	
Pretraining:	Epoch: [1][120/235]	Loss 0.3172 (0.5306)	
Pretraining:	Epoch: [1][130/235]	Loss 0.3105 (0.5139)	
Pretraining:	Epoch: [1][140/235]	Loss 0.2921 (0.4982)	
Pretraining:	Epoch: [1][150/235]	Loss 0.2905 (0.4849)	
Pretraining:	Epoch: [1][160/235]	Loss 0.2660 (0.4719)	
Pretraining:	Epoch: [1][170/235]	Loss 0.2574 (0.4602)	
Pretraining:	Epoch: [1][180/235]	Loss 0.2685 (0.4495)	
Pretraining:	Epoch: [1][190/235]	Loss 0.2653 (0.4399)	
Pretraining:	Epoch: [1][200/235]	Loss 0.2520 (0.4309)	
Pretraining:	Epoch: [1][210/235]	Loss 0.2521 (0.4223)	
Pretraining:	Epoch: [1][220/235]	Loss 0.2492 (0.4142)	
Pretraining:	Epoch: [1][230/235]	Loss 0.2401 (0.4066)	
Pretraining:	 Loss: 0.4035

Pretraining:	Epoch 2/100
----------
Pretraining:	Epoch: [2][10/235]	Loss 0.2246 (0.2364)	
Pretraining:	Epoch: [2][20/235]	Loss 0.2280 (0.2358)	
Pretraining:	Epoch: [2][30/235]	Loss 0.2221 (0.2329)	
Pretraining:	Epoch: [2][40/235]	Loss 0.2277 (0.2325)	
Pretraining:	Epoch: [2][50/235]	Loss 0.2481 (0.2299)	
Pretraining:	Epoch: [2][60/235]	Loss 0.2272 (0.2294)	
Pretraining:	Epoch: [2][70/235]	Loss 0.2111 (0.2286)	
Pretraining:	Epoch: [2][80/235]	Loss 0.2009 (0.2265)	
Pretraining:	Epoch: [2][90/235]	Loss 0.2193 (0.2257)	
Pretraining:	Epoch: [2][100/235]	Loss 0.1952 (0.2243)	
Pretraining:	Epoch: [2][110/235]	Loss 0.1975 (0.2231)	
Pretraining:	Epoch: [2][120/235]	Loss 0.2084 (0.2221)	
Pretraining:	Epoch: [2][130/235]	Loss 0.2004 (0.2213)	
Pretraining:	Epoch: [2][140/235]	Loss 0.1962 (0.2198)	
Pretraining:	Epoch: [2][150/235]	Loss 0.2140 (0.2194)	
Pretraining:	Epoch: [2][160/235]	Loss 0.1916 (0.2183)	
Pretraining:	Epoch: [2][170/235]	Loss 0.1896 (0.2174)	
Pretraining:	Epoch: [2][180/235]	Loss 0.2092 (0.2165)	
Pretraining:	Epoch: [2][190/235]	Loss 0.2050 (0.2159)	
Pretraining:	Epoch: [2][200/235]	Loss 0.1887 (0.2153)	
Pretraining:	Epoch: [2][210/235]	Loss 0.1959 (0.2142)	
Pretraining:	Epoch: [2][220/235]	Loss 0.1991 (0.2132)	
Pretraining:	Epoch: [2][230/235]	Loss 0.1937 (0.2123)	
Pretraining:	 Loss: 0.2119

Pretraining:	Epoch 3/100
----------
Pretraining:	Epoch: [3][10/235]	Loss 0.1794 (0.1895)	
Pretraining:	Epoch: [3][20/235]	Loss 0.1902 (0.1900)	
Pretraining:	Epoch: [3][30/235]	Loss 0.1851 (0.1897)	
Pretraining:	Epoch: [3][40/235]	Loss 0.1941 (0.1910)	
Pretraining:	Epoch: [3][50/235]	Loss 0.2156 (0.1905)	
Pretraining:	Epoch: [3][60/235]	Loss 0.1937 (0.1913)	
Pretraining:	Epoch: [3][70/235]	Loss 0.1814 (0.1916)	
Pretraining:	Epoch: [3][80/235]	Loss 0.1698 (0.1901)	
Pretraining:	Epoch: [3][90/235]	Loss 0.1850 (0.1898)	
Pretraining:	Epoch: [3][100/235]	Loss 0.1687 (0.1890)	
Pretraining:	Epoch: [3][110/235]	Loss 0.1719 (0.1886)	
Pretraining:	Epoch: [3][120/235]	Loss 0.1819 (0.1883)	
Pretraining:	Epoch: [3][130/235]	Loss 0.1768 (0.1880)	
Pretraining:	Epoch: [3][140/235]	Loss 0.1715 (0.1872)	
Pretraining:	Epoch: [3][150/235]	Loss 0.1926 (0.1873)	
Pretraining:	Epoch: [3][160/235]	Loss 0.1676 (0.1868)	
Pretraining:	Epoch: [3][170/235]	Loss 0.1679 (0.1864)	
Pretraining:	Epoch: [3][180/235]	Loss 0.1870 (0.1860)	
Pretraining:	Epoch: [3][190/235]	Loss 0.1859 (0.1859)	
Pretraining:	Epoch: [3][200/235]	Loss 0.1656 (0.1857)	
Pretraining:	Epoch: [3][210/235]	Loss 0.1760 (0.1851)	
Pretraining:	Epoch: [3][220/235]	Loss 0.1797 (0.1846)	
Pretraining:	Epoch: [3][230/235]	Loss 0.1743 (0.1841)	
Pretraining:	 Loss: 0.1838

Pretraining:	Epoch 4/100
----------
Pretraining:	Epoch: [4][10/235]	Loss 0.1626 (0.1712)	
Pretraining:	Epoch: [4][20/235]	Loss 0.1734 (0.1722)	
Pretraining:	Epoch: [4][30/235]	Loss 0.1673 (0.1725)	
Pretraining:	Epoch: [4][40/235]	Loss 0.1751 (0.1741)	
Pretraining:	Epoch: [4][50/235]	Loss 0.1900 (0.1726)	
Pretraining:	Epoch: [4][60/235]	Loss 0.1748 (0.1732)	
Pretraining:	Epoch: [4][70/235]	Loss 0.1653 (0.1735)	
Pretraining:	Epoch: [4][80/235]	Loss 0.1544 (0.1724)	
Pretraining:	Epoch: [4][90/235]	Loss 0.1694 (0.1724)	
Pretraining:	Epoch: [4][100/235]	Loss 0.1560 (0.1719)	
Pretraining:	Epoch: [4][110/235]	Loss 0.1580 (0.1718)	
Pretraining:	Epoch: [4][120/235]	Loss 0.1677 (0.1717)	
Pretraining:	Epoch: [4][130/235]	Loss 0.1666 (0.1718)	
Pretraining:	Epoch: [4][140/235]	Loss 0.1596 (0.1712)	
Pretraining:	Epoch: [4][150/235]	Loss 0.1818 (0.1716)	
Pretraining:	Epoch: [4][160/235]	Loss 0.1555 (0.1713)	
Pretraining:	Epoch: [4][170/235]	Loss 0.1564 (0.1710)	
Pretraining:	Epoch: [4][180/235]	Loss 0.1745 (0.1709)	
Pretraining:	Epoch: [4][190/235]	Loss 0.1744 (0.1709)	
Pretraining:	Epoch: [4][200/235]	Loss 0.1513 (0.1708)	
Pretraining:	Epoch: [4][210/235]	Loss 0.1644 (0.1704)	
Pretraining:	Epoch: [4][220/235]	Loss 0.1677 (0.1700)	
Pretraining:	Epoch: [4][230/235]	Loss 0.1628 (0.1697)	
Pretraining:	 Loss: 0.1695

Pretraining:	Epoch 5/100
----------
Pretraining:	Epoch: [5][10/235]	Loss 0.1529 (0.1596)	
Pretraining:	Epoch: [5][20/235]	Loss 0.1645 (0.1608)	
Pretraining:	Epoch: [5][30/235]	Loss 0.1550 (0.1611)	
Pretraining:	Epoch: [5][40/235]	Loss 0.1660 (0.1627)	
Pretraining:	Epoch: [5][50/235]	Loss 0.1780 (0.1616)	
Pretraining:	Epoch: [5][60/235]	Loss 0.1646 (0.1623)	
Pretraining:	Epoch: [5][70/235]	Loss 0.1548 (0.1627)	
Pretraining:	Epoch: [5][80/235]	Loss 0.1451 (0.1618)	
Pretraining:	Epoch: [5][90/235]	Loss 0.1614 (0.1620)	
Pretraining:	Epoch: [5][100/235]	Loss 0.1490 (0.1617)	
Pretraining:	Epoch: [5][110/235]	Loss 0.1494 (0.1617)	
Pretraining:	Epoch: [5][120/235]	Loss 0.1586 (0.1617)	
Pretraining:	Epoch: [5][130/235]	Loss 0.1572 (0.1619)	
Pretraining:	Epoch: [5][140/235]	Loss 0.1516 (0.1613)	
Pretraining:	Epoch: [5][150/235]	Loss 0.1743 (0.1618)	
Pretraining:	Epoch: [5][160/235]	Loss 0.1465 (0.1615)	
Pretraining:	Epoch: [5][170/235]	Loss 0.1484 (0.1613)	
Pretraining:	Epoch: [5][180/235]	Loss 0.1662 (0.1612)	
Pretraining:	Epoch: [5][190/235]	Loss 0.1658 (0.1613)	
Pretraining:	Epoch: [5][200/235]	Loss 0.1433 (0.1613)	
Pretraining:	Epoch: [5][210/235]	Loss 0.1568 (0.1610)	
Pretraining:	Epoch: [5][220/235]	Loss 0.1590 (0.1607)	
Pretraining:	Epoch: [5][230/235]	Loss 0.1547 (0.1603)	
Pretraining:	 Loss: 0.1602

Pretraining:	Epoch 6/100
----------
Pretraining:	Epoch: [6][10/235]	Loss 0.1439 (0.1506)	
Pretraining:	Epoch: [6][20/235]	Loss 0.1569 (0.1521)	
Pretraining:	Epoch: [6][30/235]	Loss 0.1474 (0.1528)	
Pretraining:	Epoch: [6][40/235]	Loss 0.1575 (0.1547)	
Pretraining:	Epoch: [6][50/235]	Loss 0.1702 (0.1538)	
Pretraining:	Epoch: [6][60/235]	Loss 0.1568 (0.1547)	
Pretraining:	Epoch: [6][70/235]	Loss 0.1475 (0.1551)	
Pretraining:	Epoch: [6][80/235]	Loss 0.1377 (0.1542)	
Pretraining:	Epoch: [6][90/235]	Loss 0.1542 (0.1543)	
Pretraining:	Epoch: [6][100/235]	Loss 0.1408 (0.1540)	
Pretraining:	Epoch: [6][110/235]	Loss 0.1419 (0.1540)	
Pretraining:	Epoch: [6][120/235]	Loss 0.1506 (0.1541)	
Pretraining:	Epoch: [6][130/235]	Loss 0.1499 (0.1541)	
Pretraining:	Epoch: [6][140/235]	Loss 0.1445 (0.1537)	
Pretraining:	Epoch: [6][150/235]	Loss 0.1677 (0.1541)	
Pretraining:	Epoch: [6][160/235]	Loss 0.1385 (0.1539)	
Pretraining:	Epoch: [6][170/235]	Loss 0.1428 (0.1538)	
Pretraining:	Epoch: [6][180/235]	Loss 0.1601 (0.1537)	
Pretraining:	Epoch: [6][190/235]	Loss 0.1592 (0.1540)	
Pretraining:	Epoch: [6][200/235]	Loss 0.1365 (0.1540)	
Pretraining:	Epoch: [6][210/235]	Loss 0.1504 (0.1538)	
Pretraining:	Epoch: [6][220/235]	Loss 0.1534 (0.1536)	
Pretraining:	Epoch: [6][230/235]	Loss 0.1484 (0.1533)	
Pretraining:	 Loss: 0.1532

Pretraining:	Epoch 7/100
----------
Pretraining:	Epoch: [7][10/235]	Loss 0.1377 (0.1447)	
Pretraining:	Epoch: [7][20/235]	Loss 0.1511 (0.1462)	
Pretraining:	Epoch: [7][30/235]	Loss 0.1416 (0.1469)	
Pretraining:	Epoch: [7][40/235]	Loss 0.1502 (0.1484)	
Pretraining:	Epoch: [7][50/235]	Loss 0.1631 (0.1474)	
Pretraining:	Epoch: [7][60/235]	Loss 0.1537 (0.1484)	
Pretraining:	Epoch: [7][70/235]	Loss 0.1431 (0.1491)	
Pretraining:	Epoch: [7][80/235]	Loss 0.1325 (0.1484)	
Pretraining:	Epoch: [7][90/235]	Loss 0.1501 (0.1486)	
Pretraining:	Epoch: [7][100/235]	Loss 0.1364 (0.1484)	
Pretraining:	Epoch: [7][110/235]	Loss 0.1367 (0.1485)	
Pretraining:	Epoch: [7][120/235]	Loss 0.1455 (0.1486)	
Pretraining:	Epoch: [7][130/235]	Loss 0.1448 (0.1487)	
Pretraining:	Epoch: [7][140/235]	Loss 0.1393 (0.1483)	
Pretraining:	Epoch: [7][150/235]	Loss 0.1634 (0.1487)	
Pretraining:	Epoch: [7][160/235]	Loss 0.1343 (0.1485)	
Pretraining:	Epoch: [7][170/235]	Loss 0.1385 (0.1484)	
Pretraining:	Epoch: [7][180/235]	Loss 0.1555 (0.1484)	
Pretraining:	Epoch: [7][190/235]	Loss 0.1536 (0.1487)	
Pretraining:	Epoch: [7][200/235]	Loss 0.1319 (0.1488)	
Pretraining:	Epoch: [7][210/235]	Loss 0.1459 (0.1486)	
Pretraining:	Epoch: [7][220/235]	Loss 0.1487 (0.1484)	
Pretraining:	Epoch: [7][230/235]	Loss 0.1441 (0.1482)	
Pretraining:	 Loss: 0.1481

Pretraining:	Epoch 8/100
----------
Pretraining:	Epoch: [8][10/235]	Loss 0.1327 (0.1401)	
Pretraining:	Epoch: [8][20/235]	Loss 0.1466 (0.1417)	
Pretraining:	Epoch: [8][30/235]	Loss 0.1367 (0.1422)	
Pretraining:	Epoch: [8][40/235]	Loss 0.1458 (0.1438)	
Pretraining:	Epoch: [8][50/235]	Loss 0.1595 (0.1430)	
Pretraining:	Epoch: [8][60/235]	Loss 0.1499 (0.1440)	
Pretraining:	Epoch: [8][70/235]	Loss 0.1392 (0.1447)	
Pretraining:	Epoch: [8][80/235]	Loss 0.1288 (0.1440)	
Pretraining:	Epoch: [8][90/235]	Loss 0.1464 (0.1442)	
Pretraining:	Epoch: [8][100/235]	Loss 0.1323 (0.1439)	
Pretraining:	Epoch: [8][110/235]	Loss 0.1326 (0.1441)	
Pretraining:	Epoch: [8][120/235]	Loss 0.1406 (0.1442)	
Pretraining:	Epoch: [8][130/235]	Loss 0.1408 (0.1443)	
Pretraining:	Epoch: [8][140/235]	Loss 0.1363 (0.1439)	
Pretraining:	Epoch: [8][150/235]	Loss 0.1626 (0.1446)	
Pretraining:	Epoch: [8][160/235]	Loss 0.1304 (0.1445)	
Pretraining:	Epoch: [8][170/235]	Loss 0.1362 (0.1445)	
Pretraining:	Epoch: [8][180/235]	Loss 0.1525 (0.1445)	
Pretraining:	Epoch: [8][190/235]	Loss 0.1504 (0.1448)	
Pretraining:	Epoch: [8][200/235]	Loss 0.1370 (0.1450)	
Pretraining:	Epoch: [8][210/235]	Loss 0.1452 (0.1450)	
Pretraining:	Epoch: [8][220/235]	Loss 0.1450 (0.1449)	
Pretraining:	Epoch: [8][230/235]	Loss 0.1421 (0.1447)	
Pretraining:	 Loss: 0.1446

Pretraining:	Epoch 9/100
----------
Pretraining:	Epoch: [9][10/235]	Loss 0.1305 (0.1374)	
Pretraining:	Epoch: [9][20/235]	Loss 0.1448 (0.1389)	
Pretraining:	Epoch: [9][30/235]	Loss 0.1331 (0.1393)	
Pretraining:	Epoch: [9][40/235]	Loss 0.1424 (0.1405)	
Pretraining:	Epoch: [9][50/235]	Loss 0.1548 (0.1396)	
Pretraining:	Epoch: [9][60/235]	Loss 0.1445 (0.1405)	
Pretraining:	Epoch: [9][70/235]	Loss 0.1360 (0.1411)	
Pretraining:	Epoch: [9][80/235]	Loss 0.1254 (0.1404)	
Pretraining:	Epoch: [9][90/235]	Loss 0.1444 (0.1406)	
Pretraining:	Epoch: [9][100/235]	Loss 0.1313 (0.1405)	
Pretraining:	Epoch: [9][110/235]	Loss 0.1300 (0.1408)	
Pretraining:	Epoch: [9][120/235]	Loss 0.1368 (0.1409)	
Pretraining:	Epoch: [9][130/235]	Loss 0.1382 (0.1410)	
Pretraining:	Epoch: [9][140/235]	Loss 0.1326 (0.1407)	
Pretraining:	Epoch: [9][150/235]	Loss 0.1573 (0.1412)	
Pretraining:	Epoch: [9][160/235]	Loss 0.1268 (0.1410)	
Pretraining:	Epoch: [9][170/235]	Loss 0.1324 (0.1409)	
Pretraining:	Epoch: [9][180/235]	Loss 0.1480 (0.1409)	
Pretraining:	Epoch: [9][190/235]	Loss 0.1466 (0.1412)	
Pretraining:	Epoch: [9][200/235]	Loss 0.1270 (0.1413)	
Pretraining:	Epoch: [9][210/235]	Loss 0.1392 (0.1411)	
Pretraining:	Epoch: [9][220/235]	Loss 0.1410 (0.1410)	
Pretraining:	Epoch: [9][230/235]	Loss 0.1373 (0.1408)	
Pretraining:	 Loss: 0.1407

Pretraining:	Epoch 10/100
----------
Pretraining:	Epoch: [10][10/235]	Loss 0.1253 (0.1328)	
Pretraining:	Epoch: [10][20/235]	Loss 0.1411 (0.1348)	
Pretraining:	Epoch: [10][30/235]	Loss 0.1296 (0.1356)	
Pretraining:	Epoch: [10][40/235]	Loss 0.1385 (0.1372)	
Pretraining:	Epoch: [10][50/235]	Loss 0.1518 (0.1366)	
Pretraining:	Epoch: [10][60/235]	Loss 0.1401 (0.1374)	
Pretraining:	Epoch: [10][70/235]	Loss 0.1331 (0.1380)	
Pretraining:	Epoch: [10][80/235]	Loss 0.1226 (0.1372)	
Pretraining:	Epoch: [10][90/235]	Loss 0.1415 (0.1374)	
Pretraining:	Epoch: [10][100/235]	Loss 0.1282 (0.1374)	
Pretraining:	Epoch: [10][110/235]	Loss 0.1274 (0.1376)	
Pretraining:	Epoch: [10][120/235]	Loss 0.1340 (0.1378)	
Pretraining:	Epoch: [10][130/235]	Loss 0.1360 (0.1379)	
Pretraining:	Epoch: [10][140/235]	Loss 0.1305 (0.1376)	
Pretraining:	Epoch: [10][150/235]	Loss 0.1548 (0.1381)	
Pretraining:	Epoch: [10][160/235]	Loss 0.1247 (0.1380)	
Pretraining:	Epoch: [10][170/235]	Loss 0.1302 (0.1379)	
Pretraining:	Epoch: [10][180/235]	Loss 0.1452 (0.1379)	
Pretraining:	Epoch: [10][190/235]	Loss 0.1435 (0.1382)	
Pretraining:	Epoch: [10][200/235]	Loss 0.1249 (0.1384)	
Pretraining:	Epoch: [10][210/235]	Loss 0.1363 (0.1382)	
Pretraining:	Epoch: [10][220/235]	Loss 0.1379 (0.1381)	
Pretraining:	Epoch: [10][230/235]	Loss 0.1343 (0.1379)	
Pretraining:	 Loss: 0.1378

Pretraining:	Epoch 11/100
----------
Pretraining:	Epoch: [11][10/235]	Loss 0.1227 (0.1304)	
Pretraining:	Epoch: [11][20/235]	Loss 0.1387 (0.1323)	
Pretraining:	Epoch: [11][30/235]	Loss 0.1267 (0.1330)	
Pretraining:	Epoch: [11][40/235]	Loss 0.1352 (0.1345)	
Pretraining:	Epoch: [11][50/235]	Loss 0.1484 (0.1339)	
Pretraining:	Epoch: [11][60/235]	Loss 0.1373 (0.1347)	
Pretraining:	Epoch: [11][70/235]	Loss 0.1303 (0.1353)	
Pretraining:	Epoch: [11][80/235]	Loss 0.1201 (0.1346)	
Pretraining:	Epoch: [11][90/235]	Loss 0.1396 (0.1348)	
Pretraining:	Epoch: [11][100/235]	Loss 0.1262 (0.1348)	
Pretraining:	Epoch: [11][110/235]	Loss 0.1254 (0.1351)	
Pretraining:	Epoch: [11][120/235]	Loss 0.1316 (0.1353)	
Pretraining:	Epoch: [11][130/235]	Loss 0.1332 (0.1354)	
Pretraining:	Epoch: [11][140/235]	Loss 0.1283 (0.1351)	
Pretraining:	Epoch: [11][150/235]	Loss 0.1522 (0.1356)	
Pretraining:	Epoch: [11][160/235]	Loss 0.1223 (0.1354)	
Pretraining:	Epoch: [11][170/235]	Loss 0.1284 (0.1354)	
Pretraining:	Epoch: [11][180/235]	Loss 0.1427 (0.1354)	
Pretraining:	Epoch: [11][190/235]	Loss 0.1404 (0.1357)	
Pretraining:	Epoch: [11][200/235]	Loss 0.1223 (0.1358)	
Pretraining:	Epoch: [11][210/235]	Loss 0.1339 (0.1357)	
Pretraining:	Epoch: [11][220/235]	Loss 0.1350 (0.1355)	
Pretraining:	Epoch: [11][230/235]	Loss 0.1317 (0.1354)	
Pretraining:	 Loss: 0.1353

Pretraining:	Epoch 12/100
----------
Pretraining:	Epoch: [12][10/235]	Loss 0.1206 (0.1283)	
Pretraining:	Epoch: [12][20/235]	Loss 0.1362 (0.1303)	
Pretraining:	Epoch: [12][30/235]	Loss 0.1241 (0.1308)	
Pretraining:	Epoch: [12][40/235]	Loss 0.1320 (0.1322)	
Pretraining:	Epoch: [12][50/235]	Loss 0.1451 (0.1314)	
Pretraining:	Epoch: [12][60/235]	Loss 0.1345 (0.1322)	
Pretraining:	Epoch: [12][70/235]	Loss 0.1279 (0.1327)	
Pretraining:	Epoch: [12][80/235]	Loss 0.1180 (0.1321)	
Pretraining:	Epoch: [12][90/235]	Loss 0.1357 (0.1322)	
Pretraining:	Epoch: [12][100/235]	Loss 0.1222 (0.1321)	
Pretraining:	Epoch: [12][110/235]	Loss 0.1229 (0.1324)	
Pretraining:	Epoch: [12][120/235]	Loss 0.1292 (0.1326)	
Pretraining:	Epoch: [12][130/235]	Loss 0.1285 (0.1327)	
Pretraining:	Epoch: [12][140/235]	Loss 0.1257 (0.1324)	
Pretraining:	Epoch: [12][150/235]	Loss 0.1498 (0.1329)	
Pretraining:	Epoch: [12][160/235]	Loss 0.1204 (0.1327)	
Pretraining:	Epoch: [12][170/235]	Loss 0.1268 (0.1327)	
Pretraining:	Epoch: [12][180/235]	Loss 0.1407 (0.1327)	
Pretraining:	Epoch: [12][190/235]	Loss 0.1374 (0.1331)	
Pretraining:	Epoch: [12][200/235]	Loss 0.1197 (0.1332)	
Pretraining:	Epoch: [12][210/235]	Loss 0.1319 (0.1331)	
Pretraining:	Epoch: [12][220/235]	Loss 0.1326 (0.1330)	
Pretraining:	Epoch: [12][230/235]	Loss 0.1297 (0.1328)	
Pretraining:	 Loss: 0.1327

Pretraining:	Epoch 13/100
----------
Pretraining:	Epoch: [13][10/235]	Loss 0.1190 (0.1267)	
Pretraining:	Epoch: [13][20/235]	Loss 0.1337 (0.1284)	
Pretraining:	Epoch: [13][30/235]	Loss 0.1222 (0.1286)	
Pretraining:	Epoch: [13][40/235]	Loss 0.1296 (0.1299)	
Pretraining:	Epoch: [13][50/235]	Loss 0.1423 (0.1291)	
Pretraining:	Epoch: [13][60/235]	Loss 0.1330 (0.1299)	
Pretraining:	Epoch: [13][70/235]	Loss 0.1259 (0.1305)	
Pretraining:	Epoch: [13][80/235]	Loss 0.1163 (0.1299)	
Pretraining:	Epoch: [13][90/235]	Loss 0.1328 (0.1300)	
Pretraining:	Epoch: [13][100/235]	Loss 0.1204 (0.1299)	
Pretraining:	Epoch: [13][110/235]	Loss 0.1217 (0.1303)	
Pretraining:	Epoch: [13][120/235]	Loss 0.1301 (0.1307)	
Pretraining:	Epoch: [13][130/235]	Loss 0.1356 (0.1312)	
Pretraining:	Epoch: [13][140/235]	Loss 0.1258 (0.1311)	
Pretraining:	Epoch: [13][150/235]	Loss 0.1489 (0.1316)	
Pretraining:	Epoch: [13][160/235]	Loss 0.1185 (0.1315)	
Pretraining:	Epoch: [13][170/235]	Loss 0.1250 (0.1314)	
Pretraining:	Epoch: [13][180/235]	Loss 0.1381 (0.1315)	
Pretraining:	Epoch: [13][190/235]	Loss 0.1350 (0.1317)	
Pretraining:	Epoch: [13][200/235]	Loss 0.1176 (0.1318)	
Pretraining:	Epoch: [13][210/235]	Loss 0.1303 (0.1317)	
Pretraining:	Epoch: [13][220/235]	Loss 0.1311 (0.1316)	
Pretraining:	Epoch: [13][230/235]	Loss 0.1277 (0.1314)	
Pretraining:	 Loss: 0.1313

Pretraining:	Epoch 14/100
----------
Pretraining:	Epoch: [14][10/235]	Loss 0.1152 (0.1245)	
Pretraining:	Epoch: [14][20/235]	Loss 0.1317 (0.1265)	
Pretraining:	Epoch: [14][30/235]	Loss 0.1223 (0.1270)	
Pretraining:	Epoch: [14][40/235]	Loss 0.1284 (0.1284)	
Pretraining:	Epoch: [14][50/235]	Loss 0.1401 (0.1277)	
Pretraining:	Epoch: [14][60/235]	Loss 0.1344 (0.1286)	
Pretraining:	Epoch: [14][70/235]	Loss 0.1246 (0.1292)	
Pretraining:	Epoch: [14][80/235]	Loss 0.1149 (0.1286)	
Pretraining:	Epoch: [14][90/235]	Loss 0.1416 (0.1289)	
Pretraining:	Epoch: [14][100/235]	Loss 0.1237 (0.1298)	
Pretraining:	Epoch: [14][110/235]	Loss 0.1232 (0.1304)	
Pretraining:	Epoch: [14][120/235]	Loss 0.1321 (0.1307)	
Pretraining:	Epoch: [14][130/235]	Loss 0.1275 (0.1309)	
Pretraining:	Epoch: [14][140/235]	Loss 0.1292 (0.1309)	
Pretraining:	Epoch: [14][150/235]	Loss 0.1493 (0.1319)	
Pretraining:	Epoch: [14][160/235]	Loss 0.1181 (0.1318)	
Pretraining:	Epoch: [14][170/235]	Loss 0.1248 (0.1317)	
Pretraining:	Epoch: [14][180/235]	Loss 0.1371 (0.1316)	
Pretraining:	Epoch: [14][190/235]	Loss 0.1338 (0.1318)	
Pretraining:	Epoch: [14][200/235]	Loss 0.1165 (0.1319)	
Pretraining:	Epoch: [14][210/235]	Loss 0.1286 (0.1316)	
Pretraining:	Epoch: [14][220/235]	Loss 0.1294 (0.1314)	
Pretraining:	Epoch: [14][230/235]	Loss 0.1263 (0.1312)	
Pretraining:	 Loss: 0.1311

Pretraining:	Epoch 15/100
----------
Pretraining:	Epoch: [15][10/235]	Loss 0.1137 (0.1234)	
Pretraining:	Epoch: [15][20/235]	Loss 0.1309 (0.1254)	
Pretraining:	Epoch: [15][30/235]	Loss 0.1183 (0.1259)	
Pretraining:	Epoch: [15][40/235]	Loss 0.1270 (0.1272)	
Pretraining:	Epoch: [15][50/235]	Loss 0.1394 (0.1264)	
Pretraining:	Epoch: [15][60/235]	Loss 0.1293 (0.1272)	
Pretraining:	Epoch: [15][70/235]	Loss 0.1229 (0.1276)	
Pretraining:	Epoch: [15][80/235]	Loss 0.1140 (0.1269)	
Pretraining:	Epoch: [15][90/235]	Loss 0.1290 (0.1270)	
Pretraining:	Epoch: [15][100/235]	Loss 0.1209 (0.1268)	
Pretraining:	Epoch: [15][110/235]	Loss 0.1218 (0.1272)	
Pretraining:	Epoch: [15][120/235]	Loss 0.1262 (0.1275)	
Pretraining:	Epoch: [15][130/235]	Loss 0.1236 (0.1276)	
Pretraining:	Epoch: [15][140/235]	Loss 0.1224 (0.1273)	
Pretraining:	Epoch: [15][150/235]	Loss 0.1448 (0.1278)	
Pretraining:	Epoch: [15][160/235]	Loss 0.1152 (0.1277)	
Pretraining:	Epoch: [15][170/235]	Loss 0.1215 (0.1276)	
Pretraining:	Epoch: [15][180/235]	Loss 0.1342 (0.1276)	
Pretraining:	Epoch: [15][190/235]	Loss 0.1314 (0.1279)	
Pretraining:	Epoch: [15][200/235]	Loss 0.1148 (0.1280)	
Pretraining:	Epoch: [15][210/235]	Loss 0.1267 (0.1279)	
Pretraining:	Epoch: [15][220/235]	Loss 0.1274 (0.1278)	
Pretraining:	Epoch: [15][230/235]	Loss 0.1243 (0.1276)	
Pretraining:	 Loss: 0.1275

Pretraining:	Epoch 16/100
----------
Pretraining:	Epoch: [16][10/235]	Loss 0.1110 (0.1207)	
Pretraining:	Epoch: [16][20/235]	Loss 0.1283 (0.1227)	
Pretraining:	Epoch: [16][30/235]	Loss 0.1161 (0.1231)	
Pretraining:	Epoch: [16][40/235]	Loss 0.1232 (0.1243)	
Pretraining:	Epoch: [16][50/235]	Loss 0.1364 (0.1236)	
Pretraining:	Epoch: [16][60/235]	Loss 0.1274 (0.1245)	
Pretraining:	Epoch: [16][70/235]	Loss 0.1216 (0.1252)	
Pretraining:	Epoch: [16][80/235]	Loss 0.1119 (0.1247)	
Pretraining:	Epoch: [16][90/235]	Loss 0.1299 (0.1249)	
Pretraining:	Epoch: [16][100/235]	Loss 0.1242 (0.1248)	
Pretraining:	Epoch: [16][110/235]	Loss 0.1229 (0.1255)	
Pretraining:	Epoch: [16][120/235]	Loss 0.1234 (0.1258)	
Pretraining:	Epoch: [16][130/235]	Loss 0.1214 (0.1259)	
Pretraining:	Epoch: [16][140/235]	Loss 0.1200 (0.1256)	
Pretraining:	Epoch: [16][150/235]	Loss 0.1432 (0.1261)	
Pretraining:	Epoch: [16][160/235]	Loss 0.1142 (0.1259)	
Pretraining:	Epoch: [16][170/235]	Loss 0.1206 (0.1259)	
Pretraining:	Epoch: [16][180/235]	Loss 0.1329 (0.1259)	
Pretraining:	Epoch: [16][190/235]	Loss 0.1299 (0.1262)	
Pretraining:	Epoch: [16][200/235]	Loss 0.1134 (0.1263)	
Pretraining:	Epoch: [16][210/235]	Loss 0.1249 (0.1262)	
Pretraining:	Epoch: [16][220/235]	Loss 0.1260 (0.1261)	
Pretraining:	Epoch: [16][230/235]	Loss 0.1226 (0.1259)	
Pretraining:	 Loss: 0.1258

Pretraining:	Epoch 17/100
----------
Pretraining:	Epoch: [17][10/235]	Loss 0.1097 (0.1190)	
Pretraining:	Epoch: [17][20/235]	Loss 0.1272 (0.1210)	
Pretraining:	Epoch: [17][30/235]	Loss 0.1146 (0.1215)	
Pretraining:	Epoch: [17][40/235]	Loss 0.1213 (0.1227)	
Pretraining:	Epoch: [17][50/235]	Loss 0.1347 (0.1220)	
Pretraining:	Epoch: [17][60/235]	Loss 0.1258 (0.1228)	
Pretraining:	Epoch: [17][70/235]	Loss 0.1201 (0.1235)	
Pretraining:	Epoch: [17][80/235]	Loss 0.1106 (0.1230)	
Pretraining:	Epoch: [17][90/235]	Loss 0.1276 (0.1231)	
Pretraining:	Epoch: [17][100/235]	Loss 0.1205 (0.1231)	
Pretraining:	Epoch: [17][110/235]	Loss 0.1206 (0.1237)	
Pretraining:	Epoch: [17][120/235]	Loss 0.1221 (0.1240)	
Pretraining:	Epoch: [17][130/235]	Loss 0.1199 (0.1242)	
Pretraining:	Epoch: [17][140/235]	Loss 0.1189 (0.1239)	
Pretraining:	Epoch: [17][150/235]	Loss 0.1419 (0.1244)	
Pretraining:	Epoch: [17][160/235]	Loss 0.1122 (0.1242)	
Pretraining:	Epoch: [17][170/235]	Loss 0.1188 (0.1242)	
Pretraining:	Epoch: [17][180/235]	Loss 0.1314 (0.1242)	
Pretraining:	Epoch: [17][190/235]	Loss 0.1283 (0.1245)	
Pretraining:	Epoch: [17][200/235]	Loss 0.1121 (0.1247)	
Pretraining:	Epoch: [17][210/235]	Loss 0.1232 (0.1246)	
Pretraining:	Epoch: [17][220/235]	Loss 0.1248 (0.1245)	
Pretraining:	Epoch: [17][230/235]	Loss 0.1213 (0.1243)	
Pretraining:	 Loss: 0.1242

Pretraining:	Epoch 18/100
----------
Pretraining:	Epoch: [18][10/235]	Loss 0.1085 (0.1172)	
Pretraining:	Epoch: [18][20/235]	Loss 0.1258 (0.1193)	
Pretraining:	Epoch: [18][30/235]	Loss 0.1137 (0.1198)	
Pretraining:	Epoch: [18][40/235]	Loss 0.1194 (0.1210)	
Pretraining:	Epoch: [18][50/235]	Loss 0.1331 (0.1204)	
Pretraining:	Epoch: [18][60/235]	Loss 0.1244 (0.1213)	
Pretraining:	Epoch: [18][70/235]	Loss 0.1190 (0.1219)	
Pretraining:	Epoch: [18][80/235]	Loss 0.1094 (0.1214)	
Pretraining:	Epoch: [18][90/235]	Loss 0.1263 (0.1216)	
Pretraining:	Epoch: [18][100/235]	Loss 0.1188 (0.1215)	
Pretraining:	Epoch: [18][110/235]	Loss 0.1196 (0.1222)	
Pretraining:	Epoch: [18][120/235]	Loss 0.1205 (0.1225)	
Pretraining:	Epoch: [18][130/235]	Loss 0.1185 (0.1227)	
Pretraining:	Epoch: [18][140/235]	Loss 0.1179 (0.1225)	
Pretraining:	Epoch: [18][150/235]	Loss 0.1408 (0.1230)	
Pretraining:	Epoch: [18][160/235]	Loss 0.1113 (0.1228)	
Pretraining:	Epoch: [18][170/235]	Loss 0.1177 (0.1228)	
Pretraining:	Epoch: [18][180/235]	Loss 0.1301 (0.1228)	
Pretraining:	Epoch: [18][190/235]	Loss 0.1271 (0.1231)	
Pretraining:	Epoch: [18][200/235]	Loss 0.1110 (0.1233)	
Pretraining:	Epoch: [18][210/235]	Loss 0.1215 (0.1232)	
Pretraining:	Epoch: [18][220/235]	Loss 0.1238 (0.1231)	
Pretraining:	Epoch: [18][230/235]	Loss 0.1197 (0.1229)	
Pretraining:	 Loss: 0.1228

Pretraining:	Epoch 19/100
----------
Pretraining:	Epoch: [19][10/235]	Loss 0.1070 (0.1158)	
Pretraining:	Epoch: [19][20/235]	Loss 0.1240 (0.1178)	
Pretraining:	Epoch: [19][30/235]	Loss 0.1121 (0.1183)	
Pretraining:	Epoch: [19][40/235]	Loss 0.1181 (0.1195)	
Pretraining:	Epoch: [19][50/235]	Loss 0.1314 (0.1189)	
Pretraining:	Epoch: [19][60/235]	Loss 0.1229 (0.1197)	
Pretraining:	Epoch: [19][70/235]	Loss 0.1178 (0.1204)	
Pretraining:	Epoch: [19][80/235]	Loss 0.1083 (0.1199)	
Pretraining:	Epoch: [19][90/235]	Loss 0.1251 (0.1202)	
Pretraining:	Epoch: [19][100/235]	Loss 0.1173 (0.1201)	
Pretraining:	Epoch: [19][110/235]	Loss 0.1179 (0.1208)	
Pretraining:	Epoch: [19][120/235]	Loss 0.1190 (0.1211)	
Pretraining:	Epoch: [19][130/235]	Loss 0.1168 (0.1213)	
Pretraining:	Epoch: [19][140/235]	Loss 0.1168 (0.1211)	
Pretraining:	Epoch: [19][150/235]	Loss 0.1394 (0.1216)	
Pretraining:	Epoch: [19][160/235]	Loss 0.1101 (0.1215)	
Pretraining:	Epoch: [19][170/235]	Loss 0.1166 (0.1214)	
Pretraining:	Epoch: [19][180/235]	Loss 0.1289 (0.1215)	
Pretraining:	Epoch: [19][190/235]	Loss 0.1259 (0.1218)	
Pretraining:	Epoch: [19][200/235]	Loss 0.1099 (0.1220)	
Pretraining:	Epoch: [19][210/235]	Loss 0.1201 (0.1218)	
Pretraining:	Epoch: [19][220/235]	Loss 0.1225 (0.1218)	
Pretraining:	Epoch: [19][230/235]	Loss 0.1184 (0.1216)	
Pretraining:	 Loss: 0.1215

Pretraining:	Epoch 20/100
----------
Pretraining:	Epoch: [20][10/235]	Loss 0.1060 (0.1147)	
Pretraining:	Epoch: [20][20/235]	Loss 0.1226 (0.1167)	
Pretraining:	Epoch: [20][30/235]	Loss 0.1108 (0.1171)	
Pretraining:	Epoch: [20][40/235]	Loss 0.1169 (0.1183)	
Pretraining:	Epoch: [20][50/235]	Loss 0.1300 (0.1178)	
Pretraining:	Epoch: [20][60/235]	Loss 0.1215 (0.1186)	
Pretraining:	Epoch: [20][70/235]	Loss 0.1169 (0.1191)	
Pretraining:	Epoch: [20][80/235]	Loss 0.1072 (0.1187)	
Pretraining:	Epoch: [20][90/235]	Loss 0.1244 (0.1189)	
Pretraining:	Epoch: [20][100/235]	Loss 0.1156 (0.1189)	
Pretraining:	Epoch: [20][110/235]	Loss 0.1155 (0.1195)	
Pretraining:	Epoch: [20][120/235]	Loss 0.1178 (0.1198)	
Pretraining:	Epoch: [20][130/235]	Loss 0.1158 (0.1200)	
Pretraining:	Epoch: [20][140/235]	Loss 0.1159 (0.1199)	
Pretraining:	Epoch: [20][150/235]	Loss 0.1380 (0.1204)	
Pretraining:	Epoch: [20][160/235]	Loss 0.1091 (0.1202)	
Pretraining:	Epoch: [20][170/235]	Loss 0.1158 (0.1202)	
Pretraining:	Epoch: [20][180/235]	Loss 0.1278 (0.1203)	
Pretraining:	Epoch: [20][190/235]	Loss 0.1245 (0.1206)	
Pretraining:	Epoch: [20][200/235]	Loss 0.1090 (0.1208)	
Pretraining:	Epoch: [20][210/235]	Loss 0.1188 (0.1206)	
Pretraining:	Epoch: [20][220/235]	Loss 0.1215 (0.1206)	
Pretraining:	Epoch: [20][230/235]	Loss 0.1175 (0.1204)	
Pretraining:	 Loss: 0.1203

Pretraining:	Epoch 21/100
----------
Pretraining:	Epoch: [21][10/235]	Loss 0.1049 (0.1138)	
Pretraining:	Epoch: [21][20/235]	Loss 0.1214 (0.1157)	
Pretraining:	Epoch: [21][30/235]	Loss 0.1098 (0.1160)	
Pretraining:	Epoch: [21][40/235]	Loss 0.1157 (0.1173)	
Pretraining:	Epoch: [21][50/235]	Loss 0.1283 (0.1167)	
Pretraining:	Epoch: [21][60/235]	Loss 0.1203 (0.1175)	
Pretraining:	Epoch: [21][70/235]	Loss 0.1160 (0.1180)	
Pretraining:	Epoch: [21][80/235]	Loss 0.1064 (0.1176)	
Pretraining:	Epoch: [21][90/235]	Loss 0.1228 (0.1178)	
Pretraining:	Epoch: [21][100/235]	Loss 0.1136 (0.1178)	
Pretraining:	Epoch: [21][110/235]	Loss 0.1130 (0.1183)	
Pretraining:	Epoch: [21][120/235]	Loss 0.1161 (0.1186)	
Pretraining:	Epoch: [21][130/235]	Loss 0.1147 (0.1188)	
Pretraining:	Epoch: [21][140/235]	Loss 0.1145 (0.1187)	
Pretraining:	Epoch: [21][150/235]	Loss 0.1365 (0.1192)	
Pretraining:	Epoch: [21][160/235]	Loss 0.1081 (0.1190)	
Pretraining:	Epoch: [21][170/235]	Loss 0.1149 (0.1190)	
Pretraining:	Epoch: [21][180/235]	Loss 0.1270 (0.1191)	
Pretraining:	Epoch: [21][190/235]	Loss 0.1234 (0.1194)	
Pretraining:	Epoch: [21][200/235]	Loss 0.1082 (0.1196)	
Pretraining:	Epoch: [21][210/235]	Loss 0.1176 (0.1195)	
Pretraining:	Epoch: [21][220/235]	Loss 0.1206 (0.1194)	
Pretraining:	Epoch: [21][230/235]	Loss 0.1162 (0.1193)	
Pretraining:	 Loss: 0.1192

Pretraining:	Epoch 22/100
----------
Pretraining:	Epoch: [22][10/235]	Loss 0.1045 (0.1128)	
Pretraining:	Epoch: [22][20/235]	Loss 0.1202 (0.1147)	
Pretraining:	Epoch: [22][30/235]	Loss 0.1089 (0.1150)	
Pretraining:	Epoch: [22][40/235]	Loss 0.1146 (0.1162)	
Pretraining:	Epoch: [22][50/235]	Loss 0.1270 (0.1156)	
Pretraining:	Epoch: [22][60/235]	Loss 0.1192 (0.1164)	
Pretraining:	Epoch: [22][70/235]	Loss 0.1150 (0.1170)	
Pretraining:	Epoch: [22][80/235]	Loss 0.1058 (0.1166)	
Pretraining:	Epoch: [22][90/235]	Loss 0.1218 (0.1168)	
Pretraining:	Epoch: [22][100/235]	Loss 0.1116 (0.1167)	
Pretraining:	Epoch: [22][110/235]	Loss 0.1099 (0.1172)	
Pretraining:	Epoch: [22][120/235]	Loss 0.1143 (0.1175)	
Pretraining:	Epoch: [22][130/235]	Loss 0.1135 (0.1176)	
Pretraining:	Epoch: [22][140/235]	Loss 0.1141 (0.1175)	
Pretraining:	Epoch: [22][150/235]	Loss 0.1356 (0.1180)	
Pretraining:	Epoch: [22][160/235]	Loss 0.1072 (0.1179)	
Pretraining:	Epoch: [22][170/235]	Loss 0.1139 (0.1179)	
Pretraining:	Epoch: [22][180/235]	Loss 0.1259 (0.1179)	
Pretraining:	Epoch: [22][190/235]	Loss 0.1224 (0.1182)	
Pretraining:	Epoch: [22][200/235]	Loss 0.1073 (0.1184)	
Pretraining:	Epoch: [22][210/235]	Loss 0.1165 (0.1183)	
Pretraining:	Epoch: [22][220/235]	Loss 0.1198 (0.1183)	
Pretraining:	Epoch: [22][230/235]	Loss 0.1152 (0.1182)	
Pretraining:	 Loss: 0.1181

Pretraining:	Epoch 23/100
----------
Pretraining:	Epoch: [23][10/235]	Loss 0.1039 (0.1121)	
Pretraining:	Epoch: [23][20/235]	Loss 0.1194 (0.1139)	
Pretraining:	Epoch: [23][30/235]	Loss 0.1080 (0.1141)	
Pretraining:	Epoch: [23][40/235]	Loss 0.1136 (0.1153)	
Pretraining:	Epoch: [23][50/235]	Loss 0.1260 (0.1147)	
Pretraining:	Epoch: [23][60/235]	Loss 0.1180 (0.1155)	
Pretraining:	Epoch: [23][70/235]	Loss 0.1141 (0.1161)	
Pretraining:	Epoch: [23][80/235]	Loss 0.1048 (0.1157)	
Pretraining:	Epoch: [23][90/235]	Loss 0.1203 (0.1159)	
Pretraining:	Epoch: [23][100/235]	Loss 0.1095 (0.1157)	
Pretraining:	Epoch: [23][110/235]	Loss 0.1070 (0.1162)	
Pretraining:	Epoch: [23][120/235]	Loss 0.1133 (0.1164)	
Pretraining:	Epoch: [23][130/235]	Loss 0.1159 (0.1168)	
Pretraining:	Epoch: [23][140/235]	Loss 0.1165 (0.1168)	
Pretraining:	Epoch: [23][150/235]	Loss 0.1349 (0.1174)	
Pretraining:	Epoch: [23][160/235]	Loss 0.1066 (0.1173)	
Pretraining:	Epoch: [23][170/235]	Loss 0.1137 (0.1173)	
Pretraining:	Epoch: [23][180/235]	Loss 0.1252 (0.1174)	
Pretraining:	Epoch: [23][190/235]	Loss 0.1216 (0.1177)	
Pretraining:	Epoch: [23][200/235]	Loss 0.1065 (0.1179)	
Pretraining:	Epoch: [23][210/235]	Loss 0.1153 (0.1178)	
Pretraining:	Epoch: [23][220/235]	Loss 0.1189 (0.1177)	
Pretraining:	Epoch: [23][230/235]	Loss 0.1144 (0.1176)	
Pretraining:	 Loss: 0.1175

Pretraining:	Epoch 24/100
----------
Pretraining:	Epoch: [24][10/235]	Loss 0.1035 (0.1112)	
Pretraining:	Epoch: [24][20/235]	Loss 0.1190 (0.1131)	
Pretraining:	Epoch: [24][30/235]	Loss 0.1072 (0.1134)	
Pretraining:	Epoch: [24][40/235]	Loss 0.1125 (0.1146)	
Pretraining:	Epoch: [24][50/235]	Loss 0.1247 (0.1140)	
Pretraining:	Epoch: [24][60/235]	Loss 0.1172 (0.1147)	
Pretraining:	Epoch: [24][70/235]	Loss 0.1137 (0.1153)	
Pretraining:	Epoch: [24][80/235]	Loss 0.1041 (0.1149)	
Pretraining:	Epoch: [24][90/235]	Loss 0.1191 (0.1151)	
Pretraining:	Epoch: [24][100/235]	Loss 0.1100 (0.1150)	
Pretraining:	Epoch: [24][110/235]	Loss 0.1082 (0.1155)	
Pretraining:	Epoch: [24][120/235]	Loss 0.1124 (0.1158)	
Pretraining:	Epoch: [24][130/235]	Loss 0.1108 (0.1159)	
Pretraining:	Epoch: [24][140/235]	Loss 0.1120 (0.1158)	
Pretraining:	Epoch: [24][150/235]	Loss 0.1340 (0.1162)	
Pretraining:	Epoch: [24][160/235]	Loss 0.1057 (0.1161)	
Pretraining:	Epoch: [24][170/235]	Loss 0.1125 (0.1162)	
Pretraining:	Epoch: [24][180/235]	Loss 0.1242 (0.1162)	
Pretraining:	Epoch: [24][190/235]	Loss 0.1204 (0.1165)	
Pretraining:	Epoch: [24][200/235]	Loss 0.1056 (0.1167)	
Pretraining:	Epoch: [24][210/235]	Loss 0.1144 (0.1166)	
Pretraining:	Epoch: [24][220/235]	Loss 0.1182 (0.1166)	
Pretraining:	Epoch: [24][230/235]	Loss 0.1138 (0.1164)	
Pretraining:	 Loss: 0.1164

Pretraining:	Epoch 25/100
----------
Pretraining:	Epoch: [25][10/235]	Loss 0.1031 (0.1106)	
Pretraining:	Epoch: [25][20/235]	Loss 0.1179 (0.1124)	
Pretraining:	Epoch: [25][30/235]	Loss 0.1065 (0.1126)	
Pretraining:	Epoch: [25][40/235]	Loss 0.1116 (0.1137)	
Pretraining:	Epoch: [25][50/235]	Loss 0.1236 (0.1131)	
Pretraining:	Epoch: [25][60/235]	Loss 0.1162 (0.1138)	
Pretraining:	Epoch: [25][70/235]	Loss 0.1130 (0.1144)	
Pretraining:	Epoch: [25][80/235]	Loss 0.1033 (0.1140)	
Pretraining:	Epoch: [25][90/235]	Loss 0.1181 (0.1142)	
Pretraining:	Epoch: [25][100/235]	Loss 0.1083 (0.1141)	
Pretraining:	Epoch: [25][110/235]	Loss 0.1063 (0.1145)	
Pretraining:	Epoch: [25][120/235]	Loss 0.1111 (0.1147)	
Pretraining:	Epoch: [25][130/235]	Loss 0.1104 (0.1149)	
Pretraining:	Epoch: [25][140/235]	Loss 0.1117 (0.1148)	
Pretraining:	Epoch: [25][150/235]	Loss 0.1329 (0.1152)	
Pretraining:	Epoch: [25][160/235]	Loss 0.1049 (0.1151)	
Pretraining:	Epoch: [25][170/235]	Loss 0.1118 (0.1152)	
Pretraining:	Epoch: [25][180/235]	Loss 0.1231 (0.1152)	
Pretraining:	Epoch: [25][190/235]	Loss 0.1191 (0.1155)	
Pretraining:	Epoch: [25][200/235]	Loss 0.1047 (0.1157)	
Pretraining:	Epoch: [25][210/235]	Loss 0.1137 (0.1156)	
Pretraining:	Epoch: [25][220/235]	Loss 0.1173 (0.1156)	
Pretraining:	Epoch: [25][230/235]	Loss 0.1134 (0.1155)	
Pretraining:	 Loss: 0.1154

Pretraining:	Epoch 26/100
----------
Pretraining:	Epoch: [26][10/235]	Loss 0.1024 (0.1101)	
Pretraining:	Epoch: [26][20/235]	Loss 0.1173 (0.1118)	
Pretraining:	Epoch: [26][30/235]	Loss 0.1057 (0.1119)	
Pretraining:	Epoch: [26][40/235]	Loss 0.1109 (0.1130)	
Pretraining:	Epoch: [26][50/235]	Loss 0.1227 (0.1124)	
Pretraining:	Epoch: [26][60/235]	Loss 0.1153 (0.1131)	
Pretraining:	Epoch: [26][70/235]	Loss 0.1120 (0.1137)	
Pretraining:	Epoch: [26][80/235]	Loss 0.1025 (0.1133)	
Pretraining:	Epoch: [26][90/235]	Loss 0.1171 (0.1134)	
Pretraining:	Epoch: [26][100/235]	Loss 0.1067 (0.1133)	
Pretraining:	Epoch: [26][110/235]	Loss 0.1048 (0.1137)	
Pretraining:	Epoch: [26][120/235]	Loss 0.1102 (0.1139)	
Pretraining:	Epoch: [26][130/235]	Loss 0.1094 (0.1141)	
Pretraining:	Epoch: [26][140/235]	Loss 0.1106 (0.1139)	
Pretraining:	Epoch: [26][150/235]	Loss 0.1321 (0.1143)	
Pretraining:	Epoch: [26][160/235]	Loss 0.1041 (0.1142)	
Pretraining:	Epoch: [26][170/235]	Loss 0.1112 (0.1143)	
Pretraining:	Epoch: [26][180/235]	Loss 0.1222 (0.1144)	
Pretraining:	Epoch: [26][190/235]	Loss 0.1182 (0.1146)	
Pretraining:	Epoch: [26][200/235]	Loss 0.1040 (0.1149)	
Pretraining:	Epoch: [26][210/235]	Loss 0.1132 (0.1148)	
Pretraining:	Epoch: [26][220/235]	Loss 0.1164 (0.1148)	
Pretraining:	Epoch: [26][230/235]	Loss 0.1129 (0.1146)	
Pretraining:	 Loss: 0.1146

Pretraining:	Epoch 27/100
----------
Pretraining:	Epoch: [27][10/235]	Loss 0.1015 (0.1094)	
Pretraining:	Epoch: [27][20/235]	Loss 0.1167 (0.1111)	
Pretraining:	Epoch: [27][30/235]	Loss 0.1047 (0.1112)	
Pretraining:	Epoch: [27][40/235]	Loss 0.1099 (0.1123)	
Pretraining:	Epoch: [27][50/235]	Loss 0.1219 (0.1118)	
Pretraining:	Epoch: [27][60/235]	Loss 0.1149 (0.1125)	
Pretraining:	Epoch: [27][70/235]	Loss 0.1110 (0.1130)	
Pretraining:	Epoch: [27][80/235]	Loss 0.1020 (0.1126)	
Pretraining:	Epoch: [27][90/235]	Loss 0.1156 (0.1128)	
Pretraining:	Epoch: [27][100/235]	Loss 0.1063 (0.1126)	
Pretraining:	Epoch: [27][110/235]	Loss 0.1035 (0.1130)	
Pretraining:	Epoch: [27][120/235]	Loss 0.1101 (0.1132)	
Pretraining:	Epoch: [27][130/235]	Loss 0.1090 (0.1134)	
Pretraining:	Epoch: [27][140/235]	Loss 0.1102 (0.1133)	
Pretraining:	Epoch: [27][150/235]	Loss 0.1316 (0.1137)	
Pretraining:	Epoch: [27][160/235]	Loss 0.1035 (0.1136)	
Pretraining:	Epoch: [27][170/235]	Loss 0.1108 (0.1137)	
Pretraining:	Epoch: [27][180/235]	Loss 0.1215 (0.1138)	
Pretraining:	Epoch: [27][190/235]	Loss 0.1178 (0.1140)	
Pretraining:	Epoch: [27][200/235]	Loss 0.1035 (0.1143)	
Pretraining:	Epoch: [27][210/235]	Loss 0.1129 (0.1142)	
Pretraining:	Epoch: [27][220/235]	Loss 0.1159 (0.1142)	
Pretraining:	Epoch: [27][230/235]	Loss 0.1128 (0.1140)	
Pretraining:	 Loss: 0.1140

Pretraining:	Epoch 28/100
----------
Pretraining:	Epoch: [28][10/235]	Loss 0.1021 (0.1090)	
Pretraining:	Epoch: [28][20/235]	Loss 0.1165 (0.1110)	
Pretraining:	Epoch: [28][30/235]	Loss 0.1046 (0.1110)	
Pretraining:	Epoch: [28][40/235]	Loss 0.1096 (0.1121)	
Pretraining:	Epoch: [28][50/235]	Loss 0.1217 (0.1115)	
Pretraining:	Epoch: [28][60/235]	Loss 0.1140 (0.1122)	
Pretraining:	Epoch: [28][70/235]	Loss 0.1102 (0.1127)	
Pretraining:	Epoch: [28][80/235]	Loss 0.1010 (0.1123)	
Pretraining:	Epoch: [28][90/235]	Loss 0.1191 (0.1126)	
Pretraining:	Epoch: [28][100/235]	Loss 0.1067 (0.1126)	
Pretraining:	Epoch: [28][110/235]	Loss 0.1044 (0.1130)	
Pretraining:	Epoch: [28][120/235]	Loss 0.1103 (0.1133)	
Pretraining:	Epoch: [28][130/235]	Loss 0.1107 (0.1135)	
Pretraining:	Epoch: [28][140/235]	Loss 0.1093 (0.1134)	
Pretraining:	Epoch: [28][150/235]	Loss 0.1322 (0.1138)	
Pretraining:	Epoch: [28][160/235]	Loss 0.1037 (0.1138)	
Pretraining:	Epoch: [28][170/235]	Loss 0.1110 (0.1138)	
Pretraining:	Epoch: [28][180/235]	Loss 0.1215 (0.1138)	
Pretraining:	Epoch: [28][190/235]	Loss 0.1174 (0.1141)	
Pretraining:	Epoch: [28][200/235]	Loss 0.1028 (0.1143)	
Pretraining:	Epoch: [28][210/235]	Loss 0.1120 (0.1142)	
Pretraining:	Epoch: [28][220/235]	Loss 0.1155 (0.1142)	
Pretraining:	Epoch: [28][230/235]	Loss 0.1132 (0.1140)	
Pretraining:	 Loss: 0.1139

Pretraining:	Epoch 29/100
----------
Pretraining:	Epoch: [29][10/235]	Loss 0.1028 (0.1091)	
Pretraining:	Epoch: [29][20/235]	Loss 0.1178 (0.1111)	
Pretraining:	Epoch: [29][30/235]	Loss 0.1052 (0.1112)	
Pretraining:	Epoch: [29][40/235]	Loss 0.1092 (0.1124)	
Pretraining:	Epoch: [29][50/235]	Loss 0.1253 (0.1123)	
Pretraining:	Epoch: [29][60/235]	Loss 0.1158 (0.1131)	
Pretraining:	Epoch: [29][70/235]	Loss 0.1110 (0.1136)	
Pretraining:	Epoch: [29][80/235]	Loss 0.1027 (0.1131)	
Pretraining:	Epoch: [29][90/235]	Loss 0.1164 (0.1134)	
Pretraining:	Epoch: [29][100/235]	Loss 0.1131 (0.1135)	
Pretraining:	Epoch: [29][110/235]	Loss 0.1072 (0.1141)	
Pretraining:	Epoch: [29][120/235]	Loss 0.1104 (0.1144)	
Pretraining:	Epoch: [29][130/235]	Loss 0.1107 (0.1146)	
Pretraining:	Epoch: [29][140/235]	Loss 0.1097 (0.1144)	
Pretraining:	Epoch: [29][150/235]	Loss 0.1320 (0.1148)	
Pretraining:	Epoch: [29][160/235]	Loss 0.1057 (0.1147)	
Pretraining:	Epoch: [29][170/235]	Loss 0.1107 (0.1147)	
Pretraining:	Epoch: [29][180/235]	Loss 0.1228 (0.1147)	
Pretraining:	Epoch: [29][190/235]	Loss 0.1178 (0.1150)	
Pretraining:	Epoch: [29][200/235]	Loss 0.1038 (0.1152)	
Pretraining:	Epoch: [29][210/235]	Loss 0.1132 (0.1150)	
Pretraining:	Epoch: [29][220/235]	Loss 0.1156 (0.1150)	
Pretraining:	Epoch: [29][230/235]	Loss 0.1118 (0.1148)	
Pretraining:	 Loss: 0.1147

Pretraining:	Epoch 30/100
----------
Pretraining:	Epoch: [30][10/235]	Loss 0.1023 (0.1102)	
Pretraining:	Epoch: [30][20/235]	Loss 0.1184 (0.1119)	
Pretraining:	Epoch: [30][30/235]	Loss 0.1042 (0.1120)	
Pretraining:	Epoch: [30][40/235]	Loss 0.1099 (0.1129)	
Pretraining:	Epoch: [30][50/235]	Loss 0.1205 (0.1120)	
Pretraining:	Epoch: [30][60/235]	Loss 0.1128 (0.1127)	
Pretraining:	Epoch: [30][70/235]	Loss 0.1127 (0.1134)	
Pretraining:	Epoch: [30][80/235]	Loss 0.1051 (0.1131)	
Pretraining:	Epoch: [30][90/235]	Loss 0.1203 (0.1135)	
Pretraining:	Epoch: [30][100/235]	Loss 0.1054 (0.1135)	
Pretraining:	Epoch: [30][110/235]	Loss 0.1051 (0.1139)	
Pretraining:	Epoch: [30][120/235]	Loss 0.1082 (0.1140)	
Pretraining:	Epoch: [30][130/235]	Loss 0.1100 (0.1142)	
Pretraining:	Epoch: [30][140/235]	Loss 0.1102 (0.1141)	
Pretraining:	Epoch: [30][150/235]	Loss 0.1310 (0.1146)	
Pretraining:	Epoch: [30][160/235]	Loss 0.1040 (0.1145)	
Pretraining:	Epoch: [30][170/235]	Loss 0.1117 (0.1145)	
Pretraining:	Epoch: [30][180/235]	Loss 0.1219 (0.1146)	
Pretraining:	Epoch: [30][190/235]	Loss 0.1169 (0.1148)	
Pretraining:	Epoch: [30][200/235]	Loss 0.1036 (0.1149)	
Pretraining:	Epoch: [30][210/235]	Loss 0.1119 (0.1148)	
Pretraining:	Epoch: [30][220/235]	Loss 0.1147 (0.1147)	
Pretraining:	Epoch: [30][230/235]	Loss 0.1110 (0.1145)	
Pretraining:	 Loss: 0.1144

Pretraining:	Epoch 31/100
----------
Pretraining:	Epoch: [31][10/235]	Loss 0.1006 (0.1081)	
Pretraining:	Epoch: [31][20/235]	Loss 0.1148 (0.1103)	
Pretraining:	Epoch: [31][30/235]	Loss 0.1046 (0.1105)	
Pretraining:	Epoch: [31][40/235]	Loss 0.1088 (0.1115)	
Pretraining:	Epoch: [31][50/235]	Loss 0.1191 (0.1108)	
Pretraining:	Epoch: [31][60/235]	Loss 0.1127 (0.1114)	
Pretraining:	Epoch: [31][70/235]	Loss 0.1092 (0.1118)	
Pretraining:	Epoch: [31][80/235]	Loss 0.1017 (0.1114)	
Pretraining:	Epoch: [31][90/235]	Loss 0.1161 (0.1116)	
Pretraining:	Epoch: [31][100/235]	Loss 0.1052 (0.1115)	
Pretraining:	Epoch: [31][110/235]	Loss 0.1055 (0.1119)	
Pretraining:	Epoch: [31][120/235]	Loss 0.1079 (0.1122)	
Pretraining:	Epoch: [31][130/235]	Loss 0.1090 (0.1124)	
Pretraining:	Epoch: [31][140/235]	Loss 0.1093 (0.1124)	
Pretraining:	Epoch: [31][150/235]	Loss 0.1325 (0.1129)	
Pretraining:	Epoch: [31][160/235]	Loss 0.1027 (0.1129)	
Pretraining:	Epoch: [31][170/235]	Loss 0.1104 (0.1129)	
Pretraining:	Epoch: [31][180/235]	Loss 0.1216 (0.1130)	
Pretraining:	Epoch: [31][190/235]	Loss 0.1157 (0.1132)	
Pretraining:	Epoch: [31][200/235]	Loss 0.1016 (0.1134)	
Pretraining:	Epoch: [31][210/235]	Loss 0.1111 (0.1132)	
Pretraining:	Epoch: [31][220/235]	Loss 0.1134 (0.1132)	
Pretraining:	Epoch: [31][230/235]	Loss 0.1106 (0.1130)	
Pretraining:	 Loss: 0.1129

Pretraining:	Epoch 32/100
----------
Pretraining:	Epoch: [32][10/235]	Loss 0.0996 (0.1064)	
Pretraining:	Epoch: [32][20/235]	Loss 0.1133 (0.1085)	
Pretraining:	Epoch: [32][30/235]	Loss 0.1023 (0.1086)	
Pretraining:	Epoch: [32][40/235]	Loss 0.1069 (0.1096)	
Pretraining:	Epoch: [32][50/235]	Loss 0.1176 (0.1089)	
Pretraining:	Epoch: [32][60/235]	Loss 0.1117 (0.1095)	
Pretraining:	Epoch: [32][70/235]	Loss 0.1090 (0.1101)	
Pretraining:	Epoch: [32][80/235]	Loss 0.1003 (0.1097)	
Pretraining:	Epoch: [32][90/235]	Loss 0.1153 (0.1100)	
Pretraining:	Epoch: [32][100/235]	Loss 0.1044 (0.1100)	
Pretraining:	Epoch: [32][110/235]	Loss 0.1045 (0.1105)	
Pretraining:	Epoch: [32][120/235]	Loss 0.1077 (0.1108)	
Pretraining:	Epoch: [32][130/235]	Loss 0.1065 (0.1110)	
Pretraining:	Epoch: [32][140/235]	Loss 0.1079 (0.1109)	
Pretraining:	Epoch: [32][150/235]	Loss 0.1312 (0.1114)	
Pretraining:	Epoch: [32][160/235]	Loss 0.1015 (0.1114)	
Pretraining:	Epoch: [32][170/235]	Loss 0.1076 (0.1114)	
Pretraining:	Epoch: [32][180/235]	Loss 0.1206 (0.1115)	
Pretraining:	Epoch: [32][190/235]	Loss 0.1144 (0.1117)	
Pretraining:	Epoch: [32][200/235]	Loss 0.1009 (0.1119)	
Pretraining:	Epoch: [32][210/235]	Loss 0.1094 (0.1118)	
Pretraining:	Epoch: [32][220/235]	Loss 0.1125 (0.1117)	
Pretraining:	Epoch: [32][230/235]	Loss 0.1092 (0.1116)	
Pretraining:	 Loss: 0.1115

Pretraining:	Epoch 33/100
----------
Pretraining:	Epoch: [33][10/235]	Loss 0.0987 (0.1057)	
Pretraining:	Epoch: [33][20/235]	Loss 0.1126 (0.1076)	
Pretraining:	Epoch: [33][30/235]	Loss 0.1012 (0.1076)	
Pretraining:	Epoch: [33][40/235]	Loss 0.1056 (0.1086)	
Pretraining:	Epoch: [33][50/235]	Loss 0.1167 (0.1080)	
Pretraining:	Epoch: [33][60/235]	Loss 0.1109 (0.1085)	
Pretraining:	Epoch: [33][70/235]	Loss 0.1085 (0.1091)	
Pretraining:	Epoch: [33][80/235]	Loss 0.0993 (0.1088)	
Pretraining:	Epoch: [33][90/235]	Loss 0.1136 (0.1090)	
Pretraining:	Epoch: [33][100/235]	Loss 0.1030 (0.1090)	
Pretraining:	Epoch: [33][110/235]	Loss 0.1026 (0.1095)	
Pretraining:	Epoch: [33][120/235]	Loss 0.1063 (0.1097)	
Pretraining:	Epoch: [33][130/235]	Loss 0.1049 (0.1099)	
Pretraining:	Epoch: [33][140/235]	Loss 0.1074 (0.1099)	
Pretraining:	Epoch: [33][150/235]	Loss 0.1289 (0.1104)	
Pretraining:	Epoch: [33][160/235]	Loss 0.1005 (0.1103)	
Pretraining:	Epoch: [33][170/235]	Loss 0.1069 (0.1103)	
Pretraining:	Epoch: [33][180/235]	Loss 0.1195 (0.1104)	
Pretraining:	Epoch: [33][190/235]	Loss 0.1136 (0.1107)	
Pretraining:	Epoch: [33][200/235]	Loss 0.1000 (0.1108)	
Pretraining:	Epoch: [33][210/235]	Loss 0.1084 (0.1107)	
Pretraining:	Epoch: [33][220/235]	Loss 0.1117 (0.1107)	
Pretraining:	Epoch: [33][230/235]	Loss 0.1084 (0.1106)	
Pretraining:	 Loss: 0.1105

Pretraining:	Epoch 34/100
----------
Pretraining:	Epoch: [34][10/235]	Loss 0.0978 (0.1053)	
Pretraining:	Epoch: [34][20/235]	Loss 0.1125 (0.1071)	
Pretraining:	Epoch: [34][30/235]	Loss 0.1003 (0.1071)	
Pretraining:	Epoch: [34][40/235]	Loss 0.1048 (0.1079)	
Pretraining:	Epoch: [34][50/235]	Loss 0.1160 (0.1073)	
Pretraining:	Epoch: [34][60/235]	Loss 0.1099 (0.1079)	
Pretraining:	Epoch: [34][70/235]	Loss 0.1075 (0.1084)	
Pretraining:	Epoch: [34][80/235]	Loss 0.0986 (0.1081)	
Pretraining:	Epoch: [34][90/235]	Loss 0.1125 (0.1083)	
Pretraining:	Epoch: [34][100/235]	Loss 0.1021 (0.1083)	
Pretraining:	Epoch: [34][110/235]	Loss 0.1011 (0.1087)	
Pretraining:	Epoch: [34][120/235]	Loss 0.1054 (0.1090)	
Pretraining:	Epoch: [34][130/235]	Loss 0.1040 (0.1092)	
Pretraining:	Epoch: [34][140/235]	Loss 0.1062 (0.1091)	
Pretraining:	Epoch: [34][150/235]	Loss 0.1273 (0.1095)	
Pretraining:	Epoch: [34][160/235]	Loss 0.0997 (0.1094)	
Pretraining:	Epoch: [34][170/235]	Loss 0.1062 (0.1095)	
Pretraining:	Epoch: [34][180/235]	Loss 0.1183 (0.1096)	
Pretraining:	Epoch: [34][190/235]	Loss 0.1128 (0.1098)	
Pretraining:	Epoch: [34][200/235]	Loss 0.0995 (0.1100)	
Pretraining:	Epoch: [34][210/235]	Loss 0.1077 (0.1099)	
Pretraining:	Epoch: [34][220/235]	Loss 0.1112 (0.1099)	
Pretraining:	Epoch: [34][230/235]	Loss 0.1079 (0.1097)	
Pretraining:	 Loss: 0.1097

Pretraining:	Epoch 35/100
----------
Pretraining:	Epoch: [35][10/235]	Loss 0.0973 (0.1049)	
Pretraining:	Epoch: [35][20/235]	Loss 0.1121 (0.1067)	
Pretraining:	Epoch: [35][30/235]	Loss 0.0997 (0.1066)	
Pretraining:	Epoch: [35][40/235]	Loss 0.1041 (0.1074)	
Pretraining:	Epoch: [35][50/235]	Loss 0.1152 (0.1067)	
Pretraining:	Epoch: [35][60/235]	Loss 0.1094 (0.1073)	
Pretraining:	Epoch: [35][70/235]	Loss 0.1065 (0.1078)	
Pretraining:	Epoch: [35][80/235]	Loss 0.0980 (0.1074)	
Pretraining:	Epoch: [35][90/235]	Loss 0.1113 (0.1077)	
Pretraining:	Epoch: [35][100/235]	Loss 0.1011 (0.1076)	
Pretraining:	Epoch: [35][110/235]	Loss 0.1000 (0.1080)	
Pretraining:	Epoch: [35][120/235]	Loss 0.1044 (0.1083)	
Pretraining:	Epoch: [35][130/235]	Loss 0.1036 (0.1085)	
Pretraining:	Epoch: [35][140/235]	Loss 0.1051 (0.1083)	
Pretraining:	Epoch: [35][150/235]	Loss 0.1265 (0.1088)	
Pretraining:	Epoch: [35][160/235]	Loss 0.0991 (0.1087)	
Pretraining:	Epoch: [35][170/235]	Loss 0.1052 (0.1087)	
Pretraining:	Epoch: [35][180/235]	Loss 0.1173 (0.1088)	
Pretraining:	Epoch: [35][190/235]	Loss 0.1119 (0.1090)	
Pretraining:	Epoch: [35][200/235]	Loss 0.0990 (0.1092)	
Pretraining:	Epoch: [35][210/235]	Loss 0.1071 (0.1091)	
Pretraining:	Epoch: [35][220/235]	Loss 0.1106 (0.1091)	
Pretraining:	Epoch: [35][230/235]	Loss 0.1076 (0.1090)	
Pretraining:	 Loss: 0.1089

Pretraining:	Epoch 36/100
----------
Pretraining:	Epoch: [36][10/235]	Loss 0.0967 (0.1042)	
Pretraining:	Epoch: [36][20/235]	Loss 0.1118 (0.1061)	
Pretraining:	Epoch: [36][30/235]	Loss 0.0989 (0.1060)	
Pretraining:	Epoch: [36][40/235]	Loss 0.1034 (0.1068)	
Pretraining:	Epoch: [36][50/235]	Loss 0.1146 (0.1061)	
Pretraining:	Epoch: [36][60/235]	Loss 0.1087 (0.1067)	
Pretraining:	Epoch: [36][70/235]	Loss 0.1062 (0.1072)	
Pretraining:	Epoch: [36][80/235]	Loss 0.0971 (0.1068)	
Pretraining:	Epoch: [36][90/235]	Loss 0.1106 (0.1070)	
Pretraining:	Epoch: [36][100/235]	Loss 0.1002 (0.1069)	
Pretraining:	Epoch: [36][110/235]	Loss 0.0995 (0.1074)	
Pretraining:	Epoch: [36][120/235]	Loss 0.1038 (0.1076)	
Pretraining:	Epoch: [36][130/235]	Loss 0.1031 (0.1078)	
Pretraining:	Epoch: [36][140/235]	Loss 0.1040 (0.1076)	
Pretraining:	Epoch: [36][150/235]	Loss 0.1258 (0.1080)	
Pretraining:	Epoch: [36][160/235]	Loss 0.0985 (0.1079)	
Pretraining:	Epoch: [36][170/235]	Loss 0.1047 (0.1079)	
Pretraining:	Epoch: [36][180/235]	Loss 0.1166 (0.1080)	
Pretraining:	Epoch: [36][190/235]	Loss 0.1111 (0.1083)	
Pretraining:	Epoch: [36][200/235]	Loss 0.0984 (0.1085)	
Pretraining:	Epoch: [36][210/235]	Loss 0.1065 (0.1084)	
Pretraining:	Epoch: [36][220/235]	Loss 0.1098 (0.1084)	
Pretraining:	Epoch: [36][230/235]	Loss 0.1066 (0.1083)	
Pretraining:	 Loss: 0.1082

Pretraining:	Epoch 37/100
----------
Pretraining:	Epoch: [37][10/235]	Loss 0.0958 (0.1027)	
Pretraining:	Epoch: [37][20/235]	Loss 0.1112 (0.1047)	
Pretraining:	Epoch: [37][30/235]	Loss 0.0981 (0.1048)	
Pretraining:	Epoch: [37][40/235]	Loss 0.1028 (0.1057)	
Pretraining:	Epoch: [37][50/235]	Loss 0.1139 (0.1051)	
Pretraining:	Epoch: [37][60/235]	Loss 0.1081 (0.1058)	
Pretraining:	Epoch: [37][70/235]	Loss 0.1052 (0.1063)	
Pretraining:	Epoch: [37][80/235]	Loss 0.0965 (0.1059)	
Pretraining:	Epoch: [37][90/235]	Loss 0.1096 (0.1061)	
Pretraining:	Epoch: [37][100/235]	Loss 0.0989 (0.1060)	
Pretraining:	Epoch: [37][110/235]	Loss 0.0971 (0.1064)	
Pretraining:	Epoch: [37][120/235]	Loss 0.1031 (0.1067)	
Pretraining:	Epoch: [37][130/235]	Loss 0.1027 (0.1068)	
Pretraining:	Epoch: [37][140/235]	Loss 0.1031 (0.1067)	
Pretraining:	Epoch: [37][150/235]	Loss 0.1253 (0.1071)	
Pretraining:	Epoch: [37][160/235]	Loss 0.0982 (0.1070)	
Pretraining:	Epoch: [37][170/235]	Loss 0.1043 (0.1070)	
Pretraining:	Epoch: [37][180/235]	Loss 0.1156 (0.1071)	
Pretraining:	Epoch: [37][190/235]	Loss 0.1106 (0.1074)	
Pretraining:	Epoch: [37][200/235]	Loss 0.0978 (0.1076)	
Pretraining:	Epoch: [37][210/235]	Loss 0.1059 (0.1075)	
Pretraining:	Epoch: [37][220/235]	Loss 0.1094 (0.1075)	
Pretraining:	Epoch: [37][230/235]	Loss 0.1062 (0.1074)	
Pretraining:	 Loss: 0.1074

Pretraining:	Epoch 38/100
----------
Pretraining:	Epoch: [38][10/235]	Loss 0.0952 (0.1019)	
Pretraining:	Epoch: [38][20/235]	Loss 0.1102 (0.1039)	
Pretraining:	Epoch: [38][30/235]	Loss 0.0975 (0.1040)	
Pretraining:	Epoch: [38][40/235]	Loss 0.1021 (0.1050)	
Pretraining:	Epoch: [38][50/235]	Loss 0.1136 (0.1044)	
Pretraining:	Epoch: [38][60/235]	Loss 0.1076 (0.1051)	
Pretraining:	Epoch: [38][70/235]	Loss 0.1045 (0.1056)	
Pretraining:	Epoch: [38][80/235]	Loss 0.0958 (0.1053)	
Pretraining:	Epoch: [38][90/235]	Loss 0.1087 (0.1054)	
Pretraining:	Epoch: [38][100/235]	Loss 0.0982 (0.1053)	
Pretraining:	Epoch: [38][110/235]	Loss 0.0962 (0.1057)	
Pretraining:	Epoch: [38][120/235]	Loss 0.1023 (0.1059)	
Pretraining:	Epoch: [38][130/235]	Loss 0.1019 (0.1061)	
Pretraining:	Epoch: [38][140/235]	Loss 0.1027 (0.1059)	
Pretraining:	Epoch: [38][150/235]	Loss 0.1250 (0.1064)	
Pretraining:	Epoch: [38][160/235]	Loss 0.0978 (0.1063)	
Pretraining:	Epoch: [38][170/235]	Loss 0.1040 (0.1063)	
Pretraining:	Epoch: [38][180/235]	Loss 0.1151 (0.1065)	
Pretraining:	Epoch: [38][190/235]	Loss 0.1104 (0.1067)	
Pretraining:	Epoch: [38][200/235]	Loss 0.0972 (0.1070)	
Pretraining:	Epoch: [38][210/235]	Loss 0.1055 (0.1069)	
Pretraining:	Epoch: [38][220/235]	Loss 0.1089 (0.1069)	
Pretraining:	Epoch: [38][230/235]	Loss 0.1059 (0.1068)	
Pretraining:	 Loss: 0.1067

Pretraining:	Epoch 39/100
----------
Pretraining:	Epoch: [39][10/235]	Loss 0.0949 (0.1016)	
Pretraining:	Epoch: [39][20/235]	Loss 0.1093 (0.1036)	
Pretraining:	Epoch: [39][30/235]	Loss 0.0969 (0.1036)	
Pretraining:	Epoch: [39][40/235]	Loss 0.1017 (0.1045)	
Pretraining:	Epoch: [39][50/235]	Loss 0.1133 (0.1040)	
Pretraining:	Epoch: [39][60/235]	Loss 0.1070 (0.1046)	
Pretraining:	Epoch: [39][70/235]	Loss 0.1042 (0.1052)	
Pretraining:	Epoch: [39][80/235]	Loss 0.0953 (0.1048)	
Pretraining:	Epoch: [39][90/235]	Loss 0.1079 (0.1049)	
Pretraining:	Epoch: [39][100/235]	Loss 0.0976 (0.1048)	
Pretraining:	Epoch: [39][110/235]	Loss 0.0955 (0.1052)	
Pretraining:	Epoch: [39][120/235]	Loss 0.1019 (0.1054)	
Pretraining:	Epoch: [39][130/235]	Loss 0.1016 (0.1056)	
Pretraining:	Epoch: [39][140/235]	Loss 0.1023 (0.1055)	
Pretraining:	Epoch: [39][150/235]	Loss 0.1246 (0.1059)	
Pretraining:	Epoch: [39][160/235]	Loss 0.0978 (0.1058)	
Pretraining:	Epoch: [39][170/235]	Loss 0.1037 (0.1059)	
Pretraining:	Epoch: [39][180/235]	Loss 0.1144 (0.1060)	
Pretraining:	Epoch: [39][190/235]	Loss 0.1100 (0.1063)	
Pretraining:	Epoch: [39][200/235]	Loss 0.0968 (0.1065)	
Pretraining:	Epoch: [39][210/235]	Loss 0.1053 (0.1065)	
Pretraining:	Epoch: [39][220/235]	Loss 0.1085 (0.1065)	
Pretraining:	Epoch: [39][230/235]	Loss 0.1056 (0.1064)	
Pretraining:	 Loss: 0.1063

Pretraining:	Epoch 40/100
----------
Pretraining:	Epoch: [40][10/235]	Loss 0.0946 (0.1014)	
Pretraining:	Epoch: [40][20/235]	Loss 0.1090 (0.1032)	
Pretraining:	Epoch: [40][30/235]	Loss 0.0965 (0.1033)	
Pretraining:	Epoch: [40][40/235]	Loss 0.1013 (0.1042)	
Pretraining:	Epoch: [40][50/235]	Loss 0.1127 (0.1036)	
Pretraining:	Epoch: [40][60/235]	Loss 0.1065 (0.1042)	
Pretraining:	Epoch: [40][70/235]	Loss 0.1037 (0.1048)	
Pretraining:	Epoch: [40][80/235]	Loss 0.0950 (0.1045)	
Pretraining:	Epoch: [40][90/235]	Loss 0.1075 (0.1046)	
Pretraining:	Epoch: [40][100/235]	Loss 0.0972 (0.1045)	
Pretraining:	Epoch: [40][110/235]	Loss 0.0951 (0.1048)	
Pretraining:	Epoch: [40][120/235]	Loss 0.1011 (0.1051)	
Pretraining:	Epoch: [40][130/235]	Loss 0.1009 (0.1052)	
Pretraining:	Epoch: [40][140/235]	Loss 0.1018 (0.1051)	
Pretraining:	Epoch: [40][150/235]	Loss 0.1247 (0.1055)	
Pretraining:	Epoch: [40][160/235]	Loss 0.0977 (0.1054)	
Pretraining:	Epoch: [40][170/235]	Loss 0.1033 (0.1055)	
Pretraining:	Epoch: [40][180/235]	Loss 0.1141 (0.1056)	
Pretraining:	Epoch: [40][190/235]	Loss 0.1099 (0.1059)	
Pretraining:	Epoch: [40][200/235]	Loss 0.0966 (0.1062)	
Pretraining:	Epoch: [40][210/235]	Loss 0.1048 (0.1061)	
Pretraining:	Epoch: [40][220/235]	Loss 0.1082 (0.1061)	
Pretraining:	Epoch: [40][230/235]	Loss 0.1056 (0.1061)	
Pretraining:	 Loss: 0.1060

Pretraining:	Epoch 41/100
----------
Pretraining:	Epoch: [41][10/235]	Loss 0.0949 (0.1013)	
Pretraining:	Epoch: [41][20/235]	Loss 0.1085 (0.1031)	
Pretraining:	Epoch: [41][30/235]	Loss 0.0964 (0.1031)	
Pretraining:	Epoch: [41][40/235]	Loss 0.1012 (0.1040)	
Pretraining:	Epoch: [41][50/235]	Loss 0.1123 (0.1034)	
Pretraining:	Epoch: [41][60/235]	Loss 0.1063 (0.1040)	
Pretraining:	Epoch: [41][70/235]	Loss 0.1030 (0.1045)	
Pretraining:	Epoch: [41][80/235]	Loss 0.0946 (0.1042)	
Pretraining:	Epoch: [41][90/235]	Loss 0.1078 (0.1043)	
Pretraining:	Epoch: [41][100/235]	Loss 0.0970 (0.1042)	
Pretraining:	Epoch: [41][110/235]	Loss 0.0949 (0.1046)	
Pretraining:	Epoch: [41][120/235]	Loss 0.1012 (0.1049)	
Pretraining:	Epoch: [41][130/235]	Loss 0.1036 (0.1052)	
Pretraining:	Epoch: [41][140/235]	Loss 0.1023 (0.1051)	
Pretraining:	Epoch: [41][150/235]	Loss 0.1250 (0.1055)	
Pretraining:	Epoch: [41][160/235]	Loss 0.0972 (0.1055)	
Pretraining:	Epoch: [41][170/235]	Loss 0.1030 (0.1055)	
Pretraining:	Epoch: [41][180/235]	Loss 0.1138 (0.1056)	
Pretraining:	Epoch: [41][190/235]	Loss 0.1088 (0.1059)	
Pretraining:	Epoch: [41][200/235]	Loss 0.0974 (0.1061)	
Pretraining:	Epoch: [41][210/235]	Loss 0.1046 (0.1060)	
Pretraining:	Epoch: [41][220/235]	Loss 0.1078 (0.1060)	
Pretraining:	Epoch: [41][230/235]	Loss 0.1055 (0.1060)	
Pretraining:	 Loss: 0.1059

Pretraining:	Epoch 42/100
----------
Pretraining:	Epoch: [42][10/235]	Loss 0.0952 (0.1010)	
Pretraining:	Epoch: [42][20/235]	Loss 0.1082 (0.1029)	
Pretraining:	Epoch: [42][30/235]	Loss 0.0960 (0.1029)	
Pretraining:	Epoch: [42][40/235]	Loss 0.1008 (0.1038)	
Pretraining:	Epoch: [42][50/235]	Loss 0.1134 (0.1035)	
Pretraining:	Epoch: [42][60/235]	Loss 0.1056 (0.1040)	
Pretraining:	Epoch: [42][70/235]	Loss 0.1028 (0.1045)	
Pretraining:	Epoch: [42][80/235]	Loss 0.0950 (0.1041)	
Pretraining:	Epoch: [42][90/235]	Loss 0.1067 (0.1042)	
Pretraining:	Epoch: [42][100/235]	Loss 0.0970 (0.1041)	
Pretraining:	Epoch: [42][110/235]	Loss 0.0952 (0.1044)	
Pretraining:	Epoch: [42][120/235]	Loss 0.1010 (0.1047)	
Pretraining:	Epoch: [42][130/235]	Loss 0.1012 (0.1049)	
Pretraining:	Epoch: [42][140/235]	Loss 0.1012 (0.1048)	
Pretraining:	Epoch: [42][150/235]	Loss 0.1238 (0.1052)	
Pretraining:	Epoch: [42][160/235]	Loss 0.0969 (0.1051)	
Pretraining:	Epoch: [42][170/235]	Loss 0.1025 (0.1051)	
Pretraining:	Epoch: [42][180/235]	Loss 0.1133 (0.1052)	
Pretraining:	Epoch: [42][190/235]	Loss 0.1082 (0.1055)	
Pretraining:	Epoch: [42][200/235]	Loss 0.0971 (0.1057)	
Pretraining:	Epoch: [42][210/235]	Loss 0.1047 (0.1056)	
Pretraining:	Epoch: [42][220/235]	Loss 0.1071 (0.1056)	
Pretraining:	Epoch: [42][230/235]	Loss 0.1051 (0.1055)	
Pretraining:	 Loss: 0.1054

Pretraining:	Epoch 43/100
----------
Pretraining:	Epoch: [43][10/235]	Loss 0.0956 (0.1007)	
Pretraining:	Epoch: [43][20/235]	Loss 0.1084 (0.1029)	
Pretraining:	Epoch: [43][30/235]	Loss 0.0956 (0.1028)	
Pretraining:	Epoch: [43][40/235]	Loss 0.1012 (0.1038)	
Pretraining:	Epoch: [43][50/235]	Loss 0.1128 (0.1034)	
Pretraining:	Epoch: [43][60/235]	Loss 0.1049 (0.1039)	
Pretraining:	Epoch: [43][70/235]	Loss 0.1022 (0.1043)	
Pretraining:	Epoch: [43][80/235]	Loss 0.0946 (0.1038)	
Pretraining:	Epoch: [43][90/235]	Loss 0.1069 (0.1040)	
Pretraining:	Epoch: [43][100/235]	Loss 0.0962 (0.1038)	
Pretraining:	Epoch: [43][110/235]	Loss 0.0949 (0.1042)	
Pretraining:	Epoch: [43][120/235]	Loss 0.0998 (0.1044)	
Pretraining:	Epoch: [43][130/235]	Loss 0.1011 (0.1046)	
Pretraining:	Epoch: [43][140/235]	Loss 0.1015 (0.1045)	
Pretraining:	Epoch: [43][150/235]	Loss 0.1246 (0.1050)	
Pretraining:	Epoch: [43][160/235]	Loss 0.0974 (0.1049)	
Pretraining:	Epoch: [43][170/235]	Loss 0.1020 (0.1049)	
Pretraining:	Epoch: [43][180/235]	Loss 0.1132 (0.1051)	
Pretraining:	Epoch: [43][190/235]	Loss 0.1092 (0.1053)	
Pretraining:	Epoch: [43][200/235]	Loss 0.0961 (0.1056)	
Pretraining:	Epoch: [43][210/235]	Loss 0.1042 (0.1055)	
Pretraining:	Epoch: [43][220/235]	Loss 0.1075 (0.1055)	
Pretraining:	Epoch: [43][230/235]	Loss 0.1044 (0.1054)	
Pretraining:	 Loss: 0.1053

Pretraining:	Epoch 44/100
----------
Pretraining:	Epoch: [44][10/235]	Loss 0.0941 (0.1001)	
Pretraining:	Epoch: [44][20/235]	Loss 0.1087 (0.1023)	
Pretraining:	Epoch: [44][30/235]	Loss 0.0952 (0.1024)	
Pretraining:	Epoch: [44][40/235]	Loss 0.1008 (0.1033)	
Pretraining:	Epoch: [44][50/235]	Loss 0.1109 (0.1029)	
Pretraining:	Epoch: [44][60/235]	Loss 0.1063 (0.1036)	
Pretraining:	Epoch: [44][70/235]	Loss 0.1023 (0.1042)	
Pretraining:	Epoch: [44][80/235]	Loss 0.0940 (0.1037)	
Pretraining:	Epoch: [44][90/235]	Loss 0.1059 (0.1038)	
Pretraining:	Epoch: [44][100/235]	Loss 0.0967 (0.1036)	
Pretraining:	Epoch: [44][110/235]	Loss 0.1005 (0.1042)	
Pretraining:	Epoch: [44][120/235]	Loss 0.1010 (0.1046)	
Pretraining:	Epoch: [44][130/235]	Loss 0.1026 (0.1048)	
Pretraining:	Epoch: [44][140/235]	Loss 0.1021 (0.1048)	
Pretraining:	Epoch: [44][150/235]	Loss 0.1243 (0.1052)	
Pretraining:	Epoch: [44][160/235]	Loss 0.0976 (0.1052)	
Pretraining:	Epoch: [44][170/235]	Loss 0.1019 (0.1052)	
Pretraining:	Epoch: [44][180/235]	Loss 0.1133 (0.1053)	
Pretraining:	Epoch: [44][190/235]	Loss 0.1086 (0.1056)	
Pretraining:	Epoch: [44][200/235]	Loss 0.0957 (0.1058)	
Pretraining:	Epoch: [44][210/235]	Loss 0.1042 (0.1057)	
Pretraining:	Epoch: [44][220/235]	Loss 0.1068 (0.1057)	
Pretraining:	Epoch: [44][230/235]	Loss 0.1048 (0.1056)	
Pretraining:	 Loss: 0.1055

Pretraining:	Epoch 45/100
----------
Pretraining:	Epoch: [45][10/235]	Loss 0.0956 (0.1011)	
Pretraining:	Epoch: [45][20/235]	Loss 0.1124 (0.1037)	
Pretraining:	Epoch: [45][30/235]	Loss 0.0954 (0.1042)	
Pretraining:	Epoch: [45][40/235]	Loss 0.1007 (0.1050)	
Pretraining:	Epoch: [45][50/235]	Loss 0.1111 (0.1042)	
Pretraining:	Epoch: [45][60/235]	Loss 0.1051 (0.1046)	
Pretraining:	Epoch: [45][70/235]	Loss 0.1037 (0.1051)	
Pretraining:	Epoch: [45][80/235]	Loss 0.0949 (0.1047)	
Pretraining:	Epoch: [45][90/235]	Loss 0.1078 (0.1049)	
Pretraining:	Epoch: [45][100/235]	Loss 0.0974 (0.1049)	
Pretraining:	Epoch: [45][110/235]	Loss 0.0965 (0.1053)	
Pretraining:	Epoch: [45][120/235]	Loss 0.1011 (0.1054)	
Pretraining:	Epoch: [45][130/235]	Loss 0.1017 (0.1056)	
Pretraining:	Epoch: [45][140/235]	Loss 0.1038 (0.1056)	
Pretraining:	Epoch: [45][150/235]	Loss 0.1265 (0.1062)	
Pretraining:	Epoch: [45][160/235]	Loss 0.0980 (0.1061)	
Pretraining:	Epoch: [45][170/235]	Loss 0.1050 (0.1062)	
Pretraining:	Epoch: [45][180/235]	Loss 0.1145 (0.1063)	
Pretraining:	Epoch: [45][190/235]	Loss 0.1077 (0.1065)	
Pretraining:	Epoch: [45][200/235]	Loss 0.0961 (0.1066)	
Pretraining:	Epoch: [45][210/235]	Loss 0.1049 (0.1065)	
Pretraining:	Epoch: [45][220/235]	Loss 0.1088 (0.1066)	
Pretraining:	Epoch: [45][230/235]	Loss 0.1045 (0.1064)	
Pretraining:	 Loss: 0.1064

Pretraining:	Epoch 46/100
----------
Pretraining:	Epoch: [46][10/235]	Loss 0.0946 (0.1005)	
Pretraining:	Epoch: [46][20/235]	Loss 0.1074 (0.1026)	
Pretraining:	Epoch: [46][30/235]	Loss 0.0971 (0.1028)	
Pretraining:	Epoch: [46][40/235]	Loss 0.1005 (0.1038)	
Pretraining:	Epoch: [46][50/235]	Loss 0.1101 (0.1033)	
Pretraining:	Epoch: [46][60/235]	Loss 0.1051 (0.1038)	
Pretraining:	Epoch: [46][70/235]	Loss 0.1024 (0.1042)	
Pretraining:	Epoch: [46][80/235]	Loss 0.0936 (0.1038)	
Pretraining:	Epoch: [46][90/235]	Loss 0.1078 (0.1040)	
Pretraining:	Epoch: [46][100/235]	Loss 0.0961 (0.1039)	
Pretraining:	Epoch: [46][110/235]	Loss 0.0969 (0.1043)	
Pretraining:	Epoch: [46][120/235]	Loss 0.1006 (0.1045)	
Pretraining:	Epoch: [46][130/235]	Loss 0.1031 (0.1048)	
Pretraining:	Epoch: [46][140/235]	Loss 0.1051 (0.1049)	
Pretraining:	Epoch: [46][150/235]	Loss 0.1262 (0.1053)	
Pretraining:	Epoch: [46][160/235]	Loss 0.0974 (0.1053)	
Pretraining:	Epoch: [46][170/235]	Loss 0.1030 (0.1054)	
Pretraining:	Epoch: [46][180/235]	Loss 0.1138 (0.1055)	
Pretraining:	Epoch: [46][190/235]	Loss 0.1074 (0.1057)	
Pretraining:	Epoch: [46][200/235]	Loss 0.0953 (0.1059)	
Pretraining:	Epoch: [46][210/235]	Loss 0.1035 (0.1057)	
Pretraining:	Epoch: [46][220/235]	Loss 0.1085 (0.1057)	
Pretraining:	Epoch: [46][230/235]	Loss 0.1036 (0.1056)	
Pretraining:	 Loss: 0.1055

Pretraining:	Epoch 47/100
----------
Pretraining:	Epoch: [47][10/235]	Loss 0.0944 (0.1007)	
Pretraining:	Epoch: [47][20/235]	Loss 0.1069 (0.1025)	
Pretraining:	Epoch: [47][30/235]	Loss 0.0961 (0.1025)	
Pretraining:	Epoch: [47][40/235]	Loss 0.1009 (0.1035)	
Pretraining:	Epoch: [47][50/235]	Loss 0.1108 (0.1030)	
Pretraining:	Epoch: [47][60/235]	Loss 0.1051 (0.1035)	
Pretraining:	Epoch: [47][70/235]	Loss 0.1039 (0.1041)	
Pretraining:	Epoch: [47][80/235]	Loss 0.0946 (0.1038)	
Pretraining:	Epoch: [47][90/235]	Loss 0.1074 (0.1041)	
Pretraining:	Epoch: [47][100/235]	Loss 0.0975 (0.1041)	
Pretraining:	Epoch: [47][110/235]	Loss 0.0976 (0.1046)	
Pretraining:	Epoch: [47][120/235]	Loss 0.1031 (0.1049)	
Pretraining:	Epoch: [47][130/235]	Loss 0.1029 (0.1052)	
Pretraining:	Epoch: [47][140/235]	Loss 0.1062 (0.1052)	
Pretraining:	Epoch: [47][150/235]	Loss 0.1255 (0.1058)	
Pretraining:	Epoch: [47][160/235]	Loss 0.0971 (0.1058)	
Pretraining:	Epoch: [47][170/235]	Loss 0.1031 (0.1059)	
Pretraining:	Epoch: [47][180/235]	Loss 0.1142 (0.1060)	
Pretraining:	Epoch: [47][190/235]	Loss 0.1077 (0.1062)	
Pretraining:	Epoch: [47][200/235]	Loss 0.0958 (0.1063)	
Pretraining:	Epoch: [47][210/235]	Loss 0.1039 (0.1062)	
Pretraining:	Epoch: [47][220/235]	Loss 0.1079 (0.1062)	
Pretraining:	Epoch: [47][230/235]	Loss 0.1034 (0.1060)	
Pretraining:	 Loss: 0.1059

Pretraining:	Epoch 48/100
----------
Pretraining:	Epoch: [48][10/235]	Loss 0.0948 (0.1006)	
Pretraining:	Epoch: [48][20/235]	Loss 0.1067 (0.1026)	
Pretraining:	Epoch: [48][30/235]	Loss 0.0963 (0.1027)	
Pretraining:	Epoch: [48][40/235]	Loss 0.1024 (0.1038)	
Pretraining:	Epoch: [48][50/235]	Loss 0.1114 (0.1032)	
Pretraining:	Epoch: [48][60/235]	Loss 0.1047 (0.1036)	
Pretraining:	Epoch: [48][70/235]	Loss 0.1024 (0.1041)	
Pretraining:	Epoch: [48][80/235]	Loss 0.0965 (0.1039)	
Pretraining:	Epoch: [48][90/235]	Loss 0.1093 (0.1042)	
Pretraining:	Epoch: [48][100/235]	Loss 0.0997 (0.1044)	
Pretraining:	Epoch: [48][110/235]	Loss 0.0969 (0.1049)	
Pretraining:	Epoch: [48][120/235]	Loss 0.1014 (0.1051)	
Pretraining:	Epoch: [48][130/235]	Loss 0.1024 (0.1054)	
Pretraining:	Epoch: [48][140/235]	Loss 0.1048 (0.1054)	
Pretraining:	Epoch: [48][150/235]	Loss 0.1246 (0.1058)	
Pretraining:	Epoch: [48][160/235]	Loss 0.0967 (0.1057)	
Pretraining:	Epoch: [48][170/235]	Loss 0.1027 (0.1057)	
Pretraining:	Epoch: [48][180/235]	Loss 0.1136 (0.1058)	
Pretraining:	Epoch: [48][190/235]	Loss 0.1073 (0.1059)	
Pretraining:	Epoch: [48][200/235]	Loss 0.0950 (0.1061)	
Pretraining:	Epoch: [48][210/235]	Loss 0.1042 (0.1059)	
Pretraining:	Epoch: [48][220/235]	Loss 0.1077 (0.1059)	
Pretraining:	Epoch: [48][230/235]	Loss 0.1035 (0.1058)	
Pretraining:	 Loss: 0.1057

Pretraining:	Epoch 49/100
----------
Pretraining:	Epoch: [49][10/235]	Loss 0.0946 (0.0997)	
Pretraining:	Epoch: [49][20/235]	Loss 0.1064 (0.1017)	
Pretraining:	Epoch: [49][30/235]	Loss 0.0958 (0.1019)	
Pretraining:	Epoch: [49][40/235]	Loss 0.1009 (0.1029)	
Pretraining:	Epoch: [49][50/235]	Loss 0.1100 (0.1022)	
Pretraining:	Epoch: [49][60/235]	Loss 0.1032 (0.1026)	
Pretraining:	Epoch: [49][70/235]	Loss 0.1006 (0.1031)	
Pretraining:	Epoch: [49][80/235]	Loss 0.0947 (0.1027)	
Pretraining:	Epoch: [49][90/235]	Loss 0.1076 (0.1029)	
Pretraining:	Epoch: [49][100/235]	Loss 0.0978 (0.1029)	
Pretraining:	Epoch: [49][110/235]	Loss 0.0954 (0.1034)	
Pretraining:	Epoch: [49][120/235]	Loss 0.1001 (0.1037)	
Pretraining:	Epoch: [49][130/235]	Loss 0.1006 (0.1039)	
Pretraining:	Epoch: [49][140/235]	Loss 0.1037 (0.1039)	
Pretraining:	Epoch: [49][150/235]	Loss 0.1229 (0.1043)	
Pretraining:	Epoch: [49][160/235]	Loss 0.0963 (0.1043)	
Pretraining:	Epoch: [49][170/235]	Loss 0.1015 (0.1043)	
Pretraining:	Epoch: [49][180/235]	Loss 0.1125 (0.1044)	
Pretraining:	Epoch: [49][190/235]	Loss 0.1066 (0.1046)	
Pretraining:	Epoch: [49][200/235]	Loss 0.0942 (0.1047)	
Pretraining:	Epoch: [49][210/235]	Loss 0.1034 (0.1046)	
Pretraining:	Epoch: [49][220/235]	Loss 0.1066 (0.1046)	
Pretraining:	Epoch: [49][230/235]	Loss 0.1028 (0.1046)	
Pretraining:	 Loss: 0.1045

Pretraining:	Epoch 50/100
----------
Pretraining:	Epoch: [50][10/235]	Loss 0.0940 (0.0991)	
Pretraining:	Epoch: [50][20/235]	Loss 0.1057 (0.1010)	
Pretraining:	Epoch: [50][30/235]	Loss 0.0951 (0.1011)	
Pretraining:	Epoch: [50][40/235]	Loss 0.0992 (0.1020)	
Pretraining:	Epoch: [50][50/235]	Loss 0.1090 (0.1013)	
Pretraining:	Epoch: [50][60/235]	Loss 0.1023 (0.1017)	
Pretraining:	Epoch: [50][70/235]	Loss 0.0998 (0.1022)	
Pretraining:	Epoch: [50][80/235]	Loss 0.0937 (0.1019)	
Pretraining:	Epoch: [50][90/235]	Loss 0.1063 (0.1020)	
Pretraining:	Epoch: [50][100/235]	Loss 0.0960 (0.1020)	
Pretraining:	Epoch: [50][110/235]	Loss 0.0937 (0.1024)	
Pretraining:	Epoch: [50][120/235]	Loss 0.0991 (0.1027)	
Pretraining:	Epoch: [50][130/235]	Loss 0.0989 (0.1029)	
Pretraining:	Epoch: [50][140/235]	Loss 0.1024 (0.1028)	
Pretraining:	Epoch: [50][150/235]	Loss 0.1214 (0.1032)	
Pretraining:	Epoch: [50][160/235]	Loss 0.0958 (0.1032)	
Pretraining:	Epoch: [50][170/235]	Loss 0.1006 (0.1032)	
Pretraining:	Epoch: [50][180/235]	Loss 0.1116 (0.1033)	
Pretraining:	Epoch: [50][190/235]	Loss 0.1060 (0.1036)	
Pretraining:	Epoch: [50][200/235]	Loss 0.0935 (0.1038)	
Pretraining:	Epoch: [50][210/235]	Loss 0.1025 (0.1037)	
Pretraining:	Epoch: [50][220/235]	Loss 0.1055 (0.1037)	
Pretraining:	Epoch: [50][230/235]	Loss 0.1022 (0.1036)	
Pretraining:	 Loss: 0.1035

Pretraining:	Epoch 51/100
----------
Pretraining:	Epoch: [51][10/235]	Loss 0.0932 (0.0982)	
Pretraining:	Epoch: [51][20/235]	Loss 0.1049 (0.1001)	
Pretraining:	Epoch: [51][30/235]	Loss 0.0941 (0.1002)	
Pretraining:	Epoch: [51][40/235]	Loss 0.0982 (0.1011)	
Pretraining:	Epoch: [51][50/235]	Loss 0.1081 (0.1005)	
Pretraining:	Epoch: [51][60/235]	Loss 0.1016 (0.1009)	
Pretraining:	Epoch: [51][70/235]	Loss 0.0992 (0.1014)	
Pretraining:	Epoch: [51][80/235]	Loss 0.0927 (0.1011)	
Pretraining:	Epoch: [51][90/235]	Loss 0.1052 (0.1012)	
Pretraining:	Epoch: [51][100/235]	Loss 0.0942 (0.1011)	
Pretraining:	Epoch: [51][110/235]	Loss 0.0928 (0.1016)	
Pretraining:	Epoch: [51][120/235]	Loss 0.0979 (0.1018)	
Pretraining:	Epoch: [51][130/235]	Loss 0.0975 (0.1020)	
Pretraining:	Epoch: [51][140/235]	Loss 0.1005 (0.1018)	
Pretraining:	Epoch: [51][150/235]	Loss 0.1205 (0.1022)	
Pretraining:	Epoch: [51][160/235]	Loss 0.0950 (0.1022)	
Pretraining:	Epoch: [51][170/235]	Loss 0.0999 (0.1022)	
Pretraining:	Epoch: [51][180/235]	Loss 0.1107 (0.1023)	
Pretraining:	Epoch: [51][190/235]	Loss 0.1052 (0.1026)	
Pretraining:	Epoch: [51][200/235]	Loss 0.0930 (0.1028)	
Pretraining:	Epoch: [51][210/235]	Loss 0.1016 (0.1027)	
Pretraining:	Epoch: [51][220/235]	Loss 0.1048 (0.1027)	
Pretraining:	Epoch: [51][230/235]	Loss 0.1014 (0.1026)	
Pretraining:	 Loss: 0.1025

Pretraining:	Epoch 52/100
----------
Pretraining:	Epoch: [52][10/235]	Loss 0.0926 (0.0973)	
Pretraining:	Epoch: [52][20/235]	Loss 0.1041 (0.0993)	
Pretraining:	Epoch: [52][30/235]	Loss 0.0932 (0.0994)	
Pretraining:	Epoch: [52][40/235]	Loss 0.0974 (0.1003)	
Pretraining:	Epoch: [52][50/235]	Loss 0.1074 (0.0997)	
Pretraining:	Epoch: [52][60/235]	Loss 0.1010 (0.1002)	
Pretraining:	Epoch: [52][70/235]	Loss 0.0984 (0.1007)	
Pretraining:	Epoch: [52][80/235]	Loss 0.0916 (0.1003)	
Pretraining:	Epoch: [52][90/235]	Loss 0.1033 (0.1004)	
Pretraining:	Epoch: [52][100/235]	Loss 0.0932 (0.1003)	
Pretraining:	Epoch: [52][110/235]	Loss 0.0920 (0.1007)	
Pretraining:	Epoch: [52][120/235]	Loss 0.0966 (0.1009)	
Pretraining:	Epoch: [52][130/235]	Loss 0.0964 (0.1011)	
Pretraining:	Epoch: [52][140/235]	Loss 0.0992 (0.1010)	
Pretraining:	Epoch: [52][150/235]	Loss 0.1200 (0.1013)	
Pretraining:	Epoch: [52][160/235]	Loss 0.0944 (0.1013)	
Pretraining:	Epoch: [52][170/235]	Loss 0.0993 (0.1013)	
Pretraining:	Epoch: [52][180/235]	Loss 0.1100 (0.1014)	
Pretraining:	Epoch: [52][190/235]	Loss 0.1045 (0.1017)	
Pretraining:	Epoch: [52][200/235]	Loss 0.0921 (0.1019)	
Pretraining:	Epoch: [52][210/235]	Loss 0.1009 (0.1018)	
Pretraining:	Epoch: [52][220/235]	Loss 0.1042 (0.1018)	
Pretraining:	Epoch: [52][230/235]	Loss 0.1007 (0.1018)	
Pretraining:	 Loss: 0.1017

Pretraining:	Epoch 53/100
----------
Pretraining:	Epoch: [53][10/235]	Loss 0.0916 (0.0965)	
Pretraining:	Epoch: [53][20/235]	Loss 0.1036 (0.0985)	
Pretraining:	Epoch: [53][30/235]	Loss 0.0923 (0.0987)	
Pretraining:	Epoch: [53][40/235]	Loss 0.0974 (0.0997)	
Pretraining:	Epoch: [53][50/235]	Loss 0.1069 (0.0992)	
Pretraining:	Epoch: [53][60/235]	Loss 0.1005 (0.0996)	
Pretraining:	Epoch: [53][70/235]	Loss 0.0977 (0.1001)	
Pretraining:	Epoch: [53][80/235]	Loss 0.0907 (0.0997)	
Pretraining:	Epoch: [53][90/235]	Loss 0.1021 (0.0998)	
Pretraining:	Epoch: [53][100/235]	Loss 0.0922 (0.0996)	
Pretraining:	Epoch: [53][110/235]	Loss 0.0913 (0.1000)	
Pretraining:	Epoch: [53][120/235]	Loss 0.0959 (0.1003)	
Pretraining:	Epoch: [53][130/235]	Loss 0.0956 (0.1004)	
Pretraining:	Epoch: [53][140/235]	Loss 0.0983 (0.1003)	
Pretraining:	Epoch: [53][150/235]	Loss 0.1195 (0.1007)	
Pretraining:	Epoch: [53][160/235]	Loss 0.0939 (0.1006)	
Pretraining:	Epoch: [53][170/235]	Loss 0.0988 (0.1006)	
Pretraining:	Epoch: [53][180/235]	Loss 0.1097 (0.1008)	
Pretraining:	Epoch: [53][190/235]	Loss 0.1040 (0.1010)	
Pretraining:	Epoch: [53][200/235]	Loss 0.0918 (0.1013)	
Pretraining:	Epoch: [53][210/235]	Loss 0.1003 (0.1012)	
Pretraining:	Epoch: [53][220/235]	Loss 0.1037 (0.1012)	
Pretraining:	Epoch: [53][230/235]	Loss 0.1002 (0.1011)	
Pretraining:	 Loss: 0.1011

Pretraining:	Epoch 54/100
----------
Pretraining:	Epoch: [54][10/235]	Loss 0.0910 (0.0960)	
Pretraining:	Epoch: [54][20/235]	Loss 0.1029 (0.0980)	
Pretraining:	Epoch: [54][30/235]	Loss 0.0919 (0.0981)	
Pretraining:	Epoch: [54][40/235]	Loss 0.0965 (0.0991)	
Pretraining:	Epoch: [54][50/235]	Loss 0.1063 (0.0986)	
Pretraining:	Epoch: [54][60/235]	Loss 0.1003 (0.0990)	
Pretraining:	Epoch: [54][70/235]	Loss 0.0973 (0.0996)	
Pretraining:	Epoch: [54][80/235]	Loss 0.0901 (0.0992)	
Pretraining:	Epoch: [54][90/235]	Loss 0.1011 (0.0992)	
Pretraining:	Epoch: [54][100/235]	Loss 0.0920 (0.0991)	
Pretraining:	Epoch: [54][110/235]	Loss 0.0907 (0.0995)	
Pretraining:	Epoch: [54][120/235]	Loss 0.0951 (0.0997)	
Pretraining:	Epoch: [54][130/235]	Loss 0.0951 (0.0999)	
Pretraining:	Epoch: [54][140/235]	Loss 0.0977 (0.0997)	
Pretraining:	Epoch: [54][150/235]	Loss 0.1190 (0.1001)	
Pretraining:	Epoch: [54][160/235]	Loss 0.0934 (0.1001)	
Pretraining:	Epoch: [54][170/235]	Loss 0.0985 (0.1001)	
Pretraining:	Epoch: [54][180/235]	Loss 0.1093 (0.1003)	
Pretraining:	Epoch: [54][190/235]	Loss 0.1039 (0.1006)	
Pretraining:	Epoch: [54][200/235]	Loss 0.0914 (0.1008)	
Pretraining:	Epoch: [54][210/235]	Loss 0.0999 (0.1007)	
Pretraining:	Epoch: [54][220/235]	Loss 0.1035 (0.1008)	
Pretraining:	Epoch: [54][230/235]	Loss 0.1001 (0.1007)	
Pretraining:	 Loss: 0.1006

Pretraining:	Epoch 55/100
----------
Pretraining:	Epoch: [55][10/235]	Loss 0.0906 (0.0956)	
Pretraining:	Epoch: [55][20/235]	Loss 0.1023 (0.0977)	
Pretraining:	Epoch: [55][30/235]	Loss 0.0917 (0.0978)	
Pretraining:	Epoch: [55][40/235]	Loss 0.0961 (0.0988)	
Pretraining:	Epoch: [55][50/235]	Loss 0.1060 (0.0983)	
Pretraining:	Epoch: [55][60/235]	Loss 0.0999 (0.0988)	
Pretraining:	Epoch: [55][70/235]	Loss 0.0968 (0.0993)	
Pretraining:	Epoch: [55][80/235]	Loss 0.0899 (0.0989)	
Pretraining:	Epoch: [55][90/235]	Loss 0.1008 (0.0990)	
Pretraining:	Epoch: [55][100/235]	Loss 0.0918 (0.0988)	
Pretraining:	Epoch: [55][110/235]	Loss 0.0903 (0.0992)	
Pretraining:	Epoch: [55][120/235]	Loss 0.0946 (0.0994)	
Pretraining:	Epoch: [55][130/235]	Loss 0.0946 (0.0996)	
Pretraining:	Epoch: [55][140/235]	Loss 0.0974 (0.0994)	
Pretraining:	Epoch: [55][150/235]	Loss 0.1186 (0.0998)	
Pretraining:	Epoch: [55][160/235]	Loss 0.0932 (0.0998)	
Pretraining:	Epoch: [55][170/235]	Loss 0.0982 (0.0998)	
Pretraining:	Epoch: [55][180/235]	Loss 0.1090 (0.1000)	
Pretraining:	Epoch: [55][190/235]	Loss 0.1035 (0.1003)	
Pretraining:	Epoch: [55][200/235]	Loss 0.0909 (0.1005)	
Pretraining:	Epoch: [55][210/235]	Loss 0.0996 (0.1004)	
Pretraining:	Epoch: [55][220/235]	Loss 0.1029 (0.1004)	
Pretraining:	Epoch: [55][230/235]	Loss 0.0996 (0.1004)	
Pretraining:	 Loss: 0.1003

Pretraining:	Epoch 56/100
----------
Pretraining:	Epoch: [56][10/235]	Loss 0.0901 (0.0953)	
Pretraining:	Epoch: [56][20/235]	Loss 0.1021 (0.0973)	
Pretraining:	Epoch: [56][30/235]	Loss 0.0916 (0.0975)	
Pretraining:	Epoch: [56][40/235]	Loss 0.0956 (0.0985)	
Pretraining:	Epoch: [56][50/235]	Loss 0.1061 (0.0981)	
Pretraining:	Epoch: [56][60/235]	Loss 0.0999 (0.0986)	
Pretraining:	Epoch: [56][70/235]	Loss 0.0966 (0.0991)	
Pretraining:	Epoch: [56][80/235]	Loss 0.0895 (0.0987)	
Pretraining:	Epoch: [56][90/235]	Loss 0.1005 (0.0987)	
Pretraining:	Epoch: [56][100/235]	Loss 0.0915 (0.0986)	
Pretraining:	Epoch: [56][110/235]	Loss 0.0900 (0.0990)	
Pretraining:	Epoch: [56][120/235]	Loss 0.0945 (0.0992)	
Pretraining:	Epoch: [56][130/235]	Loss 0.0946 (0.0993)	
Pretraining:	Epoch: [56][140/235]	Loss 0.0970 (0.0992)	
Pretraining:	Epoch: [56][150/235]	Loss 0.1184 (0.0996)	
Pretraining:	Epoch: [56][160/235]	Loss 0.0929 (0.0995)	
Pretraining:	Epoch: [56][170/235]	Loss 0.0980 (0.0996)	
Pretraining:	Epoch: [56][180/235]	Loss 0.1087 (0.0997)	
Pretraining:	Epoch: [56][190/235]	Loss 0.1037 (0.1000)	
Pretraining:	Epoch: [56][200/235]	Loss 0.0908 (0.1003)	
Pretraining:	Epoch: [56][210/235]	Loss 0.0991 (0.1002)	
Pretraining:	Epoch: [56][220/235]	Loss 0.1028 (0.1002)	
Pretraining:	Epoch: [56][230/235]	Loss 0.0994 (0.1001)	
Pretraining:	 Loss: 0.1001

Pretraining:	Epoch 57/100
----------
Pretraining:	Epoch: [57][10/235]	Loss 0.0899 (0.0952)	
Pretraining:	Epoch: [57][20/235]	Loss 0.1018 (0.0972)	
Pretraining:	Epoch: [57][30/235]	Loss 0.0911 (0.0973)	
Pretraining:	Epoch: [57][40/235]	Loss 0.0954 (0.0983)	
Pretraining:	Epoch: [57][50/235]	Loss 0.1053 (0.0978)	
Pretraining:	Epoch: [57][60/235]	Loss 0.0998 (0.0983)	
Pretraining:	Epoch: [57][70/235]	Loss 0.0963 (0.0988)	
Pretraining:	Epoch: [57][80/235]	Loss 0.0900 (0.0984)	
Pretraining:	Epoch: [57][90/235]	Loss 0.1010 (0.0985)	
Pretraining:	Epoch: [57][100/235]	Loss 0.0917 (0.0984)	
Pretraining:	Epoch: [57][110/235]	Loss 0.0904 (0.0988)	
Pretraining:	Epoch: [57][120/235]	Loss 0.0941 (0.0991)	
Pretraining:	Epoch: [57][130/235]	Loss 0.0989 (0.0994)	
Pretraining:	Epoch: [57][140/235]	Loss 0.1044 (0.0995)	
Pretraining:	Epoch: [57][150/235]	Loss 0.1214 (0.1001)	
Pretraining:	Epoch: [57][160/235]	Loss 0.0959 (0.1002)	
Pretraining:	Epoch: [57][170/235]	Loss 0.0991 (0.1003)	
Pretraining:	Epoch: [57][180/235]	Loss 0.1091 (0.1004)	
Pretraining:	Epoch: [57][190/235]	Loss 0.1034 (0.1006)	
Pretraining:	Epoch: [57][200/235]	Loss 0.0911 (0.1008)	
Pretraining:	Epoch: [57][210/235]	Loss 0.0991 (0.1007)	
Pretraining:	Epoch: [57][220/235]	Loss 0.1027 (0.1007)	
Pretraining:	Epoch: [57][230/235]	Loss 0.0994 (0.1007)	
Pretraining:	 Loss: 0.1006

Pretraining:	Epoch 58/100
----------
Pretraining:	Epoch: [58][10/235]	Loss 0.0907 (0.0952)	
Pretraining:	Epoch: [58][20/235]	Loss 0.1022 (0.0973)	
Pretraining:	Epoch: [58][30/235]	Loss 0.0905 (0.0975)	
Pretraining:	Epoch: [58][40/235]	Loss 0.0954 (0.0984)	
Pretraining:	Epoch: [58][50/235]	Loss 0.1055 (0.0978)	
Pretraining:	Epoch: [58][60/235]	Loss 0.0992 (0.0983)	
Pretraining:	Epoch: [58][70/235]	Loss 0.0961 (0.0989)	
Pretraining:	Epoch: [58][80/235]	Loss 0.0891 (0.0985)	
Pretraining:	Epoch: [58][90/235]	Loss 0.1029 (0.0987)	
Pretraining:	Epoch: [58][100/235]	Loss 0.0913 (0.0985)	
Pretraining:	Epoch: [58][110/235]	Loss 0.0895 (0.0989)	
Pretraining:	Epoch: [58][120/235]	Loss 0.0942 (0.0991)	
Pretraining:	Epoch: [58][130/235]	Loss 0.0937 (0.0992)	
Pretraining:	Epoch: [58][140/235]	Loss 0.0967 (0.0991)	
Pretraining:	Epoch: [58][150/235]	Loss 0.1187 (0.0995)	
Pretraining:	Epoch: [58][160/235]	Loss 0.0928 (0.0996)	
Pretraining:	Epoch: [58][170/235]	Loss 0.0984 (0.0996)	
Pretraining:	Epoch: [58][180/235]	Loss 0.1088 (0.0998)	
Pretraining:	Epoch: [58][190/235]	Loss 0.1038 (0.1001)	
Pretraining:	Epoch: [58][200/235]	Loss 0.0909 (0.1003)	
Pretraining:	Epoch: [58][210/235]	Loss 0.0989 (0.1002)	
Pretraining:	Epoch: [58][220/235]	Loss 0.1024 (0.1003)	
Pretraining:	Epoch: [58][230/235]	Loss 0.0996 (0.1002)	
Pretraining:	 Loss: 0.1001

Pretraining:	Epoch 59/100
----------
Pretraining:	Epoch: [59][10/235]	Loss 0.0908 (0.0950)	
Pretraining:	Epoch: [59][20/235]	Loss 0.1021 (0.0971)	
Pretraining:	Epoch: [59][30/235]	Loss 0.0906 (0.0975)	
Pretraining:	Epoch: [59][40/235]	Loss 0.0954 (0.0985)	
Pretraining:	Epoch: [59][50/235]	Loss 0.1054 (0.0980)	
Pretraining:	Epoch: [59][60/235]	Loss 0.0991 (0.0984)	
Pretraining:	Epoch: [59][70/235]	Loss 0.0966 (0.0989)	
Pretraining:	Epoch: [59][80/235]	Loss 0.0895 (0.0986)	
Pretraining:	Epoch: [59][90/235]	Loss 0.1005 (0.0986)	
Pretraining:	Epoch: [59][100/235]	Loss 0.0921 (0.0985)	
Pretraining:	Epoch: [59][110/235]	Loss 0.0896 (0.0989)	
Pretraining:	Epoch: [59][120/235]	Loss 0.0935 (0.0990)	
Pretraining:	Epoch: [59][130/235]	Loss 0.0940 (0.0991)	
Pretraining:	Epoch: [59][140/235]	Loss 0.0973 (0.0990)	
Pretraining:	Epoch: [59][150/235]	Loss 0.1177 (0.0994)	
Pretraining:	Epoch: [59][160/235]	Loss 0.0927 (0.0994)	
Pretraining:	Epoch: [59][170/235]	Loss 0.0986 (0.0994)	
Pretraining:	Epoch: [59][180/235]	Loss 0.1083 (0.0996)	
Pretraining:	Epoch: [59][190/235]	Loss 0.1031 (0.0999)	
Pretraining:	Epoch: [59][200/235]	Loss 0.0919 (0.1001)	
Pretraining:	Epoch: [59][210/235]	Loss 0.0988 (0.1000)	
Pretraining:	Epoch: [59][220/235]	Loss 0.1024 (0.1001)	
Pretraining:	Epoch: [59][230/235]	Loss 0.0996 (0.1000)	
Pretraining:	 Loss: 0.0999

Pretraining:	Epoch 60/100
----------
Pretraining:	Epoch: [60][10/235]	Loss 0.0907 (0.0951)	
Pretraining:	Epoch: [60][20/235]	Loss 0.1016 (0.0972)	
Pretraining:	Epoch: [60][30/235]	Loss 0.0913 (0.0976)	
Pretraining:	Epoch: [60][40/235]	Loss 0.0962 (0.0985)	
Pretraining:	Epoch: [60][50/235]	Loss 0.1052 (0.0980)	
Pretraining:	Epoch: [60][60/235]	Loss 0.0990 (0.0985)	
Pretraining:	Epoch: [60][70/235]	Loss 0.0968 (0.0991)	
Pretraining:	Epoch: [60][80/235]	Loss 0.0894 (0.0987)	
Pretraining:	Epoch: [60][90/235]	Loss 0.1005 (0.0989)	
Pretraining:	Epoch: [60][100/235]	Loss 0.0925 (0.0988)	
Pretraining:	Epoch: [60][110/235]	Loss 0.0909 (0.0992)	
Pretraining:	Epoch: [60][120/235]	Loss 0.0944 (0.0994)	
Pretraining:	Epoch: [60][130/235]	Loss 0.0944 (0.0995)	
Pretraining:	Epoch: [60][140/235]	Loss 0.0966 (0.0993)	
Pretraining:	Epoch: [60][150/235]	Loss 0.1175 (0.0997)	
Pretraining:	Epoch: [60][160/235]	Loss 0.0926 (0.0996)	
Pretraining:	Epoch: [60][170/235]	Loss 0.0974 (0.0996)	
Pretraining:	Epoch: [60][180/235]	Loss 0.1085 (0.0997)	
Pretraining:	Epoch: [60][190/235]	Loss 0.1049 (0.1000)	
Pretraining:	Epoch: [60][200/235]	Loss 0.0910 (0.1002)	
Pretraining:	Epoch: [60][210/235]	Loss 0.1006 (0.1002)	
Pretraining:	Epoch: [60][220/235]	Loss 0.1026 (0.1003)	
Pretraining:	Epoch: [60][230/235]	Loss 0.0992 (0.1002)	
Pretraining:	 Loss: 0.1001

Pretraining:	Epoch 61/100
----------
Pretraining:	Epoch: [61][10/235]	Loss 0.0905 (0.0952)	
Pretraining:	Epoch: [61][20/235]	Loss 0.1018 (0.0972)	
Pretraining:	Epoch: [61][30/235]	Loss 0.0928 (0.0977)	
Pretraining:	Epoch: [61][40/235]	Loss 0.0956 (0.0988)	
Pretraining:	Epoch: [61][50/235]	Loss 0.1052 (0.0984)	
Pretraining:	Epoch: [61][60/235]	Loss 0.0990 (0.0988)	
Pretraining:	Epoch: [61][70/235]	Loss 0.0962 (0.0992)	
Pretraining:	Epoch: [61][80/235]	Loss 0.0886 (0.0988)	
Pretraining:	Epoch: [61][90/235]	Loss 0.1018 (0.0988)	
Pretraining:	Epoch: [61][100/235]	Loss 0.0908 (0.0986)	
Pretraining:	Epoch: [61][110/235]	Loss 0.0904 (0.0990)	
Pretraining:	Epoch: [61][120/235]	Loss 0.0939 (0.0991)	
Pretraining:	Epoch: [61][130/235]	Loss 0.0945 (0.0992)	
Pretraining:	Epoch: [61][140/235]	Loss 0.0968 (0.0991)	
Pretraining:	Epoch: [61][150/235]	Loss 0.1177 (0.0994)	
Pretraining:	Epoch: [61][160/235]	Loss 0.0926 (0.0994)	
Pretraining:	Epoch: [61][170/235]	Loss 0.0973 (0.0994)	
Pretraining:	Epoch: [61][180/235]	Loss 0.1084 (0.0995)	
Pretraining:	Epoch: [61][190/235]	Loss 0.1034 (0.0998)	
Pretraining:	Epoch: [61][200/235]	Loss 0.0915 (0.1000)	
Pretraining:	Epoch: [61][210/235]	Loss 0.0996 (0.1000)	
Pretraining:	Epoch: [61][220/235]	Loss 0.1031 (0.1001)	
Pretraining:	Epoch: [61][230/235]	Loss 0.0992 (0.1000)	
Pretraining:	 Loss: 0.0999

Pretraining:	Epoch 62/100
----------
Pretraining:	Epoch: [62][10/235]	Loss 0.0906 (0.0950)	
Pretraining:	Epoch: [62][20/235]	Loss 0.1021 (0.0970)	
Pretraining:	Epoch: [62][30/235]	Loss 0.0914 (0.0973)	
Pretraining:	Epoch: [62][40/235]	Loss 0.0963 (0.0985)	
Pretraining:	Epoch: [62][50/235]	Loss 0.1057 (0.0981)	
Pretraining:	Epoch: [62][60/235]	Loss 0.0986 (0.0985)	
Pretraining:	Epoch: [62][70/235]	Loss 0.0958 (0.0990)	
Pretraining:	Epoch: [62][80/235]	Loss 0.0890 (0.0986)	
Pretraining:	Epoch: [62][90/235]	Loss 0.1014 (0.0986)	
Pretraining:	Epoch: [62][100/235]	Loss 0.0909 (0.0984)	
Pretraining:	Epoch: [62][110/235]	Loss 0.0904 (0.0988)	
Pretraining:	Epoch: [62][120/235]	Loss 0.0939 (0.0989)	
Pretraining:	Epoch: [62][130/235]	Loss 0.0942 (0.0991)	
Pretraining:	Epoch: [62][140/235]	Loss 0.0970 (0.0989)	
Pretraining:	Epoch: [62][150/235]	Loss 0.1178 (0.0993)	
Pretraining:	Epoch: [62][160/235]	Loss 0.0930 (0.0993)	
Pretraining:	Epoch: [62][170/235]	Loss 0.0971 (0.0993)	
Pretraining:	Epoch: [62][180/235]	Loss 0.1083 (0.0994)	
Pretraining:	Epoch: [62][190/235]	Loss 0.1027 (0.0997)	
Pretraining:	Epoch: [62][200/235]	Loss 0.0914 (0.0999)	
Pretraining:	Epoch: [62][210/235]	Loss 0.0989 (0.0999)	
Pretraining:	Epoch: [62][220/235]	Loss 0.1024 (0.0999)	
Pretraining:	Epoch: [62][230/235]	Loss 0.0994 (0.0999)	
Pretraining:	 Loss: 0.0998

Pretraining:	Epoch 63/100
----------
Pretraining:	Epoch: [63][10/235]	Loss 0.0909 (0.0954)	
Pretraining:	Epoch: [63][20/235]	Loss 0.1019 (0.0971)	
Pretraining:	Epoch: [63][30/235]	Loss 0.0910 (0.0973)	
Pretraining:	Epoch: [63][40/235]	Loss 0.0963 (0.0984)	
Pretraining:	Epoch: [63][50/235]	Loss 0.1044 (0.0979)	
Pretraining:	Epoch: [63][60/235]	Loss 0.0986 (0.0983)	
Pretraining:	Epoch: [63][70/235]	Loss 0.0955 (0.0989)	
Pretraining:	Epoch: [63][80/235]	Loss 0.0887 (0.0985)	
Pretraining:	Epoch: [63][90/235]	Loss 0.1009 (0.0986)	
Pretraining:	Epoch: [63][100/235]	Loss 0.0916 (0.0984)	
Pretraining:	Epoch: [63][110/235]	Loss 0.0901 (0.0988)	
Pretraining:	Epoch: [63][120/235]	Loss 0.0934 (0.0989)	
Pretraining:	Epoch: [63][130/235]	Loss 0.0949 (0.0990)	
Pretraining:	Epoch: [63][140/235]	Loss 0.0976 (0.0990)	
Pretraining:	Epoch: [63][150/235]	Loss 0.1177 (0.0994)	
Pretraining:	Epoch: [63][160/235]	Loss 0.0927 (0.0993)	
Pretraining:	Epoch: [63][170/235]	Loss 0.0974 (0.0994)	
Pretraining:	Epoch: [63][180/235]	Loss 0.1084 (0.0995)	
Pretraining:	Epoch: [63][190/235]	Loss 0.1031 (0.0998)	
Pretraining:	Epoch: [63][200/235]	Loss 0.0909 (0.1000)	
Pretraining:	Epoch: [63][210/235]	Loss 0.0990 (0.1000)	
Pretraining:	Epoch: [63][220/235]	Loss 0.1025 (0.1000)	
Pretraining:	Epoch: [63][230/235]	Loss 0.0992 (0.1000)	
Pretraining:	 Loss: 0.0999

Pretraining:	Epoch 64/100
----------
Pretraining:	Epoch: [64][10/235]	Loss 0.0911 (0.0966)	
Pretraining:	Epoch: [64][20/235]	Loss 0.1011 (0.0977)	
Pretraining:	Epoch: [64][30/235]	Loss 0.0907 (0.0976)	
Pretraining:	Epoch: [64][40/235]	Loss 0.0954 (0.0985)	
Pretraining:	Epoch: [64][50/235]	Loss 0.1044 (0.0979)	
Pretraining:	Epoch: [64][60/235]	Loss 0.0982 (0.0983)	
Pretraining:	Epoch: [64][70/235]	Loss 0.0959 (0.0988)	
Pretraining:	Epoch: [64][80/235]	Loss 0.0903 (0.0985)	
Pretraining:	Epoch: [64][90/235]	Loss 0.1013 (0.0987)	
Pretraining:	Epoch: [64][100/235]	Loss 0.0932 (0.0986)	
Pretraining:	Epoch: [64][110/235]	Loss 0.0902 (0.0990)	
Pretraining:	Epoch: [64][120/235]	Loss 0.0943 (0.0992)	
Pretraining:	Epoch: [64][130/235]	Loss 0.0975 (0.0993)	
Pretraining:	Epoch: [64][140/235]	Loss 0.0984 (0.0993)	
Pretraining:	Epoch: [64][150/235]	Loss 0.1177 (0.0997)	
Pretraining:	Epoch: [64][160/235]	Loss 0.0929 (0.0997)	
Pretraining:	Epoch: [64][170/235]	Loss 0.0979 (0.0998)	
Pretraining:	Epoch: [64][180/235]	Loss 0.1083 (0.0999)	
Pretraining:	Epoch: [64][190/235]	Loss 0.1028 (0.1002)	
Pretraining:	Epoch: [64][200/235]	Loss 0.0919 (0.1004)	
Pretraining:	Epoch: [64][210/235]	Loss 0.1006 (0.1004)	
Pretraining:	Epoch: [64][220/235]	Loss 0.1039 (0.1004)	
Pretraining:	Epoch: [64][230/235]	Loss 0.0996 (0.1004)	
Pretraining:	 Loss: 0.1003

Pretraining:	Epoch 65/100
----------
Pretraining:	Epoch: [65][10/235]	Loss 0.0934 (0.0976)	
Pretraining:	Epoch: [65][20/235]	Loss 0.1025 (0.0989)	
Pretraining:	Epoch: [65][30/235]	Loss 0.0908 (0.0987)	
Pretraining:	Epoch: [65][40/235]	Loss 0.0955 (0.0994)	
Pretraining:	Epoch: [65][50/235]	Loss 0.1046 (0.0987)	
Pretraining:	Epoch: [65][60/235]	Loss 0.0983 (0.0990)	
Pretraining:	Epoch: [65][70/235]	Loss 0.0952 (0.0994)	
Pretraining:	Epoch: [65][80/235]	Loss 0.0914 (0.0990)	
Pretraining:	Epoch: [65][90/235]	Loss 0.1032 (0.0992)	
Pretraining:	Epoch: [65][100/235]	Loss 0.0942 (0.0992)	
Pretraining:	Epoch: [65][110/235]	Loss 0.0913 (0.0996)	
Pretraining:	Epoch: [65][120/235]	Loss 0.0946 (0.0999)	
Pretraining:	Epoch: [65][130/235]	Loss 0.0953 (0.1000)	
Pretraining:	Epoch: [65][140/235]	Loss 0.0979 (0.0999)	
Pretraining:	Epoch: [65][150/235]	Loss 0.1194 (0.1003)	
Pretraining:	Epoch: [65][160/235]	Loss 0.0927 (0.1003)	
Pretraining:	Epoch: [65][170/235]	Loss 0.0979 (0.1003)	
Pretraining:	Epoch: [65][180/235]	Loss 0.1081 (0.1004)	
Pretraining:	Epoch: [65][190/235]	Loss 0.1016 (0.1006)	
Pretraining:	Epoch: [65][200/235]	Loss 0.0912 (0.1008)	
Pretraining:	Epoch: [65][210/235]	Loss 0.1011 (0.1007)	
Pretraining:	Epoch: [65][220/235]	Loss 0.1049 (0.1008)	
Pretraining:	Epoch: [65][230/235]	Loss 0.0992 (0.1007)	
Pretraining:	 Loss: 0.1006

Pretraining:	Epoch 66/100
----------
Pretraining:	Epoch: [66][10/235]	Loss 0.0926 (0.0970)	
Pretraining:	Epoch: [66][20/235]	Loss 0.1037 (0.0987)	
Pretraining:	Epoch: [66][30/235]	Loss 0.0924 (0.0986)	
Pretraining:	Epoch: [66][40/235]	Loss 0.0954 (0.0995)	
Pretraining:	Epoch: [66][50/235]	Loss 0.1049 (0.0988)	
Pretraining:	Epoch: [66][60/235]	Loss 0.0997 (0.0992)	
Pretraining:	Epoch: [66][70/235]	Loss 0.0959 (0.0997)	
Pretraining:	Epoch: [66][80/235]	Loss 0.0901 (0.0993)	
Pretraining:	Epoch: [66][90/235]	Loss 0.1018 (0.0994)	
Pretraining:	Epoch: [66][100/235]	Loss 0.0932 (0.0993)	
Pretraining:	Epoch: [66][110/235]	Loss 0.0912 (0.0997)	
Pretraining:	Epoch: [66][120/235]	Loss 0.0954 (0.0999)	
Pretraining:	Epoch: [66][130/235]	Loss 0.0951 (0.1000)	
Pretraining:	Epoch: [66][140/235]	Loss 0.0975 (0.0998)	
Pretraining:	Epoch: [66][150/235]	Loss 0.1180 (0.1002)	
Pretraining:	Epoch: [66][160/235]	Loss 0.0926 (0.1001)	
Pretraining:	Epoch: [66][170/235]	Loss 0.0977 (0.1000)	
Pretraining:	Epoch: [66][180/235]	Loss 0.1077 (0.1001)	
Pretraining:	Epoch: [66][190/235]	Loss 0.1014 (0.1003)	
Pretraining:	Epoch: [66][200/235]	Loss 0.0905 (0.1005)	
Pretraining:	Epoch: [66][210/235]	Loss 0.0998 (0.1004)	
Pretraining:	Epoch: [66][220/235]	Loss 0.1052 (0.1005)	
Pretraining:	Epoch: [66][230/235]	Loss 0.0996 (0.1004)	
Pretraining:	 Loss: 0.1003

Pretraining:	Epoch 67/100
----------
Pretraining:	Epoch: [67][10/235]	Loss 0.0922 (0.0969)	
Pretraining:	Epoch: [67][20/235]	Loss 0.1033 (0.0984)	
Pretraining:	Epoch: [67][30/235]	Loss 0.0920 (0.0985)	
Pretraining:	Epoch: [67][40/235]	Loss 0.0956 (0.0992)	
Pretraining:	Epoch: [67][50/235]	Loss 0.1039 (0.0985)	
Pretraining:	Epoch: [67][60/235]	Loss 0.0987 (0.0988)	
Pretraining:	Epoch: [67][70/235]	Loss 0.0957 (0.0992)	
Pretraining:	Epoch: [67][80/235]	Loss 0.0893 (0.0988)	
Pretraining:	Epoch: [67][90/235]	Loss 0.1005 (0.0988)	
Pretraining:	Epoch: [67][100/235]	Loss 0.0933 (0.0987)	
Pretraining:	Epoch: [67][110/235]	Loss 0.0910 (0.0991)	
Pretraining:	Epoch: [67][120/235]	Loss 0.0951 (0.0994)	
Pretraining:	Epoch: [67][130/235]	Loss 0.0960 (0.0995)	
Pretraining:	Epoch: [67][140/235]	Loss 0.0977 (0.0994)	
Pretraining:	Epoch: [67][150/235]	Loss 0.1181 (0.0997)	
Pretraining:	Epoch: [67][160/235]	Loss 0.0924 (0.0996)	
Pretraining:	Epoch: [67][170/235]	Loss 0.0973 (0.0996)	
Pretraining:	Epoch: [67][180/235]	Loss 0.1070 (0.0997)	
Pretraining:	Epoch: [67][190/235]	Loss 0.1016 (0.0999)	
Pretraining:	Epoch: [67][200/235]	Loss 0.0904 (0.1001)	
Pretraining:	Epoch: [67][210/235]	Loss 0.0987 (0.1001)	
Pretraining:	Epoch: [67][220/235]	Loss 0.1039 (0.1001)	
Pretraining:	Epoch: [67][230/235]	Loss 0.1002 (0.1001)	
Pretraining:	 Loss: 0.1000

Pretraining:	Epoch 68/100
----------
Pretraining:	Epoch: [68][10/235]	Loss 0.0925 (0.0966)	
Pretraining:	Epoch: [68][20/235]	Loss 0.1031 (0.0981)	
Pretraining:	Epoch: [68][30/235]	Loss 0.0908 (0.0981)	
Pretraining:	Epoch: [68][40/235]	Loss 0.0950 (0.0989)	
Pretraining:	Epoch: [68][50/235]	Loss 0.1035 (0.0982)	
Pretraining:	Epoch: [68][60/235]	Loss 0.0983 (0.0985)	
Pretraining:	Epoch: [68][70/235]	Loss 0.0955 (0.0989)	
Pretraining:	Epoch: [68][80/235]	Loss 0.0896 (0.0985)	
Pretraining:	Epoch: [68][90/235]	Loss 0.1008 (0.0985)	
Pretraining:	Epoch: [68][100/235]	Loss 0.0933 (0.0984)	
Pretraining:	Epoch: [68][110/235]	Loss 0.0914 (0.0989)	
Pretraining:	Epoch: [68][120/235]	Loss 0.0943 (0.0991)	
Pretraining:	Epoch: [68][130/235]	Loss 0.0958 (0.0993)	
Pretraining:	Epoch: [68][140/235]	Loss 0.0971 (0.0991)	
Pretraining:	Epoch: [68][150/235]	Loss 0.1183 (0.0995)	
Pretraining:	Epoch: [68][160/235]	Loss 0.0924 (0.0994)	
Pretraining:	Epoch: [68][170/235]	Loss 0.0974 (0.0994)	
Pretraining:	Epoch: [68][180/235]	Loss 0.1071 (0.0995)	
Pretraining:	Epoch: [68][190/235]	Loss 0.1019 (0.0998)	
Pretraining:	Epoch: [68][200/235]	Loss 0.0900 (0.1000)	
Pretraining:	Epoch: [68][210/235]	Loss 0.0975 (0.0999)	
Pretraining:	Epoch: [68][220/235]	Loss 0.1018 (0.1000)	
Pretraining:	Epoch: [68][230/235]	Loss 0.0991 (0.0999)	
Pretraining:	 Loss: 0.0999

Pretraining:	Epoch 69/100
----------
Pretraining:	Epoch: [69][10/235]	Loss 0.0918 (0.0961)	
Pretraining:	Epoch: [69][20/235]	Loss 0.1020 (0.0977)	
Pretraining:	Epoch: [69][30/235]	Loss 0.0901 (0.0978)	
Pretraining:	Epoch: [69][40/235]	Loss 0.0945 (0.0986)	
Pretraining:	Epoch: [69][50/235]	Loss 0.1037 (0.0979)	
Pretraining:	Epoch: [69][60/235]	Loss 0.0979 (0.0982)	
Pretraining:	Epoch: [69][70/235]	Loss 0.0946 (0.0986)	
Pretraining:	Epoch: [69][80/235]	Loss 0.0896 (0.0981)	
Pretraining:	Epoch: [69][90/235]	Loss 0.1022 (0.0983)	
Pretraining:	Epoch: [69][100/235]	Loss 0.0929 (0.0982)	
Pretraining:	Epoch: [69][110/235]	Loss 0.0908 (0.0987)	
Pretraining:	Epoch: [69][120/235]	Loss 0.0935 (0.0988)	
Pretraining:	Epoch: [69][130/235]	Loss 0.0958 (0.0990)	
Pretraining:	Epoch: [69][140/235]	Loss 0.0959 (0.0988)	
Pretraining:	Epoch: [69][150/235]	Loss 0.1175 (0.0992)	
Pretraining:	Epoch: [69][160/235]	Loss 0.0923 (0.0991)	
Pretraining:	Epoch: [69][170/235]	Loss 0.0968 (0.0991)	
Pretraining:	Epoch: [69][180/235]	Loss 0.1068 (0.0992)	
Pretraining:	Epoch: [69][190/235]	Loss 0.1014 (0.0994)	
Pretraining:	Epoch: [69][200/235]	Loss 0.0896 (0.0996)	
Pretraining:	Epoch: [69][210/235]	Loss 0.0972 (0.0996)	
Pretraining:	Epoch: [69][220/235]	Loss 0.1005 (0.0996)	
Pretraining:	Epoch: [69][230/235]	Loss 0.0981 (0.0995)	
Pretraining:	 Loss: 0.0994

Pretraining:	Epoch 70/100
----------
Pretraining:	Epoch: [70][10/235]	Loss 0.0916 (0.0957)	
Pretraining:	Epoch: [70][20/235]	Loss 0.1008 (0.0972)	
Pretraining:	Epoch: [70][30/235]	Loss 0.0897 (0.0973)	
Pretraining:	Epoch: [70][40/235]	Loss 0.0940 (0.0980)	
Pretraining:	Epoch: [70][50/235]	Loss 0.1033 (0.0973)	
Pretraining:	Epoch: [70][60/235]	Loss 0.0973 (0.0976)	
Pretraining:	Epoch: [70][70/235]	Loss 0.0937 (0.0980)	
Pretraining:	Epoch: [70][80/235]	Loss 0.0894 (0.0976)	
Pretraining:	Epoch: [70][90/235]	Loss 0.1025 (0.0978)	
Pretraining:	Epoch: [70][100/235]	Loss 0.0924 (0.0977)	
Pretraining:	Epoch: [70][110/235]	Loss 0.0900 (0.0982)	
Pretraining:	Epoch: [70][120/235]	Loss 0.0931 (0.0984)	
Pretraining:	Epoch: [70][130/235]	Loss 0.0947 (0.0985)	
Pretraining:	Epoch: [70][140/235]	Loss 0.0954 (0.0984)	
Pretraining:	Epoch: [70][150/235]	Loss 0.1165 (0.0987)	
Pretraining:	Epoch: [70][160/235]	Loss 0.0922 (0.0986)	
Pretraining:	Epoch: [70][170/235]	Loss 0.0966 (0.0987)	
Pretraining:	Epoch: [70][180/235]	Loss 0.1065 (0.0988)	
Pretraining:	Epoch: [70][190/235]	Loss 0.1012 (0.0990)	
Pretraining:	Epoch: [70][200/235]	Loss 0.0893 (0.0992)	
Pretraining:	Epoch: [70][210/235]	Loss 0.0968 (0.0991)	
Pretraining:	Epoch: [70][220/235]	Loss 0.1003 (0.0991)	
Pretraining:	Epoch: [70][230/235]	Loss 0.0974 (0.0991)	
Pretraining:	 Loss: 0.0990

Pretraining:	Epoch 71/100
----------
Pretraining:	Epoch: [71][10/235]	Loss 0.0912 (0.0950)	
Pretraining:	Epoch: [71][20/235]	Loss 0.1005 (0.0965)	
Pretraining:	Epoch: [71][30/235]	Loss 0.0893 (0.0966)	
Pretraining:	Epoch: [71][40/235]	Loss 0.0934 (0.0974)	
Pretraining:	Epoch: [71][50/235]	Loss 0.1028 (0.0967)	
Pretraining:	Epoch: [71][60/235]	Loss 0.0965 (0.0970)	
Pretraining:	Epoch: [71][70/235]	Loss 0.0933 (0.0974)	
Pretraining:	Epoch: [71][80/235]	Loss 0.0886 (0.0970)	
Pretraining:	Epoch: [71][90/235]	Loss 0.1018 (0.0971)	
Pretraining:	Epoch: [71][100/235]	Loss 0.0917 (0.0971)	
Pretraining:	Epoch: [71][110/235]	Loss 0.0896 (0.0976)	
Pretraining:	Epoch: [71][120/235]	Loss 0.0930 (0.0978)	
Pretraining:	Epoch: [71][130/235]	Loss 0.0936 (0.0979)	
Pretraining:	Epoch: [71][140/235]	Loss 0.0949 (0.0978)	
Pretraining:	Epoch: [71][150/235]	Loss 0.1158 (0.0981)	
Pretraining:	Epoch: [71][160/235]	Loss 0.0917 (0.0981)	
Pretraining:	Epoch: [71][170/235]	Loss 0.0960 (0.0981)	
Pretraining:	Epoch: [71][180/235]	Loss 0.1062 (0.0982)	
Pretraining:	Epoch: [71][190/235]	Loss 0.1007 (0.0984)	
Pretraining:	Epoch: [71][200/235]	Loss 0.0890 (0.0986)	
Pretraining:	Epoch: [71][210/235]	Loss 0.0964 (0.0986)	
Pretraining:	Epoch: [71][220/235]	Loss 0.1000 (0.0986)	
Pretraining:	Epoch: [71][230/235]	Loss 0.0971 (0.0985)	
Pretraining:	 Loss: 0.0984

Pretraining:	Epoch 72/100
----------
Pretraining:	Epoch: [72][10/235]	Loss 0.0904 (0.0942)	
Pretraining:	Epoch: [72][20/235]	Loss 0.1001 (0.0958)	
Pretraining:	Epoch: [72][30/235]	Loss 0.0887 (0.0958)	
Pretraining:	Epoch: [72][40/235]	Loss 0.0928 (0.0966)	
Pretraining:	Epoch: [72][50/235]	Loss 0.1022 (0.0960)	
Pretraining:	Epoch: [72][60/235]	Loss 0.0959 (0.0963)	
Pretraining:	Epoch: [72][70/235]	Loss 0.0927 (0.0968)	
Pretraining:	Epoch: [72][80/235]	Loss 0.0879 (0.0963)	
Pretraining:	Epoch: [72][90/235]	Loss 0.1007 (0.0965)	
Pretraining:	Epoch: [72][100/235]	Loss 0.0902 (0.0964)	
Pretraining:	Epoch: [72][110/235]	Loss 0.0888 (0.0969)	
Pretraining:	Epoch: [72][120/235]	Loss 0.0926 (0.0971)	
Pretraining:	Epoch: [72][130/235]	Loss 0.0928 (0.0972)	
Pretraining:	Epoch: [72][140/235]	Loss 0.0944 (0.0971)	
Pretraining:	Epoch: [72][150/235]	Loss 0.1152 (0.0974)	
Pretraining:	Epoch: [72][160/235]	Loss 0.0911 (0.0974)	
Pretraining:	Epoch: [72][170/235]	Loss 0.0955 (0.0974)	
Pretraining:	Epoch: [72][180/235]	Loss 0.1059 (0.0975)	
Pretraining:	Epoch: [72][190/235]	Loss 0.0999 (0.0978)	
Pretraining:	Epoch: [72][200/235]	Loss 0.0885 (0.0980)	
Pretraining:	Epoch: [72][210/235]	Loss 0.0961 (0.0979)	
Pretraining:	Epoch: [72][220/235]	Loss 0.0996 (0.0980)	
Pretraining:	Epoch: [72][230/235]	Loss 0.0967 (0.0979)	
Pretraining:	 Loss: 0.0978

Pretraining:	Epoch 73/100
----------
Pretraining:	Epoch: [73][10/235]	Loss 0.0896 (0.0935)	
Pretraining:	Epoch: [73][20/235]	Loss 0.0997 (0.0950)	
Pretraining:	Epoch: [73][30/235]	Loss 0.0883 (0.0951)	
Pretraining:	Epoch: [73][40/235]	Loss 0.0923 (0.0959)	
Pretraining:	Epoch: [73][50/235]	Loss 0.1017 (0.0953)	
Pretraining:	Epoch: [73][60/235]	Loss 0.0954 (0.0957)	
Pretraining:	Epoch: [73][70/235]	Loss 0.0923 (0.0961)	
Pretraining:	Epoch: [73][80/235]	Loss 0.0873 (0.0957)	
Pretraining:	Epoch: [73][90/235]	Loss 0.1001 (0.0958)	
Pretraining:	Epoch: [73][100/235]	Loss 0.0894 (0.0957)	
Pretraining:	Epoch: [73][110/235]	Loss 0.0881 (0.0962)	
Pretraining:	Epoch: [73][120/235]	Loss 0.0916 (0.0964)	
Pretraining:	Epoch: [73][130/235]	Loss 0.0919 (0.0965)	
Pretraining:	Epoch: [73][140/235]	Loss 0.0938 (0.0964)	
Pretraining:	Epoch: [73][150/235]	Loss 0.1145 (0.0967)	
Pretraining:	Epoch: [73][160/235]	Loss 0.0902 (0.0967)	
Pretraining:	Epoch: [73][170/235]	Loss 0.0950 (0.0967)	
Pretraining:	Epoch: [73][180/235]	Loss 0.1055 (0.0968)	
Pretraining:	Epoch: [73][190/235]	Loss 0.0995 (0.0971)	
Pretraining:	Epoch: [73][200/235]	Loss 0.0880 (0.0973)	
Pretraining:	Epoch: [73][210/235]	Loss 0.0956 (0.0973)	
Pretraining:	Epoch: [73][220/235]	Loss 0.0993 (0.0973)	
Pretraining:	Epoch: [73][230/235]	Loss 0.0961 (0.0972)	
Pretraining:	 Loss: 0.0972

Pretraining:	Epoch 74/100
----------
Pretraining:	Epoch: [74][10/235]	Loss 0.0886 (0.0927)	
Pretraining:	Epoch: [74][20/235]	Loss 0.0992 (0.0944)	
Pretraining:	Epoch: [74][30/235]	Loss 0.0878 (0.0944)	
Pretraining:	Epoch: [74][40/235]	Loss 0.0919 (0.0953)	
Pretraining:	Epoch: [74][50/235]	Loss 0.1012 (0.0948)	
Pretraining:	Epoch: [74][60/235]	Loss 0.0952 (0.0952)	
Pretraining:	Epoch: [74][70/235]	Loss 0.0920 (0.0956)	
Pretraining:	Epoch: [74][80/235]	Loss 0.0865 (0.0952)	
Pretraining:	Epoch: [74][90/235]	Loss 0.0991 (0.0953)	
Pretraining:	Epoch: [74][100/235]	Loss 0.0888 (0.0952)	
Pretraining:	Epoch: [74][110/235]	Loss 0.0873 (0.0956)	
Pretraining:	Epoch: [74][120/235]	Loss 0.0909 (0.0958)	
Pretraining:	Epoch: [74][130/235]	Loss 0.0911 (0.0959)	
Pretraining:	Epoch: [74][140/235]	Loss 0.0932 (0.0958)	
Pretraining:	Epoch: [74][150/235]	Loss 0.1140 (0.0961)	
Pretraining:	Epoch: [74][160/235]	Loss 0.0895 (0.0961)	
Pretraining:	Epoch: [74][170/235]	Loss 0.0942 (0.0961)	
Pretraining:	Epoch: [74][180/235]	Loss 0.1050 (0.0962)	
Pretraining:	Epoch: [74][190/235]	Loss 0.0993 (0.0965)	
Pretraining:	Epoch: [74][200/235]	Loss 0.0875 (0.0967)	
Pretraining:	Epoch: [74][210/235]	Loss 0.0953 (0.0967)	
Pretraining:	Epoch: [74][220/235]	Loss 0.0990 (0.0967)	
Pretraining:	Epoch: [74][230/235]	Loss 0.0957 (0.0967)	
Pretraining:	 Loss: 0.0966

Pretraining:	Epoch 75/100
----------
Pretraining:	Epoch: [75][10/235]	Loss 0.0880 (0.0923)	
Pretraining:	Epoch: [75][20/235]	Loss 0.0989 (0.0939)	
Pretraining:	Epoch: [75][30/235]	Loss 0.0874 (0.0939)	
Pretraining:	Epoch: [75][40/235]	Loss 0.0914 (0.0948)	
Pretraining:	Epoch: [75][50/235]	Loss 0.1007 (0.0943)	
Pretraining:	Epoch: [75][60/235]	Loss 0.0947 (0.0947)	
Pretraining:	Epoch: [75][70/235]	Loss 0.0917 (0.0952)	
Pretraining:	Epoch: [75][80/235]	Loss 0.0862 (0.0948)	
Pretraining:	Epoch: [75][90/235]	Loss 0.0979 (0.0948)	
Pretraining:	Epoch: [75][100/235]	Loss 0.0885 (0.0947)	
Pretraining:	Epoch: [75][110/235]	Loss 0.0866 (0.0951)	
Pretraining:	Epoch: [75][120/235]	Loss 0.0906 (0.0953)	
Pretraining:	Epoch: [75][130/235]	Loss 0.0907 (0.0954)	
Pretraining:	Epoch: [75][140/235]	Loss 0.0929 (0.0953)	
Pretraining:	Epoch: [75][150/235]	Loss 0.1136 (0.0957)	
Pretraining:	Epoch: [75][160/235]	Loss 0.0893 (0.0956)	
Pretraining:	Epoch: [75][170/235]	Loss 0.0938 (0.0957)	
Pretraining:	Epoch: [75][180/235]	Loss 0.1046 (0.0958)	
Pretraining:	Epoch: [75][190/235]	Loss 0.0989 (0.0961)	
Pretraining:	Epoch: [75][200/235]	Loss 0.0869 (0.0963)	
Pretraining:	Epoch: [75][210/235]	Loss 0.0948 (0.0963)	
Pretraining:	Epoch: [75][220/235]	Loss 0.0986 (0.0963)	
Pretraining:	Epoch: [75][230/235]	Loss 0.0951 (0.0962)	
Pretraining:	 Loss: 0.0962

Pretraining:	Epoch 76/100
----------
Pretraining:	Epoch: [76][10/235]	Loss 0.0876 (0.0920)	
Pretraining:	Epoch: [76][20/235]	Loss 0.0985 (0.0936)	
Pretraining:	Epoch: [76][30/235]	Loss 0.0872 (0.0936)	
Pretraining:	Epoch: [76][40/235]	Loss 0.0912 (0.0945)	
Pretraining:	Epoch: [76][50/235]	Loss 0.1003 (0.0940)	
Pretraining:	Epoch: [76][60/235]	Loss 0.0944 (0.0944)	
Pretraining:	Epoch: [76][70/235]	Loss 0.0913 (0.0949)	
Pretraining:	Epoch: [76][80/235]	Loss 0.0858 (0.0944)	
Pretraining:	Epoch: [76][90/235]	Loss 0.0978 (0.0945)	
Pretraining:	Epoch: [76][100/235]	Loss 0.0884 (0.0944)	
Pretraining:	Epoch: [76][110/235]	Loss 0.0864 (0.0948)	
Pretraining:	Epoch: [76][120/235]	Loss 0.0902 (0.0950)	
Pretraining:	Epoch: [76][130/235]	Loss 0.0906 (0.0951)	
Pretraining:	Epoch: [76][140/235]	Loss 0.0927 (0.0950)	
Pretraining:	Epoch: [76][150/235]	Loss 0.1134 (0.0954)	
Pretraining:	Epoch: [76][160/235]	Loss 0.0890 (0.0953)	
Pretraining:	Epoch: [76][170/235]	Loss 0.0935 (0.0954)	
Pretraining:	Epoch: [76][180/235]	Loss 0.1043 (0.0955)	
Pretraining:	Epoch: [76][190/235]	Loss 0.0987 (0.0958)	
Pretraining:	Epoch: [76][200/235]	Loss 0.0866 (0.0960)	
Pretraining:	Epoch: [76][210/235]	Loss 0.0946 (0.0960)	
Pretraining:	Epoch: [76][220/235]	Loss 0.0984 (0.0960)	
Pretraining:	Epoch: [76][230/235]	Loss 0.0949 (0.0960)	
Pretraining:	 Loss: 0.0959

Pretraining:	Epoch 77/100
----------
Pretraining:	Epoch: [77][10/235]	Loss 0.0874 (0.0917)	
Pretraining:	Epoch: [77][20/235]	Loss 0.0983 (0.0933)	
Pretraining:	Epoch: [77][30/235]	Loss 0.0868 (0.0934)	
Pretraining:	Epoch: [77][40/235]	Loss 0.0909 (0.0943)	
Pretraining:	Epoch: [77][50/235]	Loss 0.1000 (0.0937)	
Pretraining:	Epoch: [77][60/235]	Loss 0.0942 (0.0941)	
Pretraining:	Epoch: [77][70/235]	Loss 0.0913 (0.0946)	
Pretraining:	Epoch: [77][80/235]	Loss 0.0857 (0.0942)	
Pretraining:	Epoch: [77][90/235]	Loss 0.0968 (0.0943)	
Pretraining:	Epoch: [77][100/235]	Loss 0.0881 (0.0941)	
Pretraining:	Epoch: [77][110/235]	Loss 0.0860 (0.0945)	
Pretraining:	Epoch: [77][120/235]	Loss 0.0902 (0.0947)	
Pretraining:	Epoch: [77][130/235]	Loss 0.0914 (0.0949)	
Pretraining:	Epoch: [77][140/235]	Loss 0.0926 (0.0948)	
Pretraining:	Epoch: [77][150/235]	Loss 0.1132 (0.0952)	
Pretraining:	Epoch: [77][160/235]	Loss 0.0889 (0.0952)	
Pretraining:	Epoch: [77][170/235]	Loss 0.0933 (0.0952)	
Pretraining:	Epoch: [77][180/235]	Loss 0.1042 (0.0953)	
Pretraining:	Epoch: [77][190/235]	Loss 0.0984 (0.0956)	
Pretraining:	Epoch: [77][200/235]	Loss 0.0863 (0.0958)	
Pretraining:	Epoch: [77][210/235]	Loss 0.0943 (0.0958)	
Pretraining:	Epoch: [77][220/235]	Loss 0.0985 (0.0958)	
Pretraining:	Epoch: [77][230/235]	Loss 0.0944 (0.0958)	
Pretraining:	 Loss: 0.0957

Pretraining:	Epoch 78/100
----------
Pretraining:	Epoch: [78][10/235]	Loss 0.0872 (0.0915)	
Pretraining:	Epoch: [78][20/235]	Loss 0.0982 (0.0931)	
Pretraining:	Epoch: [78][30/235]	Loss 0.0868 (0.0932)	
Pretraining:	Epoch: [78][40/235]	Loss 0.0910 (0.0941)	
Pretraining:	Epoch: [78][50/235]	Loss 0.1000 (0.0936)	
Pretraining:	Epoch: [78][60/235]	Loss 0.0942 (0.0940)	
Pretraining:	Epoch: [78][70/235]	Loss 0.0913 (0.0945)	
Pretraining:	Epoch: [78][80/235]	Loss 0.0856 (0.0941)	
Pretraining:	Epoch: [78][90/235]	Loss 0.0970 (0.0942)	
Pretraining:	Epoch: [78][100/235]	Loss 0.0885 (0.0940)	
Pretraining:	Epoch: [78][110/235]	Loss 0.0859 (0.0945)	
Pretraining:	Epoch: [78][120/235]	Loss 0.0902 (0.0947)	
Pretraining:	Epoch: [78][130/235]	Loss 0.0935 (0.0950)	
Pretraining:	Epoch: [78][140/235]	Loss 0.0957 (0.0950)	
Pretraining:	Epoch: [78][150/235]	Loss 0.1140 (0.0955)	
Pretraining:	Epoch: [78][160/235]	Loss 0.0892 (0.0955)	
Pretraining:	Epoch: [78][170/235]	Loss 0.0933 (0.0956)	
Pretraining:	Epoch: [78][180/235]	Loss 0.1045 (0.0957)	
Pretraining:	Epoch: [78][190/235]	Loss 0.0982 (0.0960)	
Pretraining:	Epoch: [78][200/235]	Loss 0.0864 (0.0962)	
Pretraining:	Epoch: [78][210/235]	Loss 0.0948 (0.0961)	
Pretraining:	Epoch: [78][220/235]	Loss 0.0982 (0.0961)	
Pretraining:	Epoch: [78][230/235]	Loss 0.0943 (0.0961)	
Pretraining:	 Loss: 0.0960

Pretraining:	Epoch 79/100
----------
Pretraining:	Epoch: [79][10/235]	Loss 0.0874 (0.0912)	
Pretraining:	Epoch: [79][20/235]	Loss 0.0982 (0.0930)	
Pretraining:	Epoch: [79][30/235]	Loss 0.0864 (0.0932)	
Pretraining:	Epoch: [79][40/235]	Loss 0.0908 (0.0941)	
Pretraining:	Epoch: [79][50/235]	Loss 0.1001 (0.0937)	
Pretraining:	Epoch: [79][60/235]	Loss 0.0945 (0.0941)	
Pretraining:	Epoch: [79][70/235]	Loss 0.0913 (0.0946)	
Pretraining:	Epoch: [79][80/235]	Loss 0.0856 (0.0942)	
Pretraining:	Epoch: [79][90/235]	Loss 0.0988 (0.0943)	
Pretraining:	Epoch: [79][100/235]	Loss 0.0878 (0.0942)	
Pretraining:	Epoch: [79][110/235]	Loss 0.0857 (0.0946)	
Pretraining:	Epoch: [79][120/235]	Loss 0.0901 (0.0948)	
Pretraining:	Epoch: [79][130/235]	Loss 0.0925 (0.0950)	
Pretraining:	Epoch: [79][140/235]	Loss 0.0931 (0.0950)	
Pretraining:	Epoch: [79][150/235]	Loss 0.1132 (0.0953)	
Pretraining:	Epoch: [79][160/235]	Loss 0.0890 (0.0953)	
Pretraining:	Epoch: [79][170/235]	Loss 0.0931 (0.0953)	
Pretraining:	Epoch: [79][180/235]	Loss 0.1040 (0.0954)	
Pretraining:	Epoch: [79][190/235]	Loss 0.0984 (0.0957)	
Pretraining:	Epoch: [79][200/235]	Loss 0.0861 (0.0959)	
Pretraining:	Epoch: [79][210/235]	Loss 0.0946 (0.0958)	
Pretraining:	Epoch: [79][220/235]	Loss 0.0985 (0.0959)	
Pretraining:	Epoch: [79][230/235]	Loss 0.0944 (0.0958)	
Pretraining:	 Loss: 0.0957

Pretraining:	Epoch 80/100
----------
Pretraining:	Epoch: [80][10/235]	Loss 0.0873 (0.0912)	
Pretraining:	Epoch: [80][20/235]	Loss 0.0978 (0.0928)	
Pretraining:	Epoch: [80][30/235]	Loss 0.0860 (0.0929)	
Pretraining:	Epoch: [80][40/235]	Loss 0.0907 (0.0938)	
Pretraining:	Epoch: [80][50/235]	Loss 0.0994 (0.0933)	
Pretraining:	Epoch: [80][60/235]	Loss 0.0947 (0.0938)	
Pretraining:	Epoch: [80][70/235]	Loss 0.0916 (0.0944)	
Pretraining:	Epoch: [80][80/235]	Loss 0.0858 (0.0941)	
Pretraining:	Epoch: [80][90/235]	Loss 0.0970 (0.0942)	
Pretraining:	Epoch: [80][100/235]	Loss 0.0888 (0.0941)	
Pretraining:	Epoch: [80][110/235]	Loss 0.0880 (0.0945)	
Pretraining:	Epoch: [80][120/235]	Loss 0.0924 (0.0948)	
Pretraining:	Epoch: [80][130/235]	Loss 0.0908 (0.0950)	
Pretraining:	Epoch: [80][140/235]	Loss 0.0935 (0.0949)	
Pretraining:	Epoch: [80][150/235]	Loss 0.1147 (0.0954)	
Pretraining:	Epoch: [80][160/235]	Loss 0.0902 (0.0954)	
Pretraining:	Epoch: [80][170/235]	Loss 0.0937 (0.0954)	
Pretraining:	Epoch: [80][180/235]	Loss 0.1047 (0.0956)	
Pretraining:	Epoch: [80][190/235]	Loss 0.0992 (0.0959)	
Pretraining:	Epoch: [80][200/235]	Loss 0.0865 (0.0961)	
Pretraining:	Epoch: [80][210/235]	Loss 0.0943 (0.0960)	
Pretraining:	Epoch: [80][220/235]	Loss 0.0983 (0.0960)	
Pretraining:	Epoch: [80][230/235]	Loss 0.0946 (0.0960)	
Pretraining:	 Loss: 0.0959

Pretraining:	Epoch 81/100
----------
Pretraining:	Epoch: [81][10/235]	Loss 0.0870 (0.0914)	
Pretraining:	Epoch: [81][20/235]	Loss 0.0981 (0.0931)	
Pretraining:	Epoch: [81][30/235]	Loss 0.0862 (0.0931)	
Pretraining:	Epoch: [81][40/235]	Loss 0.0908 (0.0940)	
Pretraining:	Epoch: [81][50/235]	Loss 0.0997 (0.0935)	
Pretraining:	Epoch: [81][60/235]	Loss 0.0940 (0.0939)	
Pretraining:	Epoch: [81][70/235]	Loss 0.0924 (0.0944)	
Pretraining:	Epoch: [81][80/235]	Loss 0.0866 (0.0941)	
Pretraining:	Epoch: [81][90/235]	Loss 0.0969 (0.0944)	
Pretraining:	Epoch: [81][100/235]	Loss 0.0879 (0.0942)	
Pretraining:	Epoch: [81][110/235]	Loss 0.0859 (0.0946)	
Pretraining:	Epoch: [81][120/235]	Loss 0.0900 (0.0948)	
Pretraining:	Epoch: [81][130/235]	Loss 0.0909 (0.0949)	
Pretraining:	Epoch: [81][140/235]	Loss 0.0930 (0.0948)	
Pretraining:	Epoch: [81][150/235]	Loss 0.1131 (0.0952)	
Pretraining:	Epoch: [81][160/235]	Loss 0.0895 (0.0952)	
Pretraining:	Epoch: [81][170/235]	Loss 0.0933 (0.0952)	
Pretraining:	Epoch: [81][180/235]	Loss 0.1040 (0.0953)	
Pretraining:	Epoch: [81][190/235]	Loss 0.0985 (0.0956)	
Pretraining:	Epoch: [81][200/235]	Loss 0.0860 (0.0958)	
Pretraining:	Epoch: [81][210/235]	Loss 0.0943 (0.0958)	
Pretraining:	Epoch: [81][220/235]	Loss 0.0980 (0.0958)	
Pretraining:	Epoch: [81][230/235]	Loss 0.0948 (0.0957)	
Pretraining:	 Loss: 0.0957

Pretraining:	Epoch 82/100
----------
Pretraining:	Epoch: [82][10/235]	Loss 0.0873 (0.0912)	
Pretraining:	Epoch: [82][20/235]	Loss 0.0979 (0.0928)	
Pretraining:	Epoch: [82][30/235]	Loss 0.0862 (0.0929)	
Pretraining:	Epoch: [82][40/235]	Loss 0.0908 (0.0938)	
Pretraining:	Epoch: [82][50/235]	Loss 0.0992 (0.0932)	
Pretraining:	Epoch: [82][60/235]	Loss 0.0936 (0.0936)	
Pretraining:	Epoch: [82][70/235]	Loss 0.0910 (0.0941)	
Pretraining:	Epoch: [82][80/235]	Loss 0.0859 (0.0937)	
Pretraining:	Epoch: [82][90/235]	Loss 0.0961 (0.0939)	
Pretraining:	Epoch: [82][100/235]	Loss 0.0873 (0.0937)	
Pretraining:	Epoch: [82][110/235]	Loss 0.0855 (0.0941)	
Pretraining:	Epoch: [82][120/235]	Loss 0.0894 (0.0943)	
Pretraining:	Epoch: [82][130/235]	Loss 0.0899 (0.0944)	
Pretraining:	Epoch: [82][140/235]	Loss 0.0928 (0.0943)	
Pretraining:	Epoch: [82][150/235]	Loss 0.1129 (0.0947)	
Pretraining:	Epoch: [82][160/235]	Loss 0.0891 (0.0947)	
Pretraining:	Epoch: [82][170/235]	Loss 0.0935 (0.0948)	
Pretraining:	Epoch: [82][180/235]	Loss 0.1040 (0.0949)	
Pretraining:	Epoch: [82][190/235]	Loss 0.0982 (0.0952)	
Pretraining:	Epoch: [82][200/235]	Loss 0.0857 (0.0954)	
Pretraining:	Epoch: [82][210/235]	Loss 0.0940 (0.0954)	
Pretraining:	Epoch: [82][220/235]	Loss 0.0979 (0.0954)	
Pretraining:	Epoch: [82][230/235]	Loss 0.0944 (0.0953)	
Pretraining:	 Loss: 0.0953

Pretraining:	Epoch 83/100
----------
Pretraining:	Epoch: [83][10/235]	Loss 0.0869 (0.0910)	
Pretraining:	Epoch: [83][20/235]	Loss 0.0976 (0.0925)	
Pretraining:	Epoch: [83][30/235]	Loss 0.0860 (0.0926)	
Pretraining:	Epoch: [83][40/235]	Loss 0.0905 (0.0935)	
Pretraining:	Epoch: [83][50/235]	Loss 0.0988 (0.0930)	
Pretraining:	Epoch: [83][60/235]	Loss 0.0935 (0.0934)	
Pretraining:	Epoch: [83][70/235]	Loss 0.0910 (0.0939)	
Pretraining:	Epoch: [83][80/235]	Loss 0.0856 (0.0935)	
Pretraining:	Epoch: [83][90/235]	Loss 0.0957 (0.0937)	
Pretraining:	Epoch: [83][100/235]	Loss 0.0870 (0.0935)	
Pretraining:	Epoch: [83][110/235]	Loss 0.0850 (0.0939)	
Pretraining:	Epoch: [83][120/235]	Loss 0.0895 (0.0941)	
Pretraining:	Epoch: [83][130/235]	Loss 0.0895 (0.0942)	
Pretraining:	Epoch: [83][140/235]	Loss 0.0927 (0.0941)	
Pretraining:	Epoch: [83][150/235]	Loss 0.1128 (0.0945)	
Pretraining:	Epoch: [83][160/235]	Loss 0.0890 (0.0945)	
Pretraining:	Epoch: [83][170/235]	Loss 0.0934 (0.0946)	
Pretraining:	Epoch: [83][180/235]	Loss 0.1039 (0.0947)	
Pretraining:	Epoch: [83][190/235]	Loss 0.0978 (0.0950)	
Pretraining:	Epoch: [83][200/235]	Loss 0.0858 (0.0952)	
Pretraining:	Epoch: [83][210/235]	Loss 0.0938 (0.0952)	
Pretraining:	Epoch: [83][220/235]	Loss 0.0977 (0.0952)	
Pretraining:	Epoch: [83][230/235]	Loss 0.0942 (0.0951)	
Pretraining:	 Loss: 0.0951

Pretraining:	Epoch 84/100
----------
Pretraining:	Epoch: [84][10/235]	Loss 0.0870 (0.0911)	
Pretraining:	Epoch: [84][20/235]	Loss 0.0975 (0.0925)	
Pretraining:	Epoch: [84][30/235]	Loss 0.0859 (0.0925)	
Pretraining:	Epoch: [84][40/235]	Loss 0.0902 (0.0934)	
Pretraining:	Epoch: [84][50/235]	Loss 0.0987 (0.0929)	
Pretraining:	Epoch: [84][60/235]	Loss 0.0934 (0.0933)	
Pretraining:	Epoch: [84][70/235]	Loss 0.0911 (0.0938)	
Pretraining:	Epoch: [84][80/235]	Loss 0.0853 (0.0935)	
Pretraining:	Epoch: [84][90/235]	Loss 0.0961 (0.0936)	
Pretraining:	Epoch: [84][100/235]	Loss 0.0873 (0.0934)	
Pretraining:	Epoch: [84][110/235]	Loss 0.0848 (0.0938)	
Pretraining:	Epoch: [84][120/235]	Loss 0.0894 (0.0940)	
Pretraining:	Epoch: [84][130/235]	Loss 0.0899 (0.0941)	
Pretraining:	Epoch: [84][140/235]	Loss 0.0928 (0.0941)	
Pretraining:	Epoch: [84][150/235]	Loss 0.1134 (0.0945)	
Pretraining:	Epoch: [84][160/235]	Loss 0.0887 (0.0945)	
Pretraining:	Epoch: [84][170/235]	Loss 0.0933 (0.0945)	
Pretraining:	Epoch: [84][180/235]	Loss 0.1040 (0.0947)	
Pretraining:	Epoch: [84][190/235]	Loss 0.0975 (0.0950)	
Pretraining:	Epoch: [84][200/235]	Loss 0.0861 (0.0952)	
Pretraining:	Epoch: [84][210/235]	Loss 0.0940 (0.0951)	
Pretraining:	Epoch: [84][220/235]	Loss 0.0978 (0.0952)	
Pretraining:	Epoch: [84][230/235]	Loss 0.0943 (0.0951)	
Pretraining:	 Loss: 0.0950

Pretraining:	Epoch 85/100
----------
Pretraining:	Epoch: [85][10/235]	Loss 0.0873 (0.0912)	
Pretraining:	Epoch: [85][20/235]	Loss 0.0977 (0.0926)	
Pretraining:	Epoch: [85][30/235]	Loss 0.0860 (0.0926)	
Pretraining:	Epoch: [85][40/235]	Loss 0.0902 (0.0934)	
Pretraining:	Epoch: [85][50/235]	Loss 0.0983 (0.0929)	
Pretraining:	Epoch: [85][60/235]	Loss 0.0935 (0.0933)	
Pretraining:	Epoch: [85][70/235]	Loss 0.0913 (0.0939)	
Pretraining:	Epoch: [85][80/235]	Loss 0.0856 (0.0936)	
Pretraining:	Epoch: [85][90/235]	Loss 0.0965 (0.0937)	
Pretraining:	Epoch: [85][100/235]	Loss 0.0873 (0.0936)	
Pretraining:	Epoch: [85][110/235]	Loss 0.0849 (0.0940)	
Pretraining:	Epoch: [85][120/235]	Loss 0.0894 (0.0942)	
Pretraining:	Epoch: [85][130/235]	Loss 0.0900 (0.0943)	
Pretraining:	Epoch: [85][140/235]	Loss 0.0937 (0.0943)	
Pretraining:	Epoch: [85][150/235]	Loss 0.1147 (0.0947)	
Pretraining:	Epoch: [85][160/235]	Loss 0.0881 (0.0947)	
Pretraining:	Epoch: [85][170/235]	Loss 0.0946 (0.0948)	
Pretraining:	Epoch: [85][180/235]	Loss 0.1045 (0.0949)	
Pretraining:	Epoch: [85][190/235]	Loss 0.0982 (0.0952)	
Pretraining:	Epoch: [85][200/235]	Loss 0.0869 (0.0955)	
Pretraining:	Epoch: [85][210/235]	Loss 0.0942 (0.0955)	
Pretraining:	Epoch: [85][220/235]	Loss 0.0978 (0.0955)	
Pretraining:	Epoch: [85][230/235]	Loss 0.0943 (0.0954)	
Pretraining:	 Loss: 0.0954

Pretraining:	Epoch 86/100
----------
Pretraining:	Epoch: [86][10/235]	Loss 0.0885 (0.0918)	
Pretraining:	Epoch: [86][20/235]	Loss 0.0981 (0.0931)	
Pretraining:	Epoch: [86][30/235]	Loss 0.0862 (0.0931)	
Pretraining:	Epoch: [86][40/235]	Loss 0.0903 (0.0939)	
Pretraining:	Epoch: [86][50/235]	Loss 0.0985 (0.0933)	
Pretraining:	Epoch: [86][60/235]	Loss 0.0932 (0.0937)	
Pretraining:	Epoch: [86][70/235]	Loss 0.0909 (0.0942)	
Pretraining:	Epoch: [86][80/235]	Loss 0.0856 (0.0939)	
Pretraining:	Epoch: [86][90/235]	Loss 0.0977 (0.0941)	
Pretraining:	Epoch: [86][100/235]	Loss 0.0886 (0.0940)	
Pretraining:	Epoch: [86][110/235]	Loss 0.0870 (0.0945)	
Pretraining:	Epoch: [86][120/235]	Loss 0.0900 (0.0947)	
Pretraining:	Epoch: [86][130/235]	Loss 0.0904 (0.0948)	
Pretraining:	Epoch: [86][140/235]	Loss 0.0954 (0.0948)	
Pretraining:	Epoch: [86][150/235]	Loss 0.1129 (0.0953)	
Pretraining:	Epoch: [86][160/235]	Loss 0.0899 (0.0955)	
Pretraining:	Epoch: [86][170/235]	Loss 0.0942 (0.0957)	
Pretraining:	Epoch: [86][180/235]	Loss 0.1045 (0.0958)	
Pretraining:	Epoch: [86][190/235]	Loss 0.0984 (0.0961)	
Pretraining:	Epoch: [86][200/235]	Loss 0.0871 (0.0963)	
Pretraining:	Epoch: [86][210/235]	Loss 0.0959 (0.0963)	
Pretraining:	Epoch: [86][220/235]	Loss 0.0999 (0.0963)	
Pretraining:	Epoch: [86][230/235]	Loss 0.0962 (0.0963)	
Pretraining:	 Loss: 0.0962

Pretraining:	Epoch 87/100
----------
Pretraining:	Epoch: [87][10/235]	Loss 0.0899 (0.0930)	
Pretraining:	Epoch: [87][20/235]	Loss 0.0996 (0.0945)	
Pretraining:	Epoch: [87][30/235]	Loss 0.0870 (0.0943)	
Pretraining:	Epoch: [87][40/235]	Loss 0.0909 (0.0951)	
Pretraining:	Epoch: [87][50/235]	Loss 0.0999 (0.0945)	
Pretraining:	Epoch: [87][60/235]	Loss 0.0946 (0.0948)	
Pretraining:	Epoch: [87][70/235]	Loss 0.0920 (0.0953)	
Pretraining:	Epoch: [87][80/235]	Loss 0.0871 (0.0949)	
Pretraining:	Epoch: [87][90/235]	Loss 0.0990 (0.0951)	
Pretraining:	Epoch: [87][100/235]	Loss 0.0912 (0.0951)	
Pretraining:	Epoch: [87][110/235]	Loss 0.0883 (0.0956)	
Pretraining:	Epoch: [87][120/235]	Loss 0.0920 (0.0957)	
Pretraining:	Epoch: [87][130/235]	Loss 0.0927 (0.0958)	
Pretraining:	Epoch: [87][140/235]	Loss 0.0944 (0.0958)	
Pretraining:	Epoch: [87][150/235]	Loss 0.1132 (0.0961)	
Pretraining:	Epoch: [87][160/235]	Loss 0.0883 (0.0960)	
Pretraining:	Epoch: [87][170/235]	Loss 0.0935 (0.0960)	
Pretraining:	Epoch: [87][180/235]	Loss 0.1040 (0.0961)	
Pretraining:	Epoch: [87][190/235]	Loss 0.0984 (0.0963)	
Pretraining:	Epoch: [87][200/235]	Loss 0.0873 (0.0965)	
Pretraining:	Epoch: [87][210/235]	Loss 0.0947 (0.0964)	
Pretraining:	Epoch: [87][220/235]	Loss 0.1003 (0.0965)	
Pretraining:	Epoch: [87][230/235]	Loss 0.0963 (0.0964)	
Pretraining:	 Loss: 0.0964

Pretraining:	Epoch 88/100
----------
Pretraining:	Epoch: [88][10/235]	Loss 0.0897 (0.0926)	
Pretraining:	Epoch: [88][20/235]	Loss 0.0993 (0.0941)	
Pretraining:	Epoch: [88][30/235]	Loss 0.0872 (0.0940)	
Pretraining:	Epoch: [88][40/235]	Loss 0.0919 (0.0948)	
Pretraining:	Epoch: [88][50/235]	Loss 0.0996 (0.0943)	
Pretraining:	Epoch: [88][60/235]	Loss 0.0952 (0.0947)	
Pretraining:	Epoch: [88][70/235]	Loss 0.0916 (0.0952)	
Pretraining:	Epoch: [88][80/235]	Loss 0.0879 (0.0949)	
Pretraining:	Epoch: [88][90/235]	Loss 0.0969 (0.0950)	
Pretraining:	Epoch: [88][100/235]	Loss 0.0913 (0.0949)	
Pretraining:	Epoch: [88][110/235]	Loss 0.0879 (0.0954)	
Pretraining:	Epoch: [88][120/235]	Loss 0.0917 (0.0956)	
Pretraining:	Epoch: [88][130/235]	Loss 0.0912 (0.0957)	
Pretraining:	Epoch: [88][140/235]	Loss 0.0944 (0.0956)	
Pretraining:	Epoch: [88][150/235]	Loss 0.1142 (0.0960)	
Pretraining:	Epoch: [88][160/235]	Loss 0.0882 (0.0959)	
Pretraining:	Epoch: [88][170/235]	Loss 0.0935 (0.0959)	
Pretraining:	Epoch: [88][180/235]	Loss 0.1035 (0.0959)	
Pretraining:	Epoch: [88][190/235]	Loss 0.0982 (0.0962)	
Pretraining:	Epoch: [88][200/235]	Loss 0.0878 (0.0964)	
Pretraining:	Epoch: [88][210/235]	Loss 0.0951 (0.0963)	
Pretraining:	Epoch: [88][220/235]	Loss 0.0995 (0.0964)	
Pretraining:	Epoch: [88][230/235]	Loss 0.0970 (0.0963)	
Pretraining:	 Loss: 0.0963

Pretraining:	Epoch 89/100
----------
Pretraining:	Epoch: [89][10/235]	Loss 0.0886 (0.0929)	
Pretraining:	Epoch: [89][20/235]	Loss 0.0996 (0.0944)	
Pretraining:	Epoch: [89][30/235]	Loss 0.0872 (0.0944)	
Pretraining:	Epoch: [89][40/235]	Loss 0.0922 (0.0951)	
Pretraining:	Epoch: [89][50/235]	Loss 0.1001 (0.0945)	
Pretraining:	Epoch: [89][60/235]	Loss 0.0958 (0.0949)	
Pretraining:	Epoch: [89][70/235]	Loss 0.0926 (0.0954)	
Pretraining:	Epoch: [89][80/235]	Loss 0.0880 (0.0951)	
Pretraining:	Epoch: [89][90/235]	Loss 0.0965 (0.0953)	
Pretraining:	Epoch: [89][100/235]	Loss 0.0905 (0.0952)	
Pretraining:	Epoch: [89][110/235]	Loss 0.0868 (0.0956)	
Pretraining:	Epoch: [89][120/235]	Loss 0.0904 (0.0958)	
Pretraining:	Epoch: [89][130/235]	Loss 0.0916 (0.0959)	
Pretraining:	Epoch: [89][140/235]	Loss 0.0939 (0.0958)	
Pretraining:	Epoch: [89][150/235]	Loss 0.1142 (0.0961)	
Pretraining:	Epoch: [89][160/235]	Loss 0.0886 (0.0961)	
Pretraining:	Epoch: [89][170/235]	Loss 0.0950 (0.0961)	
Pretraining:	Epoch: [89][180/235]	Loss 0.1036 (0.0962)	
Pretraining:	Epoch: [89][190/235]	Loss 0.0985 (0.0964)	
Pretraining:	Epoch: [89][200/235]	Loss 0.0882 (0.0966)	
Pretraining:	Epoch: [89][210/235]	Loss 0.0957 (0.0966)	
Pretraining:	Epoch: [89][220/235]	Loss 0.0994 (0.0967)	
Pretraining:	Epoch: [89][230/235]	Loss 0.0963 (0.0966)	
Pretraining:	 Loss: 0.0966

Pretraining:	Epoch 90/100
----------
Pretraining:	Epoch: [90][10/235]	Loss 0.0884 (0.0937)	
Pretraining:	Epoch: [90][20/235]	Loss 0.1003 (0.0949)	
Pretraining:	Epoch: [90][30/235]	Loss 0.0869 (0.0948)	
Pretraining:	Epoch: [90][40/235]	Loss 0.0912 (0.0954)	
Pretraining:	Epoch: [90][50/235]	Loss 0.0995 (0.0946)	
Pretraining:	Epoch: [90][60/235]	Loss 0.0951 (0.0949)	
Pretraining:	Epoch: [90][70/235]	Loss 0.0925 (0.0954)	
Pretraining:	Epoch: [90][80/235]	Loss 0.0882 (0.0952)	
Pretraining:	Epoch: [90][90/235]	Loss 0.0986 (0.0955)	
Pretraining:	Epoch: [90][100/235]	Loss 0.0895 (0.0955)	
Pretraining:	Epoch: [90][110/235]	Loss 0.0870 (0.0960)	
Pretraining:	Epoch: [90][120/235]	Loss 0.0905 (0.0962)	
Pretraining:	Epoch: [90][130/235]	Loss 0.0922 (0.0962)	
Pretraining:	Epoch: [90][140/235]	Loss 0.0940 (0.0961)	
Pretraining:	Epoch: [90][150/235]	Loss 0.1138 (0.0965)	
Pretraining:	Epoch: [90][160/235]	Loss 0.0895 (0.0964)	
Pretraining:	Epoch: [90][170/235]	Loss 0.0950 (0.0964)	
Pretraining:	Epoch: [90][180/235]	Loss 0.1040 (0.0966)	
Pretraining:	Epoch: [90][190/235]	Loss 0.0989 (0.0968)	
Pretraining:	Epoch: [90][200/235]	Loss 0.0879 (0.0970)	
Pretraining:	Epoch: [90][210/235]	Loss 0.0962 (0.0969)	
Pretraining:	Epoch: [90][220/235]	Loss 0.1008 (0.0970)	
Pretraining:	Epoch: [90][230/235]	Loss 0.0962 (0.0970)	
Pretraining:	 Loss: 0.0969

Pretraining:	Epoch 91/100
----------
Pretraining:	Epoch: [91][10/235]	Loss 0.0899 (0.0941)	
Pretraining:	Epoch: [91][20/235]	Loss 0.1000 (0.0955)	
Pretraining:	Epoch: [91][30/235]	Loss 0.0875 (0.0954)	
Pretraining:	Epoch: [91][40/235]	Loss 0.0915 (0.0960)	
Pretraining:	Epoch: [91][50/235]	Loss 0.1000 (0.0951)	
Pretraining:	Epoch: [91][60/235]	Loss 0.0941 (0.0953)	
Pretraining:	Epoch: [91][70/235]	Loss 0.0918 (0.0957)	
Pretraining:	Epoch: [91][80/235]	Loss 0.0880 (0.0954)	
Pretraining:	Epoch: [91][90/235]	Loss 0.0995 (0.0956)	
Pretraining:	Epoch: [91][100/235]	Loss 0.0909 (0.0957)	
Pretraining:	Epoch: [91][110/235]	Loss 0.0894 (0.0962)	
Pretraining:	Epoch: [91][120/235]	Loss 0.0938 (0.0965)	
Pretraining:	Epoch: [91][130/235]	Loss 0.0942 (0.0966)	
Pretraining:	Epoch: [91][140/235]	Loss 0.0949 (0.0965)	
Pretraining:	Epoch: [91][150/235]	Loss 0.1153 (0.0969)	
Pretraining:	Epoch: [91][160/235]	Loss 0.0904 (0.0968)	
Pretraining:	Epoch: [91][170/235]	Loss 0.0953 (0.0968)	
Pretraining:	Epoch: [91][180/235]	Loss 0.1048 (0.0970)	
Pretraining:	Epoch: [91][190/235]	Loss 0.0993 (0.0972)	
Pretraining:	Epoch: [91][200/235]	Loss 0.0889 (0.0975)	
Pretraining:	Epoch: [91][210/235]	Loss 0.0972 (0.0974)	
Pretraining:	Epoch: [91][220/235]	Loss 0.1018 (0.0974)	
Pretraining:	Epoch: [91][230/235]	Loss 0.0974 (0.0974)	
Pretraining:	 Loss: 0.0973

Pretraining:	Epoch 92/100
----------
Pretraining:	Epoch: [92][10/235]	Loss 0.0908 (0.0950)	
Pretraining:	Epoch: [92][20/235]	Loss 0.0992 (0.0963)	
Pretraining:	Epoch: [92][30/235]	Loss 0.0890 (0.0960)	
Pretraining:	Epoch: [92][40/235]	Loss 0.0925 (0.0966)	
Pretraining:	Epoch: [92][50/235]	Loss 0.0998 (0.0957)	
Pretraining:	Epoch: [92][60/235]	Loss 0.0945 (0.0958)	
Pretraining:	Epoch: [92][70/235]	Loss 0.0919 (0.0961)	
Pretraining:	Epoch: [92][80/235]	Loss 0.0888 (0.0958)	
Pretraining:	Epoch: [92][90/235]	Loss 0.0998 (0.0959)	
Pretraining:	Epoch: [92][100/235]	Loss 0.0926 (0.0958)	
Pretraining:	Epoch: [92][110/235]	Loss 0.0898 (0.0963)	
Pretraining:	Epoch: [92][120/235]	Loss 0.0952 (0.0966)	
Pretraining:	Epoch: [92][130/235]	Loss 0.0922 (0.0967)	
Pretraining:	Epoch: [92][140/235]	Loss 0.0937 (0.0965)	
Pretraining:	Epoch: [92][150/235]	Loss 0.1140 (0.0968)	
Pretraining:	Epoch: [92][160/235]	Loss 0.0897 (0.0967)	
Pretraining:	Epoch: [92][170/235]	Loss 0.0949 (0.0967)	
Pretraining:	Epoch: [92][180/235]	Loss 0.1063 (0.0968)	
Pretraining:	Epoch: [92][190/235]	Loss 0.0990 (0.0971)	
Pretraining:	Epoch: [92][200/235]	Loss 0.0882 (0.0973)	
Pretraining:	Epoch: [92][210/235]	Loss 0.0966 (0.0972)	
Pretraining:	Epoch: [92][220/235]	Loss 0.1000 (0.0972)	
Pretraining:	Epoch: [92][230/235]	Loss 0.0973 (0.0972)	
Pretraining:	 Loss: 0.0971

Pretraining:	Epoch 93/100
----------
Pretraining:	Epoch: [93][10/235]	Loss 0.0887 (0.0933)	
Pretraining:	Epoch: [93][20/235]	Loss 0.0988 (0.0947)	
Pretraining:	Epoch: [93][30/235]	Loss 0.0879 (0.0945)	
Pretraining:	Epoch: [93][40/235]	Loss 0.0934 (0.0953)	
Pretraining:	Epoch: [93][50/235]	Loss 0.1000 (0.0948)	
Pretraining:	Epoch: [93][60/235]	Loss 0.0948 (0.0949)	
Pretraining:	Epoch: [93][70/235]	Loss 0.0919 (0.0953)	
Pretraining:	Epoch: [93][80/235]	Loss 0.0891 (0.0950)	
Pretraining:	Epoch: [93][90/235]	Loss 0.0991 (0.0951)	
Pretraining:	Epoch: [93][100/235]	Loss 0.0913 (0.0951)	
Pretraining:	Epoch: [93][110/235]	Loss 0.0872 (0.0955)	
Pretraining:	Epoch: [93][120/235]	Loss 0.0923 (0.0957)	
Pretraining:	Epoch: [93][130/235]	Loss 0.0905 (0.0958)	
Pretraining:	Epoch: [93][140/235]	Loss 0.0928 (0.0956)	
Pretraining:	Epoch: [93][150/235]	Loss 0.1132 (0.0959)	
Pretraining:	Epoch: [93][160/235]	Loss 0.0884 (0.0957)	
Pretraining:	Epoch: [93][170/235]	Loss 0.0939 (0.0957)	
Pretraining:	Epoch: [93][180/235]	Loss 0.1048 (0.0958)	
Pretraining:	Epoch: [93][190/235]	Loss 0.0977 (0.0960)	
Pretraining:	Epoch: [93][200/235]	Loss 0.0871 (0.0962)	
Pretraining:	Epoch: [93][210/235]	Loss 0.0952 (0.0962)	
Pretraining:	Epoch: [93][220/235]	Loss 0.0990 (0.0962)	
Pretraining:	Epoch: [93][230/235]	Loss 0.0961 (0.0961)	
Pretraining:	 Loss: 0.0961

Pretraining:	Epoch 94/100
----------
Pretraining:	Epoch: [94][10/235]	Loss 0.0875 (0.0917)	
Pretraining:	Epoch: [94][20/235]	Loss 0.0981 (0.0932)	
Pretraining:	Epoch: [94][30/235]	Loss 0.0864 (0.0931)	
Pretraining:	Epoch: [94][40/235]	Loss 0.0916 (0.0939)	
Pretraining:	Epoch: [94][50/235]	Loss 0.0988 (0.0934)	
Pretraining:	Epoch: [94][60/235]	Loss 0.0938 (0.0936)	
Pretraining:	Epoch: [94][70/235]	Loss 0.0906 (0.0940)	
Pretraining:	Epoch: [94][80/235]	Loss 0.0874 (0.0937)	
Pretraining:	Epoch: [94][90/235]	Loss 0.0973 (0.0939)	
Pretraining:	Epoch: [94][100/235]	Loss 0.0895 (0.0938)	
Pretraining:	Epoch: [94][110/235]	Loss 0.0855 (0.0942)	
Pretraining:	Epoch: [94][120/235]	Loss 0.0908 (0.0944)	
Pretraining:	Epoch: [94][130/235]	Loss 0.0896 (0.0945)	
Pretraining:	Epoch: [94][140/235]	Loss 0.0917 (0.0943)	
Pretraining:	Epoch: [94][150/235]	Loss 0.1120 (0.0946)	
Pretraining:	Epoch: [94][160/235]	Loss 0.0876 (0.0945)	
Pretraining:	Epoch: [94][170/235]	Loss 0.0934 (0.0945)	
Pretraining:	Epoch: [94][180/235]	Loss 0.1036 (0.0946)	
Pretraining:	Epoch: [94][190/235]	Loss 0.0968 (0.0949)	
Pretraining:	Epoch: [94][200/235]	Loss 0.0864 (0.0951)	
Pretraining:	Epoch: [94][210/235]	Loss 0.0945 (0.0950)	
Pretraining:	Epoch: [94][220/235]	Loss 0.0980 (0.0951)	
Pretraining:	Epoch: [94][230/235]	Loss 0.0953 (0.0950)	
Pretraining:	 Loss: 0.0950

Pretraining:	Epoch 95/100
----------
Pretraining:	Epoch: [95][10/235]	Loss 0.0866 (0.0907)	
Pretraining:	Epoch: [95][20/235]	Loss 0.0974 (0.0922)	
Pretraining:	Epoch: [95][30/235]	Loss 0.0852 (0.0922)	
Pretraining:	Epoch: [95][40/235]	Loss 0.0903 (0.0930)	
Pretraining:	Epoch: [95][50/235]	Loss 0.0978 (0.0925)	
Pretraining:	Epoch: [95][60/235]	Loss 0.0929 (0.0927)	
Pretraining:	Epoch: [95][70/235]	Loss 0.0896 (0.0932)	
Pretraining:	Epoch: [95][80/235]	Loss 0.0858 (0.0928)	
Pretraining:	Epoch: [95][90/235]	Loss 0.0954 (0.0929)	
Pretraining:	Epoch: [95][100/235]	Loss 0.0878 (0.0928)	
Pretraining:	Epoch: [95][110/235]	Loss 0.0842 (0.0932)	
Pretraining:	Epoch: [95][120/235]	Loss 0.0894 (0.0934)	
Pretraining:	Epoch: [95][130/235]	Loss 0.0885 (0.0934)	
Pretraining:	Epoch: [95][140/235]	Loss 0.0909 (0.0933)	
Pretraining:	Epoch: [95][150/235]	Loss 0.1112 (0.0936)	
Pretraining:	Epoch: [95][160/235]	Loss 0.0869 (0.0935)	
Pretraining:	Epoch: [95][170/235]	Loss 0.0926 (0.0935)	
Pretraining:	Epoch: [95][180/235]	Loss 0.1025 (0.0937)	
Pretraining:	Epoch: [95][190/235]	Loss 0.0960 (0.0939)	
Pretraining:	Epoch: [95][200/235]	Loss 0.0856 (0.0941)	
Pretraining:	Epoch: [95][210/235]	Loss 0.0938 (0.0941)	
Pretraining:	Epoch: [95][220/235]	Loss 0.0974 (0.0941)	
Pretraining:	Epoch: [95][230/235]	Loss 0.0944 (0.0941)	
Pretraining:	 Loss: 0.0940

Pretraining:	Epoch 96/100
----------
Pretraining:	Epoch: [96][10/235]	Loss 0.0859 (0.0899)	
Pretraining:	Epoch: [96][20/235]	Loss 0.0966 (0.0915)	
Pretraining:	Epoch: [96][30/235]	Loss 0.0843 (0.0915)	
Pretraining:	Epoch: [96][40/235]	Loss 0.0892 (0.0923)	
Pretraining:	Epoch: [96][50/235]	Loss 0.0968 (0.0917)	
Pretraining:	Epoch: [96][60/235]	Loss 0.0923 (0.0920)	
Pretraining:	Epoch: [96][70/235]	Loss 0.0889 (0.0925)	
Pretraining:	Epoch: [96][80/235]	Loss 0.0849 (0.0921)	
Pretraining:	Epoch: [96][90/235]	Loss 0.0945 (0.0922)	
Pretraining:	Epoch: [96][100/235]	Loss 0.0868 (0.0920)	
Pretraining:	Epoch: [96][110/235]	Loss 0.0836 (0.0924)	
Pretraining:	Epoch: [96][120/235]	Loss 0.0887 (0.0926)	
Pretraining:	Epoch: [96][130/235]	Loss 0.0883 (0.0927)	
Pretraining:	Epoch: [96][140/235]	Loss 0.0904 (0.0926)	
Pretraining:	Epoch: [96][150/235]	Loss 0.1106 (0.0929)	
Pretraining:	Epoch: [96][160/235]	Loss 0.0864 (0.0928)	
