Training the 'CAE_bn3' architecture

The following parameters are used:
Batch size:	256
Number of workers:	4
Learning rate:	0.001
Pretraining learning rate:	0.001
Weight decay:	0.0
Pretraining weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Pretraining scheduler steps:	200
Pretraining scheduler gamma:	0.1
Number of epochs of training:	1000
Number of epochs of pretraining:	100
Clustering loss weight:	0.1
Update interval for target distribution:	80
Stop criterium tolerance:	0.01
Number of clusters:	10
Leaky relu:	True
Leaky slope:	0.01
Activations:	False
Bias:	True

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/100
----------
Pretraining:	Epoch: [1][50/235]	Loss 0.6122 (0.8384)	
Pretraining:	Epoch: [1][100/235]	Loss 0.4090 (0.6697)	
Pretraining:	Epoch: [1][150/235]	Loss 0.3742 (0.5772)	
Pretraining:	Epoch: [1][200/235]	Loss 0.3254 (0.5207)	
Pretraining:	 Loss: 0.4925

Pretraining:	Epoch 2/100
----------
Pretraining:	Epoch: [2][50/235]	Loss 0.3308 (0.3115)	
Pretraining:	Epoch: [2][100/235]	Loss 0.2634 (0.3046)	
Pretraining:	Epoch: [2][150/235]	Loss 0.2857 (0.2985)	
Pretraining:	Epoch: [2][200/235]	Loss 0.2612 (0.2921)	
Pretraining:	 Loss: 0.2878

Pretraining:	Epoch 3/100
----------
Pretraining:	Epoch: [3][50/235]	Loss 0.2718 (0.2571)	
Pretraining:	Epoch: [3][100/235]	Loss 0.2233 (0.2546)	
Pretraining:	Epoch: [3][150/235]	Loss 0.2511 (0.2524)	
Pretraining:	Epoch: [3][200/235]	Loss 0.2268 (0.2491)	
Pretraining:	 Loss: 0.2466

Pretraining:	Epoch 4/100
----------
Pretraining:	Epoch: [4][50/235]	Loss 0.2419 (0.2297)	
Pretraining:	Epoch: [4][100/235]	Loss 0.2047 (0.2285)	
Pretraining:	Epoch: [4][150/235]	Loss 0.2319 (0.2276)	
Pretraining:	Epoch: [4][200/235]	Loss 0.2060 (0.2256)	
Pretraining:	 Loss: 0.2238

Pretraining:	Epoch 5/100
----------
Pretraining:	Epoch: [5][50/235]	Loss 0.2237 (0.2118)	
Pretraining:	Epoch: [5][100/235]	Loss 0.1917 (0.2115)	
Pretraining:	Epoch: [5][150/235]	Loss 0.2177 (0.2114)	
Pretraining:	Epoch: [5][200/235]	Loss 0.1918 (0.2102)	
Pretraining:	 Loss: 0.2087

Pretraining:	Epoch 6/100
----------
Pretraining:	Epoch: [6][50/235]	Loss 0.2110 (0.1992)	
Pretraining:	Epoch: [6][100/235]	Loss 0.1823 (0.1994)	
Pretraining:	Epoch: [6][150/235]	Loss 0.2071 (0.1997)	
Pretraining:	Epoch: [6][200/235]	Loss 0.1812 (0.1987)	
Pretraining:	 Loss: 0.1975

Pretraining:	Epoch 7/100
----------
Pretraining:	Epoch: [7][50/235]	Loss 0.2016 (0.1894)	
Pretraining:	Epoch: [7][100/235]	Loss 0.1756 (0.1900)	
Pretraining:	Epoch: [7][150/235]	Loss 0.1989 (0.1905)	
Pretraining:	Epoch: [7][200/235]	Loss 0.1729 (0.1898)	
Pretraining:	 Loss: 0.1887

Pretraining:	Epoch 8/100
----------
Pretraining:	Epoch: [8][50/235]	Loss 0.1943 (0.1820)	
Pretraining:	Epoch: [8][100/235]	Loss 0.1693 (0.1827)	
Pretraining:	Epoch: [8][150/235]	Loss 0.1919 (0.1832)	
Pretraining:	Epoch: [8][200/235]	Loss 0.1663 (0.1826)	
Pretraining:	 Loss: 0.1816

Pretraining:	Epoch 9/100
----------
Pretraining:	Epoch: [9][50/235]	Loss 0.1878 (0.1760)	
Pretraining:	Epoch: [9][100/235]	Loss 0.1637 (0.1765)	
Pretraining:	Epoch: [9][150/235]	Loss 0.1865 (0.1771)	
Pretraining:	Epoch: [9][200/235]	Loss 0.1609 (0.1766)	
Pretraining:	 Loss: 0.1756

Pretraining:	Epoch 10/100
----------
Pretraining:	Epoch: [10][50/235]	Loss 0.1819 (0.1706)	
Pretraining:	Epoch: [10][100/235]	Loss 0.1587 (0.1712)	
Pretraining:	Epoch: [10][150/235]	Loss 0.1819 (0.1717)	
Pretraining:	Epoch: [10][200/235]	Loss 0.1565 (0.1714)	
Pretraining:	 Loss: 0.1705

Pretraining:	Epoch 11/100
----------
Pretraining:	Epoch: [11][50/235]	Loss 0.1769 (0.1660)	
Pretraining:	Epoch: [11][100/235]	Loss 0.1555 (0.1667)	
Pretraining:	Epoch: [11][150/235]	Loss 0.1778 (0.1673)	
Pretraining:	Epoch: [11][200/235]	Loss 0.1525 (0.1670)	
Pretraining:	 Loss: 0.1662

Pretraining:	Epoch 12/100
----------
Pretraining:	Epoch: [12][50/235]	Loss 0.1725 (0.1619)	
Pretraining:	Epoch: [12][100/235]	Loss 0.1529 (0.1627)	
Pretraining:	Epoch: [12][150/235]	Loss 0.1746 (0.1634)	
Pretraining:	Epoch: [12][200/235]	Loss 0.1490 (0.1632)	
Pretraining:	 Loss: 0.1624

Pretraining:	Epoch 13/100
----------
Pretraining:	Epoch: [13][50/235]	Loss 0.1683 (0.1584)	
Pretraining:	Epoch: [13][100/235]	Loss 0.1505 (0.1592)	
Pretraining:	Epoch: [13][150/235]	Loss 0.1718 (0.1600)	
Pretraining:	Epoch: [13][200/235]	Loss 0.1459 (0.1599)	
Pretraining:	 Loss: 0.1592

Pretraining:	Epoch 14/100
----------
Pretraining:	Epoch: [14][50/235]	Loss 0.1648 (0.1554)	
Pretraining:	Epoch: [14][100/235]	Loss 0.1485 (0.1562)	
Pretraining:	Epoch: [14][150/235]	Loss 0.1694 (0.1571)	
Pretraining:	Epoch: [14][200/235]	Loss 0.1429 (0.1571)	
Pretraining:	 Loss: 0.1564

Pretraining:	Epoch 15/100
----------
Pretraining:	Epoch: [15][50/235]	Loss 0.1619 (0.1529)	
Pretraining:	Epoch: [15][100/235]	Loss 0.1458 (0.1534)	
Pretraining:	Epoch: [15][150/235]	Loss 0.1670 (0.1544)	
Pretraining:	Epoch: [15][200/235]	Loss 0.1399 (0.1544)	
Pretraining:	 Loss: 0.1538

Pretraining:	Epoch 16/100
----------
Pretraining:	Epoch: [16][50/235]	Loss 0.1592 (0.1507)	
Pretraining:	Epoch: [16][100/235]	Loss 0.1437 (0.1512)	
Pretraining:	Epoch: [16][150/235]	Loss 0.1648 (0.1520)	
Pretraining:	Epoch: [16][200/235]	Loss 0.1374 (0.1521)	
Pretraining:	 Loss: 0.1514

Pretraining:	Epoch 17/100
----------
Pretraining:	Epoch: [17][50/235]	Loss 0.1570 (0.1487)	
Pretraining:	Epoch: [17][100/235]	Loss 0.1414 (0.1490)	
Pretraining:	Epoch: [17][150/235]	Loss 0.1624 (0.1498)	
Pretraining:	Epoch: [17][200/235]	Loss 0.1351 (0.1499)	
Pretraining:	 Loss: 0.1493

Pretraining:	Epoch 18/100
----------
Pretraining:	Epoch: [18][50/235]	Loss 0.1550 (0.1469)	
Pretraining:	Epoch: [18][100/235]	Loss 0.1394 (0.1471)	
Pretraining:	Epoch: [18][150/235]	Loss 0.1603 (0.1478)	
Pretraining:	Epoch: [18][200/235]	Loss 0.1333 (0.1479)	
Pretraining:	 Loss: 0.1474

Pretraining:	Epoch 19/100
----------
Pretraining:	Epoch: [19][50/235]	Loss 0.1529 (0.1452)	
Pretraining:	Epoch: [19][100/235]	Loss 0.1377 (0.1453)	
Pretraining:	Epoch: [19][150/235]	Loss 0.1585 (0.1460)	
Pretraining:	Epoch: [19][200/235]	Loss 0.1316 (0.1462)	
Pretraining:	 Loss: 0.1456

Pretraining:	Epoch 20/100
----------
Pretraining:	Epoch: [20][50/235]	Loss 0.1516 (0.1439)	
Pretraining:	Epoch: [20][100/235]	Loss 0.1360 (0.1439)	
Pretraining:	Epoch: [20][150/235]	Loss 0.1571 (0.1445)	
Pretraining:	Epoch: [20][200/235]	Loss 0.1301 (0.1447)	
Pretraining:	 Loss: 0.1442

Pretraining:	Epoch 21/100
----------
Pretraining:	Epoch: [21][50/235]	Loss 0.1498 (0.1427)	
Pretraining:	Epoch: [21][100/235]	Loss 0.1349 (0.1426)	
Pretraining:	Epoch: [21][150/235]	Loss 0.1560 (0.1433)	
Pretraining:	Epoch: [21][200/235]	Loss 0.1294 (0.1436)	
Pretraining:	 Loss: 0.1431

Pretraining:	Epoch 22/100
----------
Pretraining:	Epoch: [22][50/235]	Loss 0.1476 (0.1410)	
Pretraining:	Epoch: [22][100/235]	Loss 0.1340 (0.1413)	
Pretraining:	Epoch: [22][150/235]	Loss 0.1546 (0.1420)	
Pretraining:	Epoch: [22][200/235]	Loss 0.1283 (0.1422)	
Pretraining:	 Loss: 0.1417

Pretraining:	Epoch 23/100
----------
Pretraining:	Epoch: [23][50/235]	Loss 0.1455 (0.1395)	
Pretraining:	Epoch: [23][100/235]	Loss 0.1330 (0.1398)	
Pretraining:	Epoch: [23][150/235]	Loss 0.1532 (0.1405)	
Pretraining:	Epoch: [23][200/235]	Loss 0.1266 (0.1407)	
Pretraining:	 Loss: 0.1401

Pretraining:	Epoch 24/100
----------
Pretraining:	Epoch: [24][50/235]	Loss 0.1438 (0.1380)	
Pretraining:	Epoch: [24][100/235]	Loss 0.1316 (0.1383)	
Pretraining:	Epoch: [24][150/235]	Loss 0.1518 (0.1390)	
Pretraining:	Epoch: [24][200/235]	Loss 0.1253 (0.1392)	
Pretraining:	 Loss: 0.1386

Pretraining:	Epoch 25/100
----------
Pretraining:	Epoch: [25][50/235]	Loss 0.1422 (0.1365)	
Pretraining:	Epoch: [25][100/235]	Loss 0.1303 (0.1368)	
Pretraining:	Epoch: [25][150/235]	Loss 0.1505 (0.1375)	
Pretraining:	Epoch: [25][200/235]	Loss 0.1242 (0.1377)	
Pretraining:	 Loss: 0.1372

Pretraining:	Epoch 26/100
----------
Pretraining:	Epoch: [26][50/235]	Loss 0.1409 (0.1351)	
Pretraining:	Epoch: [26][100/235]	Loss 0.1293 (0.1354)	
Pretraining:	Epoch: [26][150/235]	Loss 0.1491 (0.1361)	
Pretraining:	Epoch: [26][200/235]	Loss 0.1230 (0.1363)	
Pretraining:	 Loss: 0.1358

Pretraining:	Epoch 27/100
----------
Pretraining:	Epoch: [27][50/235]	Loss 0.1396 (0.1337)	
Pretraining:	Epoch: [27][100/235]	Loss 0.1277 (0.1340)	
Pretraining:	Epoch: [27][150/235]	Loss 0.1476 (0.1347)	
Pretraining:	Epoch: [27][200/235]	Loss 0.1220 (0.1349)	
Pretraining:	 Loss: 0.1344

Pretraining:	Epoch 28/100
----------
Pretraining:	Epoch: [28][50/235]	Loss 0.1382 (0.1324)	
Pretraining:	Epoch: [28][100/235]	Loss 0.1264 (0.1327)	
Pretraining:	Epoch: [28][150/235]	Loss 0.1460 (0.1334)	
Pretraining:	Epoch: [28][200/235]	Loss 0.1207 (0.1336)	
Pretraining:	 Loss: 0.1331

Pretraining:	Epoch 29/100
----------
Pretraining:	Epoch: [29][50/235]	Loss 0.1370 (0.1311)	
Pretraining:	Epoch: [29][100/235]	Loss 0.1251 (0.1314)	
Pretraining:	Epoch: [29][150/235]	Loss 0.1447 (0.1321)	
Pretraining:	Epoch: [29][200/235]	Loss 0.1196 (0.1323)	
Pretraining:	 Loss: 0.1319

Pretraining:	Epoch 30/100
----------
Pretraining:	Epoch: [30][50/235]	Loss 0.1356 (0.1298)	
Pretraining:	Epoch: [30][100/235]	Loss 0.1242 (0.1301)	
Pretraining:	Epoch: [30][150/235]	Loss 0.1435 (0.1309)	
Pretraining:	Epoch: [30][200/235]	Loss 0.1188 (0.1312)	
Pretraining:	 Loss: 0.1308

Pretraining:	Epoch 31/100
----------
Pretraining:	Epoch: [31][50/235]	Loss 0.1346 (0.1287)	
Pretraining:	Epoch: [31][100/235]	Loss 0.1230 (0.1290)	
Pretraining:	Epoch: [31][150/235]	Loss 0.1423 (0.1298)	
Pretraining:	Epoch: [31][200/235]	Loss 0.1178 (0.1301)	
Pretraining:	 Loss: 0.1297

Pretraining:	Epoch 32/100
----------
Pretraining:	Epoch: [32][50/235]	Loss 0.1334 (0.1277)	
Pretraining:	Epoch: [32][100/235]	Loss 0.1221 (0.1280)	
Pretraining:	Epoch: [32][150/235]	Loss 0.1411 (0.1287)	
Pretraining:	Epoch: [32][200/235]	Loss 0.1169 (0.1291)	
Pretraining:	 Loss: 0.1287

Pretraining:	Epoch 33/100
----------
Pretraining:	Epoch: [33][50/235]	Loss 0.1323 (0.1266)	
Pretraining:	Epoch: [33][100/235]	Loss 0.1210 (0.1270)	
Pretraining:	Epoch: [33][150/235]	Loss 0.1401 (0.1277)	
Pretraining:	Epoch: [33][200/235]	Loss 0.1158 (0.1281)	
Pretraining:	 Loss: 0.1277

Pretraining:	Epoch 34/100
----------
Pretraining:	Epoch: [34][50/235]	Loss 0.1315 (0.1257)	
Pretraining:	Epoch: [34][100/235]	Loss 0.1198 (0.1260)	
Pretraining:	Epoch: [34][150/235]	Loss 0.1391 (0.1268)	
Pretraining:	Epoch: [34][200/235]	Loss 0.1150 (0.1272)	
Pretraining:	 Loss: 0.1268

Pretraining:	Epoch 35/100
----------
Pretraining:	Epoch: [35][50/235]	Loss 0.1305 (0.1248)	
Pretraining:	Epoch: [35][100/235]	Loss 0.1189 (0.1251)	
Pretraining:	Epoch: [35][150/235]	Loss 0.1381 (0.1259)	
Pretraining:	Epoch: [35][200/235]	Loss 0.1142 (0.1262)	
Pretraining:	 Loss: 0.1259

Pretraining:	Epoch 36/100
----------
Pretraining:	Epoch: [36][50/235]	Loss 0.1295 (0.1240)	
Pretraining:	Epoch: [36][100/235]	Loss 0.1182 (0.1243)	
Pretraining:	Epoch: [36][150/235]	Loss 0.1372 (0.1251)	
Pretraining:	Epoch: [36][200/235]	Loss 0.1135 (0.1255)	
Pretraining:	 Loss: 0.1251

Pretraining:	Epoch 37/100
----------
Pretraining:	Epoch: [37][50/235]	Loss 0.1285 (0.1233)	
Pretraining:	Epoch: [37][100/235]	Loss 0.1174 (0.1236)	
Pretraining:	Epoch: [37][150/235]	Loss 0.1366 (0.1243)	
Pretraining:	Epoch: [37][200/235]	Loss 0.1129 (0.1247)	
Pretraining:	 Loss: 0.1244

Pretraining:	Epoch 38/100
----------
Pretraining:	Epoch: [38][50/235]	Loss 0.1276 (0.1226)	
Pretraining:	Epoch: [38][100/235]	Loss 0.1165 (0.1229)	
Pretraining:	Epoch: [38][150/235]	Loss 0.1361 (0.1236)	
Pretraining:	Epoch: [38][200/235]	Loss 0.1122 (0.1240)	
Pretraining:	 Loss: 0.1237

Pretraining:	Epoch 39/100
----------
Pretraining:	Epoch: [39][50/235]	Loss 0.1268 (0.1219)	
Pretraining:	Epoch: [39][100/235]	Loss 0.1157 (0.1222)	
Pretraining:	Epoch: [39][150/235]	Loss 0.1356 (0.1229)	
Pretraining:	Epoch: [39][200/235]	Loss 0.1116 (0.1234)	
Pretraining:	 Loss: 0.1231

Pretraining:	Epoch 40/100
----------
Pretraining:	Epoch: [40][50/235]	Loss 0.1262 (0.1213)	
Pretraining:	Epoch: [40][100/235]	Loss 0.1151 (0.1215)	
Pretraining:	Epoch: [40][150/235]	Loss 0.1353 (0.1223)	
Pretraining:	Epoch: [40][200/235]	Loss 0.1115 (0.1228)	
Pretraining:	 Loss: 0.1225

Pretraining:	Epoch 41/100
----------
Pretraining:	Epoch: [41][50/235]	Loss 0.1260 (0.1207)	
Pretraining:	Epoch: [41][100/235]	Loss 0.1143 (0.1209)	
Pretraining:	Epoch: [41][150/235]	Loss 0.1348 (0.1217)	
Pretraining:	Epoch: [41][200/235]	Loss 0.1110 (0.1223)	
Pretraining:	 Loss: 0.1220

Pretraining:	Epoch 42/100
----------
Pretraining:	Epoch: [42][50/235]	Loss 0.1256 (0.1202)	
Pretraining:	Epoch: [42][100/235]	Loss 0.1138 (0.1205)	
Pretraining:	Epoch: [42][150/235]	Loss 0.1339 (0.1213)	
Pretraining:	Epoch: [42][200/235]	Loss 0.1099 (0.1219)	
Pretraining:	 Loss: 0.1216

Pretraining:	Epoch 43/100
----------
Pretraining:	Epoch: [43][50/235]	Loss 0.1249 (0.1201)	
Pretraining:	Epoch: [43][100/235]	Loss 0.1133 (0.1205)	
Pretraining:	Epoch: [43][150/235]	Loss 0.1340 (0.1212)	
Pretraining:	Epoch: [43][200/235]	Loss 0.1094 (0.1217)	
Pretraining:	 Loss: 0.1214

Pretraining:	Epoch 44/100
----------
Pretraining:	Epoch: [44][50/235]	Loss 0.1241 (0.1202)	
Pretraining:	Epoch: [44][100/235]	Loss 0.1143 (0.1207)	
Pretraining:	Epoch: [44][150/235]	Loss 0.1345 (0.1216)	
Pretraining:	Epoch: [44][200/235]	Loss 0.1090 (0.1219)	
Pretraining:	 Loss: 0.1215

Pretraining:	Epoch 45/100
----------
Pretraining:	Epoch: [45][50/235]	Loss 0.1245 (0.1201)	
Pretraining:	Epoch: [45][100/235]	Loss 0.1145 (0.1205)	
Pretraining:	Epoch: [45][150/235]	Loss 0.1343 (0.1215)	
Pretraining:	Epoch: [45][200/235]	Loss 0.1081 (0.1217)	
Pretraining:	 Loss: 0.1211

Pretraining:	Epoch 46/100
----------
Pretraining:	Epoch: [46][50/235]	Loss 0.1253 (0.1190)	
Pretraining:	Epoch: [46][100/235]	Loss 0.1117 (0.1193)	
Pretraining:	Epoch: [46][150/235]	Loss 0.1323 (0.1202)	
Pretraining:	Epoch: [46][200/235]	Loss 0.1069 (0.1205)	
Pretraining:	 Loss: 0.1200

Pretraining:	Epoch 47/100
----------
Pretraining:	Epoch: [47][50/235]	Loss 0.1242 (0.1175)	
Pretraining:	Epoch: [47][100/235]	Loss 0.1105 (0.1179)	
Pretraining:	Epoch: [47][150/235]	Loss 0.1314 (0.1189)	
Pretraining:	Epoch: [47][200/235]	Loss 0.1059 (0.1192)	
Pretraining:	 Loss: 0.1187

Pretraining:	Epoch 48/100
----------
Pretraining:	Epoch: [48][50/235]	Loss 0.1230 (0.1163)	
Pretraining:	Epoch: [48][100/235]	Loss 0.1094 (0.1167)	
Pretraining:	Epoch: [48][150/235]	Loss 0.1308 (0.1177)	
Pretraining:	Epoch: [48][200/235]	Loss 0.1050 (0.1181)	
Pretraining:	 Loss: 0.1177

Pretraining:	Epoch 49/100
----------
Pretraining:	Epoch: [49][50/235]	Loss 0.1217 (0.1153)	
Pretraining:	Epoch: [49][100/235]	Loss 0.1085 (0.1157)	
Pretraining:	Epoch: [49][150/235]	Loss 0.1302 (0.1167)	
Pretraining:	Epoch: [49][200/235]	Loss 0.1042 (0.1171)	
Pretraining:	 Loss: 0.1167

Pretraining:	Epoch 50/100
----------
Pretraining:	Epoch: [50][50/235]	Loss 0.1206 (0.1143)	
Pretraining:	Epoch: [50][100/235]	Loss 0.1077 (0.1148)	
Pretraining:	Epoch: [50][150/235]	Loss 0.1293 (0.1157)	
Pretraining:	Epoch: [50][200/235]	Loss 0.1035 (0.1162)	
Pretraining:	 Loss: 0.1158

Pretraining:	Epoch 51/100
----------
Pretraining:	Epoch: [51][50/235]	Loss 0.1197 (0.1134)	
Pretraining:	Epoch: [51][100/235]	Loss 0.1071 (0.1139)	
Pretraining:	Epoch: [51][150/235]	Loss 0.1286 (0.1148)	
Pretraining:	Epoch: [51][200/235]	Loss 0.1029 (0.1153)	
Pretraining:	 Loss: 0.1150

Pretraining:	Epoch 52/100
----------
Pretraining:	Epoch: [52][50/235]	Loss 0.1189 (0.1127)	
Pretraining:	Epoch: [52][100/235]	Loss 0.1063 (0.1131)	
Pretraining:	Epoch: [52][150/235]	Loss 0.1278 (0.1140)	
Pretraining:	Epoch: [52][200/235]	Loss 0.1023 (0.1145)	
Pretraining:	 Loss: 0.1142

Pretraining:	Epoch 53/100
----------
Pretraining:	Epoch: [53][50/235]	Loss 0.1181 (0.1119)	
Pretraining:	Epoch: [53][100/235]	Loss 0.1057 (0.1125)	
Pretraining:	Epoch: [53][150/235]	Loss 0.1272 (0.1134)	
Pretraining:	Epoch: [53][200/235]	Loss 0.1015 (0.1139)	
Pretraining:	 Loss: 0.1136

Pretraining:	Epoch 54/100
----------
Pretraining:	Epoch: [54][50/235]	Loss 0.1175 (0.1113)	
Pretraining:	Epoch: [54][100/235]	Loss 0.1053 (0.1118)	
Pretraining:	Epoch: [54][150/235]	Loss 0.1265 (0.1127)	
Pretraining:	Epoch: [54][200/235]	Loss 0.1012 (0.1132)	
Pretraining:	 Loss: 0.1129

Pretraining:	Epoch 55/100
----------
Pretraining:	Epoch: [55][50/235]	Loss 0.1170 (0.1107)	
Pretraining:	Epoch: [55][100/235]	Loss 0.1050 (0.1113)	
Pretraining:	Epoch: [55][150/235]	Loss 0.1263 (0.1122)	
Pretraining:	Epoch: [55][200/235]	Loss 0.1006 (0.1127)	
Pretraining:	 Loss: 0.1124

Pretraining:	Epoch 56/100
----------
Pretraining:	Epoch: [56][50/235]	Loss 0.1167 (0.1102)	
Pretraining:	Epoch: [56][100/235]	Loss 0.1045 (0.1107)	
Pretraining:	Epoch: [56][150/235]	Loss 0.1260 (0.1116)	
Pretraining:	Epoch: [56][200/235]	Loss 0.1002 (0.1122)	
Pretraining:	 Loss: 0.1119

Pretraining:	Epoch 57/100
----------
Pretraining:	Epoch: [57][50/235]	Loss 0.1163 (0.1097)	
Pretraining:	Epoch: [57][100/235]	Loss 0.1040 (0.1103)	
Pretraining:	Epoch: [57][150/235]	Loss 0.1256 (0.1112)	
Pretraining:	Epoch: [57][200/235]	Loss 0.0998 (0.1117)	
Pretraining:	 Loss: 0.1114

Pretraining:	Epoch 58/100
----------
Pretraining:	Epoch: [58][50/235]	Loss 0.1159 (0.1092)	
Pretraining:	Epoch: [58][100/235]	Loss 0.1035 (0.1098)	
Pretraining:	Epoch: [58][150/235]	Loss 0.1251 (0.1107)	
Pretraining:	Epoch: [58][200/235]	Loss 0.0994 (0.1112)	
Pretraining:	 Loss: 0.1110

Pretraining:	Epoch 59/100
----------
Pretraining:	Epoch: [59][50/235]	Loss 0.1154 (0.1088)	
Pretraining:	Epoch: [59][100/235]	Loss 0.1031 (0.1094)	
Pretraining:	Epoch: [59][150/235]	Loss 0.1251 (0.1103)	
Pretraining:	Epoch: [59][200/235]	Loss 0.0990 (0.1108)	
Pretraining:	 Loss: 0.1105

Pretraining:	Epoch 60/100
----------
Pretraining:	Epoch: [60][50/235]	Loss 0.1151 (0.1084)	
Pretraining:	Epoch: [60][100/235]	Loss 0.1025 (0.1089)	
Pretraining:	Epoch: [60][150/235]	Loss 0.1246 (0.1098)	
Pretraining:	Epoch: [60][200/235]	Loss 0.0987 (0.1104)	
Pretraining:	 Loss: 0.1101

Pretraining:	Epoch 61/100
----------
Pretraining:	Epoch: [61][50/235]	Loss 0.1146 (0.1080)	
Pretraining:	Epoch: [61][100/235]	Loss 0.1025 (0.1085)	
Pretraining:	Epoch: [61][150/235]	Loss 0.1242 (0.1094)	
Pretraining:	Epoch: [61][200/235]	Loss 0.0986 (0.1100)	
Pretraining:	 Loss: 0.1097

Pretraining:	Epoch 62/100
----------
Pretraining:	Epoch: [62][50/235]	Loss 0.1143 (0.1076)	
Pretraining:	Epoch: [62][100/235]	Loss 0.1022 (0.1082)	
Pretraining:	Epoch: [62][150/235]	Loss 0.1239 (0.1090)	
Pretraining:	Epoch: [62][200/235]	Loss 0.0981 (0.1096)	
Pretraining:	 Loss: 0.1093

Pretraining:	Epoch 63/100
----------
Pretraining:	Epoch: [63][50/235]	Loss 0.1139 (0.1073)	
Pretraining:	Epoch: [63][100/235]	Loss 0.1021 (0.1078)	
Pretraining:	Epoch: [63][150/235]	Loss 0.1233 (0.1086)	
Pretraining:	Epoch: [63][200/235]	Loss 0.0978 (0.1092)	
Pretraining:	 Loss: 0.1089

Pretraining:	Epoch 64/100
----------
Pretraining:	Epoch: [64][50/235]	Loss 0.1134 (0.1069)	
Pretraining:	Epoch: [64][100/235]	Loss 0.1017 (0.1074)	
Pretraining:	Epoch: [64][150/235]	Loss 0.1228 (0.1083)	
Pretraining:	Epoch: [64][200/235]	Loss 0.0974 (0.1088)	
Pretraining:	 Loss: 0.1086

Pretraining:	Epoch 65/100
----------
Pretraining:	Epoch: [65][50/235]	Loss 0.1131 (0.1067)	
Pretraining:	Epoch: [65][100/235]	Loss 0.1015 (0.1072)	
Pretraining:	Epoch: [65][150/235]	Loss 0.1228 (0.1081)	
Pretraining:	Epoch: [65][200/235]	Loss 0.0971 (0.1086)	
Pretraining:	 Loss: 0.1083

Pretraining:	Epoch 66/100
----------
Pretraining:	Epoch: [66][50/235]	Loss 0.1128 (0.1066)	
Pretraining:	Epoch: [66][100/235]	Loss 0.1013 (0.1069)	
Pretraining:	Epoch: [66][150/235]	Loss 0.1229 (0.1079)	
Pretraining:	Epoch: [66][200/235]	Loss 0.0970 (0.1085)	
Pretraining:	 Loss: 0.1082

Pretraining:	Epoch 67/100
----------
Pretraining:	Epoch: [67][50/235]	Loss 0.1125 (0.1065)	
Pretraining:	Epoch: [67][100/235]	Loss 0.1014 (0.1068)	
Pretraining:	Epoch: [67][150/235]	Loss 0.1228 (0.1080)	
Pretraining:	Epoch: [67][200/235]	Loss 0.0975 (0.1085)	
Pretraining:	 Loss: 0.1083

Pretraining:	Epoch 68/100
----------
Pretraining:	Epoch: [68][50/235]	Loss 0.1132 (0.1068)	
Pretraining:	Epoch: [68][100/235]	Loss 0.1010 (0.1069)	
Pretraining:	Epoch: [68][150/235]	Loss 0.1228 (0.1081)	
Pretraining:	Epoch: [68][200/235]	Loss 0.0974 (0.1085)	
Pretraining:	 Loss: 0.1083

Pretraining:	Epoch 69/100
----------
Pretraining:	Epoch: [69][50/235]	Loss 0.1139 (0.1069)	
Pretraining:	Epoch: [69][100/235]	Loss 0.1014 (0.1071)	
Pretraining:	Epoch: [69][150/235]	Loss 0.1222 (0.1083)	
Pretraining:	Epoch: [69][200/235]	Loss 0.0971 (0.1087)	
Pretraining:	 Loss: 0.1085

Pretraining:	Epoch 70/100
----------
Pretraining:	Epoch: [70][50/235]	Loss 0.1132 (0.1070)	
Pretraining:	Epoch: [70][100/235]	Loss 0.1013 (0.1073)	
Pretraining:	Epoch: [70][150/235]	Loss 0.1241 (0.1087)	
Pretraining:	Epoch: [70][200/235]	Loss 0.0968 (0.1090)	
Pretraining:	 Loss: 0.1087

Pretraining:	Epoch 71/100
----------
Pretraining:	Epoch: [71][50/235]	Loss 0.1126 (0.1069)	
Pretraining:	Epoch: [71][100/235]	Loss 0.1004 (0.1070)	
Pretraining:	Epoch: [71][150/235]	Loss 0.1245 (0.1083)	
Pretraining:	Epoch: [71][200/235]	Loss 0.0966 (0.1086)	
Pretraining:	 Loss: 0.1083

Pretraining:	Epoch 72/100
----------
Pretraining:	Epoch: [72][50/235]	Loss 0.1126 (0.1065)	
Pretraining:	Epoch: [72][100/235]	Loss 0.0999 (0.1066)	
Pretraining:	Epoch: [72][150/235]	Loss 0.1221 (0.1076)	
Pretraining:	Epoch: [72][200/235]	Loss 0.0962 (0.1079)	
Pretraining:	 Loss: 0.1077

Pretraining:	Epoch 73/100
----------
Pretraining:	Epoch: [73][50/235]	Loss 0.1122 (0.1060)	
Pretraining:	Epoch: [73][100/235]	Loss 0.0995 (0.1062)	
Pretraining:	Epoch: [73][150/235]	Loss 0.1212 (0.1072)	
Pretraining:	Epoch: [73][200/235]	Loss 0.0962 (0.1075)	
Pretraining:	 Loss: 0.1072

Pretraining:	Epoch 74/100
----------
Pretraining:	Epoch: [74][50/235]	Loss 0.1121 (0.1057)	
Pretraining:	Epoch: [74][100/235]	Loss 0.0991 (0.1059)	
Pretraining:	Epoch: [74][150/235]	Loss 0.1209 (0.1067)	
Pretraining:	Epoch: [74][200/235]	Loss 0.0958 (0.1070)	
Pretraining:	 Loss: 0.1068

Pretraining:	Epoch 75/100
----------
Pretraining:	Epoch: [75][50/235]	Loss 0.1116 (0.1053)	
Pretraining:	Epoch: [75][100/235]	Loss 0.0983 (0.1055)	
Pretraining:	Epoch: [75][150/235]	Loss 0.1204 (0.1062)	
Pretraining:	Epoch: [75][200/235]	Loss 0.0952 (0.1065)	
Pretraining:	 Loss: 0.1063

Pretraining:	Epoch 76/100
----------
Pretraining:	Epoch: [76][50/235]	Loss 0.1107 (0.1046)	
Pretraining:	Epoch: [76][100/235]	Loss 0.0976 (0.1048)	
Pretraining:	Epoch: [76][150/235]	Loss 0.1197 (0.1055)	
Pretraining:	Epoch: [76][200/235]	Loss 0.0946 (0.1059)	
Pretraining:	 Loss: 0.1056

Pretraining:	Epoch 77/100
----------
Pretraining:	Epoch: [77][50/235]	Loss 0.1099 (0.1038)	
Pretraining:	Epoch: [77][100/235]	Loss 0.0973 (0.1041)	
Pretraining:	Epoch: [77][150/235]	Loss 0.1191 (0.1048)	
Pretraining:	Epoch: [77][200/235]	Loss 0.0939 (0.1052)	
Pretraining:	 Loss: 0.1050

Pretraining:	Epoch 78/100
----------
Pretraining:	Epoch: [78][50/235]	Loss 0.1093 (0.1033)	
Pretraining:	Epoch: [78][100/235]	Loss 0.0970 (0.1035)	
Pretraining:	Epoch: [78][150/235]	Loss 0.1185 (0.1043)	
Pretraining:	Epoch: [78][200/235]	Loss 0.0936 (0.1047)	
Pretraining:	 Loss: 0.1045

Pretraining:	Epoch 79/100
----------
Pretraining:	Epoch: [79][50/235]	Loss 0.1085 (0.1029)	
Pretraining:	Epoch: [79][100/235]	Loss 0.0965 (0.1030)	
Pretraining:	Epoch: [79][150/235]	Loss 0.1180 (0.1038)	
Pretraining:	Epoch: [79][200/235]	Loss 0.0932 (0.1043)	
Pretraining:	 Loss: 0.1040

Pretraining:	Epoch 80/100
----------
Pretraining:	Epoch: [80][50/235]	Loss 0.1082 (0.1026)	
Pretraining:	Epoch: [80][100/235]	Loss 0.0962 (0.1027)	
Pretraining:	Epoch: [80][150/235]	Loss 0.1175 (0.1034)	
Pretraining:	Epoch: [80][200/235]	Loss 0.0928 (0.1039)	
Pretraining:	 Loss: 0.1036

Pretraining:	Epoch 81/100
----------
Pretraining:	Epoch: [81][50/235]	Loss 0.1077 (0.1023)	
Pretraining:	Epoch: [81][100/235]	Loss 0.0958 (0.1023)	
Pretraining:	Epoch: [81][150/235]	Loss 0.1169 (0.1031)	
Pretraining:	Epoch: [81][200/235]	Loss 0.0925 (0.1036)	
Pretraining:	 Loss: 0.1033

Pretraining:	Epoch 82/100
----------
Pretraining:	Epoch: [82][50/235]	Loss 0.1075 (0.1022)	
